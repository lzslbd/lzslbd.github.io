{"meta":{"title":"LBD's Blog","subtitle":"大数据与分布式","description":"大数据与分布式","author":"LBD","url":"http://linbingdong.com"},"pages":[{"title":"","date":"2017-10-02T13:28:33.000Z","updated":"2017-01-17T12:21:07.000Z","comments":true,"path":"google2e1f6eca2abac7fc.html","permalink":"http://linbingdong.com/google2e1f6eca2abac7fc.html","excerpt":"","text":"google-site-verification: google2e1f6eca2abac7fc.html","raw":null,"content":null},{"title":"","date":"2017-11-23T02:03:15.000Z","updated":"2017-11-23T02:03:15.000Z","comments":true,"path":"about/index.html","permalink":"http://linbingdong.com/about/index.html","excerpt":"","text":"不忘初心，砥砺前行 AboutLBD，北邮本科，国科大研究生。目前专注于分布式系统、分布式数据库。 Contact邮 箱 : lzslbd@163.com微 信 : linbingdong公众号 : FullStackPlan","raw":null,"content":null},{"title":"分类","date":"2017-01-02T08:05:09.000Z","updated":"2017-03-07T12:50:47.000Z","comments":true,"path":"categories/index.html","permalink":"http://linbingdong.com/categories/index.html","excerpt":"","text":"","raw":null,"content":null},{"title":"标签","date":"2017-01-02T08:00:41.000Z","updated":"2017-03-07T12:50:33.000Z","comments":true,"path":"tags/index.html","permalink":"http://linbingdong.com/tags/index.html","excerpt":"","text":"","raw":null,"content":null}],"posts":[{"title":"分布式系统的问题","slug":"分布式系统的问题","date":"2018-04-17T16:00:00.000Z","updated":"2018-04-22T14:36:57.000Z","comments":true,"path":"2018/04/18/分布式系统的问题/","link":"","permalink":"http://linbingdong.com/2018/04/18/分布式系统的问题/","excerpt":"\n本文内容翻译自《Designing Data-Intensive Applications》一书的第8章。\n","text":"本文内容翻译自《Designing Data-Intensive Applications》一书的第8章。 近几章主要介绍系统如何处理错误。例如，我们讨论了副本故障转移，复制滞后和事务的并发控制。当我们理解实际系统中可能出现的各种边界情况时，我们就能更好地处理它们。 前几章虽然谈论了很多关于错误的问题，但是还是太乐观了。在本章中，我们将最悲观地假设“任何可能出故障的，最终都会出故障”。 分布式系统编程与在单机上编写软件有本质区别——主要区别在于分布式系统中有很多新奇的可能出故障的方式。 本章中，我们将了解在实践中出现的问题，并了解哪些我们可以依赖，哪些不行。 最后，作为工程师，我们的任务是构建能够完成工作的系统（即满足用户所期望的保证），尽管各个部件都出错了。 在第9章中，我们将看看可以在分布式系统中提供这种保证的算法的一些示例。 但首先，在本章中，我们必须了解我们面临的挑战。 本章是对分布式系统中可能出现的问题的悲观和沮丧的概述。 我们将研究网络问题（第277页的“不可靠的网络”）; 时钟和时序问题（第287页上的“不可靠的时钟”）; 我们将讨论它们可以避免的程度。 所有这些问题造成的后果都会让人迷惑，因此我们将探讨如何思考分布式系统的状态以及如何推理已发生的事情（第300页的“知识，真相和谎言”）。 错误和部分故障当你在单机上写程序时，它通常会以一种可预测的方式运行：要么正常工作，要么无法工作。有bug的软件可能会让人觉得电脑出问题了（通常重新启动就能解决问题），但大部分还是软件写得不好的后果。 没有什么根本原因能让单机上的软件表现得奇怪：当硬件正常工作时，相同的操作总是产生相同的结果（这是确定性的）。如果存在硬件问题（例如，内存损坏或连接器松动），其后果通常是整个系统失效（例如“蓝屏死机”，无法启动）。具有良好软件的单机通常功能完好或完全损坏，而不在两者之间。 这是计算机设计中的一个慎重选择：如果发生内部故障，我们宁愿计算机完全崩溃，而不是返回错误的结果，因为错误的结果很难处理，并且令人困惑。因此，计算机隐藏了它们实现所依赖的模糊物理现实，并提出了一个理想化的系统模型，它可以与数学完美结合起来。CPU指令总是做同样的事情; 如果你将一些数据写入内存或磁盘，则该数据保持完好并且不会随机损坏。 这种始终正确计算的设计目标可以追溯到第一台数字计算机。 当你编写运行在多台计算机上并通过网络连接的软件时，情况完全不同。 在分布式系统中，我们不再处于理想系统模型中 - 我们别无选择，只能面对物理世界的混乱现实。 而在现实世界中，正如这个轶事所示，各种各样的事情可能会出错： 在我有限的经验中，我处理过单个数据中心（DC）中的长时间网络分区，PDU（配电单元）故障，交换机故障，整个机架的意外电源故障，全DC主干故障，全DC 电力故障和一位低血糖驾驶员将他的福特皮卡撞进空调系统。我甚至不是一个运维人员。——Coda Hale 在分布式系统中，可能出现这样的情况，尽管系统的其他部分工作正常，但系统的某些部分可能会以某种不可预知的方式出故障。这就叫做部分故障。该问题的难点在于部分故障是不确定的：如果你试图做任何包含多个节点和网络的事情，它可能有时工作正常，有时出现不可预知的故障。正如我们将要看到的，你可能甚至不知道某件事是否成功，因为消息在网络中传播所花费的时间也是不确定的！ 这种不确定性和部分故障的可能性是分布式系统难以处理的原因。 云计算和超级计算关于如何构建大型计算系统有一系列哲学： 规模的一端是高性能计算（HPC）领域。拥有数千个CPU的超级计算机通常用于计算密集型科学计算任务，如天气预报或分子动力学（模拟原子和分子的运动）。 另一端是云计算，云计算没有非常明确的定义，但通常与多租户数据中心，连接IP网络的商品计算机（通常是以太网），弹性/按需资源分配以及按时计费联系在一起。 有了这些哲学，处理错误的方法就非常不同了。在超级计算机中，作业通常会对其计算状态不时地做检查点到持久存储上。如果一个节点发生故障，通常的解决方案是简单地停止整个集群工作负载。故障节点修复后，从上一个检查点重新开始计算。因此，超级计算机更像是一台单节点计算机而不是分布式系统：它通过升级为完全故障来处理部分故障 - 当系统的任何部分发生故障，简单地让整个系统崩溃（就像单机上的内核恐慌一样）。 在本书中，我们重点介绍实现互联网服务的系统，这些系统通常看起来与超级计算机有很大不同： 许多与互联网有关的应用程序都是在线的，在某种意义上它们需要能够随时为用户提供低延迟服务。服务不可用（例如，停止群集以进行修复）是不可接受的。相比之下，像天气模拟这样的离线（批处理）作业可以停止并重启，而且影响很小。 超级计算机通常由专用硬件构建，其中每个节点都非常可靠，并且节点通过共享内存和远程直接内存访问（RDMA）进行通信。另一方面，云服务中的节点是由普通机器构建的，它们能以较低的成本提供相同的性能，但也具有较高的故障率。 大型数据中心网络通常基于IP和以太网，以Clos拓扑排列来提供高对分带宽。超级计算机通常使用专门的网络拓扑结构，例如多维网格和toruses，这为具有已知通信模式的HPC工作负载提供了更好的性能。 系统越大，系统中有组件出故障的概率越高。随着时间的推移，故障被修复，新的组件又出故障，但是在一个有数千个节点的系统中，认为系统中总是在发生故障是一个合理的假设。当错误处理策略不够有效时，一个大型系统最终会花费大量的时间从故障中恢复，而不是做有用的工作。 如果系统可以容忍失败的节点并且仍然作为一个整体继续工作，这对于操作和维护是一个非常有用的特性：例如，可以执行滚动升级（参阅第4章），一次重启一个节点，系统继续为用户提供服务而不中断。在云环境中，如果一台虚拟机运行不佳，可以将其杀死并请求一台新的虚拟机（希望新的虚拟机速度更快）。 在地理分布式部署中（保持数据在地理位置上接近用户以减少访问延迟），通信很可能通过互联网进行，与本地网络相比，速度慢且不可靠。超级计算机通常假设它们的所有节点都靠近在一起。 如果我们想让分布式系统工作，就必须接受部分故障的可能性，并在软件中建立容错机制。换句话说，我们需要从不可靠的组件中构建可靠的系统。（正如在第6页的“可靠性”中所讨论的那样，没有完美的可靠性，所以我们需要了解我们可以实际承诺的极限。） 即使在只有少数节点的小型系统中，考虑部分故障也很重要。在一个小型系统中，很可能大部分组件在大多数时间都正常工作。但是，迟早会有一部分系统出现故障，软件将不得不以某种方式处理它。故障处理必须是软件设计的一部分，并且软件的操作员需要知道发生故障时软件会出现什么行为。 假定错误很少发生，并只往好的想是不明智的。考虑各种可能的错误（甚至是不太可能的错误），并在测试环境中人为地创建这些情况以查看会发生什么是非常重要的。在分布式系统中，抱着怀疑，悲观和偏执的态度才能取得成功。 从不可靠的组件中构建可靠的系统你可能会怀疑这是否有道理——直觉上，一个系统只能和其最不可靠的组件（它最薄弱的环节）一样可靠。事实并非如此：事实上，从不太可靠的基础构建更可靠的系统，这在计算中是一个古老的想法。 例如： 纠错码允许数字数据在通信信道上准确传输，偶尔会出现某些位错误，例如由于无线网络上的无线电干扰。 IP（互联网协议）是不可靠的：数据包可能丢失，延迟，重复或乱序。TCP（传输控制协议）在IP之上提供了一个更可靠的传输层：它确保丢失的数据包被重传，消除重复，并且数据包被重新组装为它们的发送顺序。 虽然系统可能比其基础部分更可靠，但它的可靠性总是有限的。例如，纠错码可以处理少量的单比特错误，但是如果信号被干扰所淹没，那么通过通信信道可以获得的数据量就有一个基本限制。TCP可以对我们隐藏数据包丢失，重复和乱序，但它不能在网络中奇迹般地消除延迟。 虽然更可靠的更高级别的系统并不完美，但它仍然很有用，因为它可以处理一些棘手的低级故障，因此通常也可以更轻松地解决和处理其余的故障。 不可靠的网络正如在第二部分的介绍中所讨论的，我们在本书中关注的分布式系统是shared-nothing系统：即一堆机器通过网络连接。网络是这些机器可以通信的唯一方式。我们假设每台机器有自己的内存和磁盘，一台机器无法访问另一台机器的内存或磁盘（除了通过网络向服务发出请求外）。 shared-nothing并不是构建系统的唯一方式，但它已经成为构建互联网服务的主要方式，原因有几个：它相对便宜，因为它不需要特殊的硬件，可以利用商品化的云计算服务， 可以通过跨多个地理分布的数据中心进行冗余来实现高可靠性。 互联网和数据中心的大部分内部网络（通常是以太网）都是异步分组网络。 在这种网络中，一个节点可以向另一个节点发送一个消息（一个数据包），但是网络不能保证它何时到达，甚至是否能到达。如果你发送请求并期待响应，很多事情可能会出错（其中一些如图8-1所示）： 你的请求可能已经丢失（可能是某人拔掉了网线）。 你的请求可能正在队列中等待，稍后会被发送（也许网络或收件人过载）。 远程节点可能失败（可能崩溃或掉电）。 远程节点可能暂时停止了响应（可能正在经历长时间的垃圾回收暂停;请参阅第295页上的“进程暂停”），但稍后它会再次开始响应。 远程节点可能处理了你的请求，但响应在网络上丢失了（可能是网络交换机配置错误）。 远程节点可能已经处理了你的请求，但响应已经延迟并且将稍后发送（可能是网络或你自己的机器过载）。 发送方甚至无法知道数据包是否已经被发送：唯一的选择是让接收方发送响应消息，这可能会丢失或延迟。这些问题在异步网络中难以区分：你拥有的唯一信息是你尚未收到响应。如果你向另一个节点发送请求并且未收到回复，也无法知道是什么原因。 处理该问题通常的方法是使用超时：一段时间后就放弃等待并假设响应不会送达。但是，当发生超时时，你仍然不知道远程节点是否收到了你的请求（如果请求仍然在某个地方排队，它仍然可能会被传送给接收方，即使发送方已经放弃了）。 网络故障实践几十年来我们一直在建立计算机网络——人们可能希望现在我们已经知道了如何使它们变得可靠。但是，似乎我们还没有成功。 有一些系统的研究和大量的轶事证据表明，即使在由公司运营的数据中心那样的受控环境中，网络问题也可能非常普遍。在一家中等规模的数据中心进行的一项研究发现，每个月大约发生12次网络故障，其中一半单台机器断开连接，一半整个机架断开连接。另一项研究测量了架顶式交换机，汇聚交换机和负载平衡器等组件的故障率，发现添加冗余网络设备不会像你所希望的那样减少故障，因为它不能防范人为错误（例如，配置错误的交换机），这是造成网络中断的主要原因。 公共云服务（如EC2）因频繁出现短暂的网络故障而臭名昭着，管理良好的专用数据中心网络会比较稳定。尽管如此，没有人能够避免网络问题的干扰：例如，交换机软件升级期间的问题可能会触发网络拓扑重新配置，在此期间网络数据包可能会延迟超过一分钟。鲨鱼可能咬住海底电缆并损坏它们。其他令人惊讶的故障包括网络接口有时会丢弃所有入站数据包，但成功发送出站数据包。因此，仅仅因为网络链接在一个方向上正常工作并不能保证它也在相反的方向也正常工作。 网络分区当网络的一部分由于网络故障而与其余部分断开时，有时称为网络分区或网络分割。 在本书中，我们使用更一般的术语网络故障，以避免与如第6章所述的存储系统的分区（碎片）混淆。 即使你的环境中很少发生网络故障，但可能发生故障的事实意味着你的软件需要能够处理它们。网络上的通信总有可能会失败，这是没有办法的。 如果网络故障的错误处理未经过定义和测试，则可能会发生反复无常的错误：例如，即使网络恢复，群集也可能会死锁并永久无法为请求提供服务，甚至可能会删除你的所有数据。如果软件不在受控的情况下，可能会有意想不到的行为。 处理网络故障并不一定意味着容忍它们：如果你的网络通常相当可靠，则有效的方法可能是在网络遇到问题时向用户简单显示错误消息。但是，你需要知道你的软件会对网络问题做出什么反应，并确保系统能够从中恢复。刻意地触发网络问题并测试系统响应是有意义的（这是Chaos Monkey背后的想法;请参阅第6页的“可靠性”）。 检测故障很多系统都需要自动检测故障节点。 例如： 负载平衡器需要停止向死节点发送请求。 在single-leader复制的分布式数据库中，如果leader发生故障，需要提升一个follower成为新的leader（参阅第152页的“处理节点故障”）。 不幸的是，网络的不确定性使得判断一个节点是否正常工作变得很困难。在某些特定情况下，你可能会收到一些反馈信息，以明确告诉你某些组件不正常工作： 如果你可以到达运行节点的机器，但没有进程正在监听目标端口（例如，因为进程崩溃），操作系统将通过发送RST或FIN数据包来帮助关闭或拒绝TCP连接。但是，如果节点在处理请求过程中崩溃，你将无法知道远程节点实际已经处理了多少数据。 如果节点进程崩溃（或被管理员杀死）但节点的操作系统仍在运行，脚本可以通知其他节点有关崩溃的信息，以便另一个节点可以快速接管而无需等待超时。 如果你有权限访问数据中心网络交换机的管理界面，则可以查询它们以检测硬件级别的链路故障（例如，远程机器是否关闭电源）。如果你通过互联网连接，或者你处于共享数据中心但无权限无法访问交换机，或者由于网络问题而无法访问管理界面，则无法使用该选项。 如果路由器确定你尝试连接的IP地址无法访问，它可能会用ICMP目标无法访问的数据包回复你。但是，路由器不具备神奇的故障检测能力——它受到与网络其他组成部分相同的限制。 远程节点宕机的快速反馈很有用，但你不能指望它。即使TCP确认数据包已发送，应用程序在处理数据之前可能已崩溃。如果你想确认一个请求是成功的，需要在应用程序本身积极响应。 相反，如果出现问题，你可能会在某个层次上得到错误响应，但通常你必须假设根本得不到响应。你可以重试几次（TCP重试是透明的，但您你可以在应用程序级别重试），等待超时过去，并且如果在超时范围内没有收到响应，才最终宣布节点失效。 超时和无限延迟如果超时是检测故障的唯一可靠方法，那么超时时间应该多长？不幸的是没有简单的答案。 超时时间长意味着需要长时间等待才能宣告一个节点死亡（并且在此期间，用户可能不得不等待或看到错误消息）。超时时间短可以更快地检测到故障，但是会带来更高的误判的风险，例如节点可能只是暂时变慢（比如由于工作或网络负载高峰）就被误判为死亡。 过早地宣告一个节点已经死亡是有问题的：如果节点实际上处于活动状态并且正在执行一些操作（例如，发送电子邮件），然后另一个节点接管，那么该操作最终可能会执行两次。我们将在第300页的“知识，真相和谎言”以及第9章和第11章中更详细地讨论该问题。 当一个节点被宣告死亡时，其职责需要转移到其他节点，这会给其他节点和网络带来额外的负担。如果系统已经处于高负载状态，过早宣告节点死亡会使问题变得更糟。特别地，可能节点实际上并未死亡，只是由于负载太高而响应缓慢。将其负载转移到其他节点可能会导致瀑布式的失败（在极端情况下，所有节点都宣告对方死亡，然后一切都停止工作）。 假设一个虚拟系统的网络可以保证数据包的最大延迟——每个数据包要么在一段时间内送达，要么丢失，但时间永远不会超过d。此外，假设可以保证非故障节点在总是在一段时间r内处理请求。在这种情况下，可以保证每个成功的请求都会在2d + r的时间内收到响应，并且如果在此时间内没有收到响应，则知道网络或远程节点不工作。如果情况真如上述那样，2d + r将是一个合理的超时时间。 不幸的是，我们所使用的大多数系统都没有这些保证：异步网络具有无限的延迟（即它们尽可能快地发送数据包，但数据包到达所需的时间没有上限） ，并且大多数服务器实现不能保证它们可以在特定时间内处理请求（请参阅“响应时间保证”（第298页））。对于故障检测，大部分时间内快是不够的：如果超时时间较短，则往返时间只需要瞬间上升就会导致系统失去平衡。 网络拥塞和排队在开车汽车时，由于交通堵塞，在路上花的时间往往不尽相同。类似的，计算机网络上的数据包延迟的可变性通常也是由于排队： 如果多个不同的节点同时尝试向相同的目的地发送数据包，则网络交换机必须将它们排队并将它们逐个送入目标网络链路（如图8-2所示）。在繁忙的网络链路上，数据包可能需要等待一段时间才能获得一个槽（这称为网络拥塞）。如果传入的数据太多以至于交换机队列填满，数据包将被丢弃，因此需要重新发送数据包，即使网络运行良好。 当数据包到达目标机器时，如果所有CPU内核当前都处于繁忙状态，则来自网络的传入请求将被操作系统排队，直到应用程序准备好处理它为止。根据机器的负载情况，这可能需要一段任意长度的时间。 在虚拟化环境中，当另一个虚拟机正在使用CPU核的时候，正在运行的操作系统通常会暂停几十毫秒。在此期间，虚拟机无法使用网络中的任何数据，因此输入数据被虚拟机监视器排队（缓冲），这进一步增加了网络延迟的可变性。 TCP执行流量控制（也称为拥塞避免或背压），节点限制自己的发送速率以避免网络链路或接收节点过载。这意味着甚至在数据进入网络之前，发送者也会让数据排队。 此外，如果TCP在某个超时时间内未得到确认（根据观察的往返时间计算），则认为数据包丢失，并且丢失的数据包将自动重新发送。尽管应用程序没有看到数据包丢失和重传，但它确实会看到由此产生的延迟（等待超时过期，然后等待重传的数据包得到确认）。 TCP与UDP 一些对延迟敏感的应用程序（如视频会议和IP语音（VoIP））使用UDP而不是TCP。这是延迟的可靠性和可变性之间的折衷：由于UDP不执行流量控制并且不重传丢失的数据包，所以它避免了一些可变网络延迟的原因（尽管它仍然易受交换机队列和调度延迟的影响）。 在延迟数据毫无价值的情况下，UDP是一个不错的选择。例如，在VoIP电话呼叫中，可能没有足够的时间在其数据将在扬声器上播放之前重新传输丢失的数据包。在这种情况下，重传数据包没有意义——应用程序必须用无声填充丢失数据包的时隙（导致声音短暂中断），然后在数据流中继续。相反，重试发生在人类层面。（“你能再说一遍吗？刚刚没声音了。”） 所有这些因素都会造成网络延迟的变化。当系统接近其最大容量时，排队延迟的范围很大：拥有大量备用容量的系统可以轻松消化队列，而在高度使用的系统中，很快就会排起长队列。 在公有云和多租户数据中心中，资源被许多客户共享：网络链路和交换机，甚至每台计算机的网络接口和CPU（在虚拟机上运行时）都是共享的。批处理工作负载（如MapReduce）（请参阅第10章）可以轻松地使网络链接饱和。由于你无法控制或了解其他客户对共享资源的使用情况，如果你身边的某个人正在使用大量资源，网络延迟可能会变化无常。 在这样的环境中，你只能通过实验来选择超时时间：在一个延长的周期中测试和多台机器的网络往返时间分布，以确定延迟可变性的期望。然后，考虑应用程序的特性，你可以在故障检测延迟与过早超时风险之间确定一个适当的折衷。 更好的是，系统不是使用配置的常量超时，而是能够连续测量响应时间及其变化（抖动），并根据观察到的响应时间分布自动调整超时。这可以用Phi Accrual故障检测器完成，该检测器在Akka和Cassandra中被使用。TCP重传超时运行原理类似。 同步与异步网络如果我们可以依赖网络来传递具有固定最大延迟的数据包，而不是丢弃数据包，那么分布式系统就会简单得多。为什么我们不能在硬件级别解决这个问题，并使网络可靠，以便软件不必考虑这些问题？ 为了回答这个问题，将数据中心网络与非常可靠的传统固定电话网络（非蜂窝，非VoIP）进行比较是很有趣的：延迟音频帧和掉话是非常罕见的。电话呼叫需要始终较低的端到端延迟和足够的带宽来传输语音的音频样本。在计算机网络中拥有类似的可靠性和可预测性不是很好吗？ 当你通过电话网络拨打电话时，它会建立一条线路：沿着两个呼叫者之间的整个路由为呼叫分配固定的有保证的带宽量。该线路保持占用，直到通话结束。例如，ISDN网络以每秒4000帧的固定速率运行。呼叫建立后，每个帧内（每个方向）分配16位空间。因此，在通话期间，每一方都保证能够每250微秒发送一个精确的16位音频数据。 这种网络是同步的：即使数据通过多个路由器，也不会受到排队的影响，因为呼叫的16位空间已经在网络的下一跳中保留下来了。而且由于没有排队，网络的最大端到端延迟是固定的。我们称之为有限的延迟。 我们不能简单地使网络延迟可预测吗？ 请注意，电话网络中的线路与TCP连接非常不同：线路是固定数量的预留带宽，在线路建立时没有人可以使用，而TCP连接的数据包有机会使用任何可用的网络带宽。你可以为TCP提供可变大小的数据块（例如电子邮件或网页），TCP会尽可能在最短的时间内传输它。当TCP连接空闲时，不使用任何带宽。如果数据中心网络和互联网是线路交换网络，那么建立线路后可以确保最大往返时间。然而，它们并不是：以太网和IP是分组交换协议，它们受到排队的影响，从而导致网络无限延迟。这些协议没有线路的概念。 为什么数据中心网络和互联网使用分组交换？答案是，它们针对突发流量进行了优化。一个电路适用于音频或视频通话，在通话期间需要每秒传送相当恒定的比特数。另一方面，请求网页，发送电子邮件或传输文件没有任何特定的带宽需求，我们只是希望它尽快完成。 如果你想通过线路传输文件，则必须猜测带宽分配。如果你猜的太低，传输速度会不必要的太慢，导致网络容量没有使用。如果你猜得太高，线路就无法建立（因为如果无法保证其带宽分配，网络不能建立线路）。因此，使用线路进行突发数据传输会浪费网络容量，并导致传输不必要的缓慢。相比之下，TCP会动态调整数据传输速率以适应可用的网络容量。 已经有一些尝试构建支持线路交换和分组交换的混合网络，例如ATM。例如InfiniBand：它实现了链路层的端到端流量控制，减少了网络排队的概率，尽管它仍然可能因链路拥塞而遭受延迟。通过谨慎使用服务质量（QoS，数据包的优先级和调度）和准入控制（限速发送器），可以仿真分组网络上的线路交换，或提供统计上有界的延迟。 延迟和资源使用 更一般地说，你可以将可变延迟视为动态资源分区的结果。 假设两台电话交换机之间有一条线路，可以同时进行10,000个呼叫。通过此线路切换的每个电路都占用其中一个呼叫插槽。因此，你可以将线路视为可由多达10,000个并发用户共享的资源。资源以静态方式分配：即使你现在是线路上唯一的电话，并且所有其他9,999个插槽都未使用，你的线路仍将分配跟线路被充分利用时相同的固定数量的带宽。 相比之下，互联网动态分享网络带宽。发送者竞争以尽可能快地通过网络获得它们的分组，并且网络交换机决定发送哪个分组（即，带宽分配）。这种方法有排队的缺点，但优点是它最大限度地利用了线路。线路成本固定，所以如果你更充分地利用它，通过该线路发送的每个字节都更便宜。 CPU也会出现类似的情况：如果你在多个线程之间动态共享每个CPU核，则有时候一个线程必须在另一个线程运行时等待操作系统的运行队列，因此线程可能被暂停不同的时间长度。但是，与为每个线程分配静态数量的CPU周期相比，这会更充分地利用硬件（请参阅第298页的“响应时间保证”）。更高的硬件利用率也是使用虚拟机的重要动机。 如果资源是静态分区的（例如，专用硬件和专用带宽分配），则在某些环境中可实现延迟保证。但是，这是以降低利用率为代价的。换句话说，它是更昂贵的。另一方面，动态资源分配下的多租户提供了更好的利用率，所以它更便宜，但它具有可变延迟的缺点。 网络中的可变延迟不是自然规律，而仅仅是成本/收益折衷的结果。 但是，此类服务质量目前尚未在多租户数据中心和公有云或通过互联网进行通信时可用。当前部署的技术无法让我们对网络的延迟或可靠性做出任何保证：我们必须假定网络拥塞，排队和无限延迟可能发生。因此，超时时间没有“正确”的值，需要通过实验确定。 未完待续。。。","raw":null,"content":null,"categories":[{"name":"分布式系统","slug":"分布式系统","permalink":"http://linbingdong.com/categories/分布式系统/"}],"tags":[{"name":"分布式系统","slug":"分布式系统","permalink":"http://linbingdong.com/tags/分布式系统/"}]},{"title":"PhxPaxos源码分析——Paxos算法实现","slug":"PhxPaxos源码分析——Paxos算法实现","date":"2017-11-20T16:00:00.000Z","updated":"2017-11-22T00:41:19.000Z","comments":true,"path":"2017/11/21/PhxPaxos源码分析——Paxos算法实现/","link":"","permalink":"http://linbingdong.com/2017/11/21/PhxPaxos源码分析——Paxos算法实现/","excerpt":"这篇主要来分析Paxos算法实现的部分，我想这应该也是读者最感兴趣的。在看这篇文章之前，如果之前对Paxos算法没有了解的童鞋可以看下这篇文章：Paxos算法原理与推导，相信了解Paxos算法后再来通过源码看算法实现应该会很酸爽。","text":"这篇主要来分析Paxos算法实现的部分，我想这应该也是读者最感兴趣的。在看这篇文章之前，如果之前对Paxos算法没有了解的童鞋可以看下这篇文章：Paxos算法原理与推导，相信了解Paxos算法后再来通过源码看算法实现应该会很酸爽。 Paxos算法中最重要的两个角色是Proposer和Acceptor。当然Leaner也很重要，特别是在PhxPaxos的实现中，Leaner具有重要的功能。但是因为《Paxos Made Simple》论文中主要还是Proposer和Acceptor，因此这篇文章还是以这两个角色为主，通过源码来回顾论文中Paxos算法的过程，同时也看看工程实现和论文的描述有什么区别。 这里先贴出Paxos算法的过程，方便大家对照接下来的工程实现。 Prepare阶段： (a) Proposer选择一个提案编号N，然后向半数以上的Acceptor发送编号为N的Prepare请求。 (b) 如果一个Acceptor收到一个编号为N的Prepare请求，且N大于该Acceptor已经响应过的所有Prepare请求的编号，那么它就会将它已经接受过的编号最大的提案（如果有的话）作为响应反馈给Proposer，同时该Acceptor承诺不再接受任何编号小于N的提案。 Accept阶段： (a) 如果Proposer收到半数以上Acceptor对其发出的编号为N的Prepare请求的响应，那么它就会发送一个针对[N,V]提案的Accept请求给半数以上的Acceptor。注意：V就是收到的响应中编号最大的提案的value，如果响应中不包含任何提案，那么V就由Proposer自己决定。 (b) 如果Acceptor收到一个针对编号为N的提案的Accept请求，只要该Acceptor没有对编号大于N的Prepare请求做出过响应，它就接受该提案。 Proposer因为Proposer需要维护或者说记录一些状态信息，包括自己的提案编号ProposalID、提出的Value、其他Proposer提出的最大的提案编号HighestOtherProposalID、Acceptor已经接受过的编号最大的提案的值等，因此这里专门有一个ProposerState类来管理这些信息。同样Acceptor也有一个AcceptorState类来管理Acceptor相关的信息。 先来看下ProposerState的定义： class ProposerState&#123;public: ProposerState(const Config * poConfig); ~ProposerState(); void Init(); void SetStartProposalID(const uint64_t llProposalID); void NewPrepare(); void AddPreAcceptValue(const BallotNumber &amp; oOtherPreAcceptBallot, const std::string &amp; sOtherPreAcceptValue); ///////////////////////// const uint64_t GetProposalID(); const std::string &amp; GetValue(); void SetValue(const std::string &amp; sValue); void SetOtherProposalID(const uint64_t llOtherProposalID); void ResetHighestOtherPreAcceptBallot();public: uint64_t m_llProposalID; uint64_t m_llHighestOtherProposalID; std::string m_sValue; BallotNumber m_oHighestOtherPreAcceptBallot; Config * m_poConfig;&#125;; 基本都是对这些信息的set跟get，很容易理解。直接来看Proposer类的定义： class Proposer : public Base&#123;public: Proposer( const Config * poConfig, const MsgTransport * poMsgTransport, const Instance * poInstance, const Learner * poLearner, const IOLoop * poIOLoop); ~Proposer(); //设置起始的ProposalID void SetStartProposalID(const uint64_t llProposalID); //初始化新的一轮Paxos过程，每一轮叫做一个Paxos Instance，每一轮确定一个值 virtual void InitForNewPaxosInstance(); //Proposer发起提案的入口函数。参数sValue即Proposer自己想提出的value，当然最终提出的value不一定是这个，需要根据Acceptor再Prepare阶段的回复来确定 int NewValue(const std::string &amp; sValue); //判断Proposer是否处于Prepare阶段或Accept阶段 bool IsWorking(); ///////////////////////////// //对应Paxos算法中的Prepare阶段 void Prepare(const bool bNeedNewBallot = true); //Prepare阶段等待Acceptor的回复，统计投票并确定是否进入Accept阶段 void OnPrepareReply(const PaxosMsg &amp; oPaxosMsg); //Prepare阶段被拒绝 void OnExpiredPrepareReply(const PaxosMsg &amp; oPaxosMsg); //对应Paxos算法中的Accept阶段 void Accept(); //Accept阶段等待Acceptor的回复，统计投票并确定值(Value)是否被选定(Chosen) void OnAcceptReply(const PaxosMsg &amp; oPaxosMsg); //Accept阶段被拒绝 void OnExpiredAcceptReply(const PaxosMsg &amp; oPaxosMsg); //Prepare阶段超时 void OnPrepareTimeout(); //Accept阶段超时 void OnAcceptTimeout(); //退出Prepare阶段 void ExitPrepare(); //退出Accept阶段 void ExitAccept(); //取消跳过Prepare阶段，也就是必须先Prepare阶段再Accept阶段 void CancelSkipPrepare(); ///////////////////////////// void AddPrepareTimer(const int iTimeoutMs = 0); void AddAcceptTimer(const int iTimeoutMs = 0);public: ProposerState m_oProposerState; MsgCounter m_oMsgCounter; Learner * m_poLearner; bool m_bIsPreparing; bool m_bIsAccepting; IOLoop * m_poIOLoop; uint32_t m_iPrepareTimerID; int m_iLastPrepareTimeoutMs; uint32_t m_iAcceptTimerID; int m_iLastAcceptTimeoutMs; uint64_t m_llTimeoutInstanceID; bool m_bCanSkipPrepare; bool m_bWasRejectBySomeone; TimeStat m_oTimeStat;&#125;; NewValue下面就从NewValue方法入手： int Proposer :: NewValue(const std::string &amp; sValue)&#123; BP-&gt;GetProposerBP()-&gt;NewProposal(sValue); if (m_oProposerState.GetValue().size() == 0) &#123; m_oProposerState.SetValue(sValue); &#125; m_iLastPrepareTimeoutMs = START_PREPARE_TIMEOUTMS; m_iLastAcceptTimeoutMs = START_ACCEPT_TIMEOUTMS; //如果可以跳过Prepare阶段并且没有被Acceptor拒绝过，则直接进入Accept阶段 if (m_bCanSkipPrepare &amp;&amp; !m_bWasRejectBySomeone) &#123; BP-&gt;GetProposerBP()-&gt;NewProposalSkipPrepare(); PLGHead(\"skip prepare, directly start accept\"); Accept(); &#125; //否则先进入Prepare阶段 else &#123; //if not reject by someone, no need to increase ballot Prepare(m_bWasRejectBySomeone); &#125; return 0;&#125; 这里可以直接进入Accept阶段的前提是该Proposer已经发起过Prepare请求且得到半数以上的同意（即通过了Prepare阶段），并且没有被任何Acceptor拒绝（说明没有Acceptor响应过比该Proposer的提案编号更高的提案）。那么，什么情况下可以跳过Prepare请求呢，这里应该对应的是选出一个master的情况？相当于raft里的leader？ Prepare接下来直接看Prepare阶段： void Proposer :: Prepare(const bool bNeedNewBallot)&#123; PLGHead(\"START Now.InstanceID %lu MyNodeID %lu State.ProposalID %lu State.ValueLen %zu\", GetInstanceID(), m_poConfig-&gt;GetMyNodeID(), m_oProposerState.GetProposalID(), m_oProposerState.GetValue().size()); BP-&gt;GetProposerBP()-&gt;Prepare(); m_oTimeStat.Point(); ExitAccept(); //表明Proposer正处于Prepare阶段 m_bIsPreparing = true; //不能跳过Prepare阶段 m_bCanSkipPrepare = false; //目前还未被任意一个Acceptor拒绝 m_bWasRejectBySomeone = false; m_oProposerState.ResetHighestOtherPreAcceptBallot(); //如果需要产生新的投票，就调用NewPrepare产生新的ProposalID，新的ProposalID为当前已知的最大ProposalID+1 if (bNeedNewBallot) &#123; m_oProposerState.NewPrepare(); &#125; PaxosMsg oPaxosMsg; //设置Prepare消息的各个字段 oPaxosMsg.set_msgtype(MsgType_PaxosPrepare); oPaxosMsg.set_instanceid(GetInstanceID()); oPaxosMsg.set_nodeid(m_poConfig-&gt;GetMyNodeID()); oPaxosMsg.set_proposalid(m_oProposerState.GetProposalID()); //MsgCount是专门用来统计票数的，根据计算的结果确定是否通过Prepare阶段或者Accept阶段 m_oMsgCounter.StartNewRound(); //Prepare超时定时器 AddPrepareTimer(); PLGHead(\"END OK\"); //将Prepare消息发送到各个节点 BroadcastMessage(oPaxosMsg);&#125; Proposer在Prepare阶段主要做了这么几件事： 重置各个状态位，表明当前正处于Prepare阶段。 获取提案编号ProposalID。当bNeedNewBallot为true时需要将ProposalID+1。否则沿用之前的ProposalID。bNeedNewBallot是在NewValue中调用Prepare方法时传入的m_bWasRejectBySomeone参数。也就是如果之前没有被任何Acceptor拒绝（说明还没有明确出现更大的ProposalID），则不需要获取新的ProposalID。对应的场景是Prepare阶段超时了，在超时时间内没有收到过半Acceptor同意的消息，因此需要重新执行Prepare阶段，此时只需要沿用原来的ProposalID即可。 发送Prepare请求。该请求PaxosMsg是Protocol Buffer定义的一个message，包含MsgType、InstanceID、NodeID、ProposalID等字段。在BroadcastMessage(oPaxosMsg)中还会将oPaxosMsg序列化后才发送出去。 PaxosMsg的定义如下，Prepare和Accept阶段Proposer和Acceptor的所有消息都用PaxosMsg来表示： message PaxosMsg&#123; required int32 MsgType = 1; optional uint64 InstanceID = 2; optional uint64 NodeID = 3; optional uint64 ProposalID = 4; optional uint64 ProposalNodeID = 5; optional bytes Value = 6; optional uint64 PreAcceptID = 7; optional uint64 PreAcceptNodeID = 8; optional uint64 RejectByPromiseID = 9; optional uint64 NowInstanceID = 10; optional uint64 MinChosenInstanceID = 11; optional uint32 LastChecksum = 12; optional uint32 Flag = 13; optional bytes SystemVariables = 14; optional bytes MasterVariables = 15;&#125;; OnPrepareReplyProposer发出Prepare请求后就开始等待Acceptor的回复。当Proposer所在节点收到PaxosPrepareReply消息后，就会调用Proposer的OnPrepareReply(oPaxosMsg)，其中oPaxosMsg是Acceptor回复的消息。 void Proposer :: OnPrepareReply(const PaxosMsg &amp; oPaxosMsg)&#123; PLGHead(\"START Msg.ProposalID %lu State.ProposalID %lu Msg.from_nodeid %lu RejectByPromiseID %lu\", oPaxosMsg.proposalid(), m_oProposerState.GetProposalID(), oPaxosMsg.nodeid(), oPaxosMsg.rejectbypromiseid()); BP-&gt;GetProposerBP()-&gt;OnPrepareReply(); //如果Proposer不是在Prepare阶段，则忽略该消息 if (!m_bIsPreparing) &#123; BP-&gt;GetProposerBP()-&gt;OnPrepareReplyButNotPreparing(); //PLGErr(\"Not preparing, skip this msg\"); return; &#125; //如果ProposalID不同，也忽略 if (oPaxosMsg.proposalid() != m_oProposerState.GetProposalID()) &#123; BP-&gt;GetProposerBP()-&gt;OnPrepareReplyNotSameProposalIDMsg(); //PLGErr(\"ProposalID not same, skip this msg\"); return; &#125; //加入一个收到的消息，用于MsgCounter统计 m_oMsgCounter.AddReceive(oPaxosMsg.nodeid()); //如果该消息不是拒绝，即Acceptor同意本次Prepare请求 if (oPaxosMsg.rejectbypromiseid() == 0) &#123; BallotNumber oBallot(oPaxosMsg.preacceptid(), oPaxosMsg.preacceptnodeid()); PLGDebug(\"[Promise] PreAcceptedID %lu PreAcceptedNodeID %lu ValueSize %zu\", oPaxosMsg.preacceptid(), oPaxosMsg.preacceptnodeid(), oPaxosMsg.value().size()); //加入MsgCounter用于统计投票 m_oMsgCounter.AddPromiseOrAccept(oPaxosMsg.nodeid()); //将Acceptor返回的它接受过的编号最大的提案记录下来（如果有的话），用于确定Accept阶段的Value m_oProposerState.AddPreAcceptValue(oBallot, oPaxosMsg.value()); &#125; //Acceptor拒绝了Prepare请求 else &#123; PLGDebug(\"[Reject] RejectByPromiseID %lu\", oPaxosMsg.rejectbypromiseid()); //同样也要记录到MsgCounter用于统计投票 m_oMsgCounter.AddReject(oPaxosMsg.nodeid()); //记录被Acceptor拒绝过，待会儿如果重新进入Prepare阶段的话就需要获取更大的ProposalID m_bWasRejectBySomeone = true; //记录下别的Proposer提出的更大的ProposalID。这样重新发起Prepare请求时才知道需要用多大的ProposalID m_oProposerState.SetOtherProposalID(oPaxosMsg.rejectbypromiseid()); &#125; //本次Prepare请求通过了。也就是得到了半数以上Acceptor的同意 if (m_oMsgCounter.IsPassedOnThisRound()) &#123; int iUseTimeMs = m_oTimeStat.Point(); BP-&gt;GetProposerBP()-&gt;PreparePass(iUseTimeMs); PLGImp(\"[Pass] start accept, usetime %dms\", iUseTimeMs); m_bCanSkipPrepare = true; //进入Accept阶段 Accept(); &#125; //本次Prepare请求没有通过 else if (m_oMsgCounter.IsRejectedOnThisRound() || m_oMsgCounter.IsAllReceiveOnThisRound()) &#123; BP-&gt;GetProposerBP()-&gt;PrepareNotPass(); PLGImp(\"[Not Pass] wait 30ms and restart prepare\"); //随机等待一段时间后重新发起Prepare请求 AddPrepareTimer(OtherUtils::FastRand() % 30 + 10); &#125; PLGHead(\"END\");&#125; 该阶段Proposer主要做了以下事情： 判断消息是否有效。包括ProposalID是否相同，自身是否处于Prepare阶段等。因为网络是不可靠的，有些消息可能延迟很久，等收到的时候已经不需要了，所以需要做这些判断。 将收到的消息加入MsgCounter用于统计。 根据收到的消息更新自身状态。包括Acceptor承诺过的ProposalID，以及Acceptor接受过的编号最大的提案等。 根据MsgCounter统计的Acceptor投票结果决定是进入Acceptor阶段还是重新发起Prepare请求。这里如果判断需要重新发起Prepare请求的话，也不是立即进行，而是等待一段随机的时间，这样做的好处是减少不同Proposer之间的冲突，采取的策略跟raft中leader选举冲突时在一段随机的选举超时时间后重新发起选举的做法类似。 注：这里跟Paxos算法中提案编号对应的并不是ProposalID，而是BallotNumber。BallotNumber由ProposalID和NodeID组成。还实现了运算符重载。如果ProposalID大，则BallotNumber（即提案编号）大。在ProposalID相同的情况下，NodeID大的BallotNumber大。 Accept接下来Proposer就进入Accept阶段： void Proposer :: Accept()&#123; PLGHead(\"START ProposalID %lu ValueSize %zu ValueLen %zu\", m_oProposerState.GetProposalID(), m_oProposerState.GetValue().size(), m_oProposerState.GetValue().size()); BP-&gt;GetProposerBP()-&gt;Accept(); m_oTimeStat.Point(); ExitPrepare(); m_bIsAccepting = true; //设置Accept请求的消息内容 PaxosMsg oPaxosMsg; oPaxosMsg.set_msgtype(MsgType_PaxosAccept); oPaxosMsg.set_instanceid(GetInstanceID()); oPaxosMsg.set_nodeid(m_poConfig-&gt;GetMyNodeID()); oPaxosMsg.set_proposalid(m_oProposerState.GetProposalID()); oPaxosMsg.set_value(m_oProposerState.GetValue()); oPaxosMsg.set_lastchecksum(GetLastChecksum()); m_oMsgCounter.StartNewRound(); AddAcceptTimer(); PLGHead(\"END\"); //发给各个节点 BroadcastMessage(oPaxosMsg, BroadcastMessage_Type_RunSelf_Final);&#125; Accept请求中PaxosMsg里的Value是这样确定的：如果Prepare阶段有Acceptor的回复中带有提案值，则该Value为所有的Acceptor的回复中，编号最大的提案的值。否则就是Proposer在最初调用NewValue时传入的值。 OnAcceptReplyvoid Proposer :: OnAcceptReply(const PaxosMsg &amp; oPaxosMsg)&#123; PLGHead(\"START Msg.ProposalID %lu State.ProposalID %lu Msg.from_nodeid %lu RejectByPromiseID %lu\", oPaxosMsg.proposalid(), m_oProposerState.GetProposalID(), oPaxosMsg.nodeid(), oPaxosMsg.rejectbypromiseid()); BP-&gt;GetProposerBP()-&gt;OnAcceptReply(); if (!m_bIsAccepting) &#123; //PLGErr(\"Not proposing, skip this msg\"); BP-&gt;GetProposerBP()-&gt;OnAcceptReplyButNotAccepting(); return; &#125; if (oPaxosMsg.proposalid() != m_oProposerState.GetProposalID()) &#123; //PLGErr(\"ProposalID not same, skip this msg\"); BP-&gt;GetProposerBP()-&gt;OnAcceptReplyNotSameProposalIDMsg(); return; &#125; m_oMsgCounter.AddReceive(oPaxosMsg.nodeid()); if (oPaxosMsg.rejectbypromiseid() == 0) &#123; PLGDebug(\"[Accept]\"); m_oMsgCounter.AddPromiseOrAccept(oPaxosMsg.nodeid()); &#125; else &#123; PLGDebug(\"[Reject]\"); m_oMsgCounter.AddReject(oPaxosMsg.nodeid()); m_bWasRejectBySomeone = true; m_oProposerState.SetOtherProposalID(oPaxosMsg.rejectbypromiseid()); &#125; if (m_oMsgCounter.IsPassedOnThisRound()) &#123; int iUseTimeMs = m_oTimeStat.Point(); BP-&gt;GetProposerBP()-&gt;AcceptPass(iUseTimeMs); PLGImp(\"[Pass] Start send learn, usetime %dms\", iUseTimeMs); ExitAccept(); //让Leaner学习被选定（Chosen）的值 m_poLearner-&gt;ProposerSendSuccess(GetInstanceID(), m_oProposerState.GetProposalID()); &#125; else if (m_oMsgCounter.IsRejectedOnThisRound() || m_oMsgCounter.IsAllReceiveOnThisRound()) &#123; BP-&gt;GetProposerBP()-&gt;AcceptNotPass(); PLGImp(\"[Not pass] wait 30ms and Restart prepare\"); AddAcceptTimer(OtherUtils::FastRand() % 30 + 10); &#125; PLGHead(\"END\");&#125; 这里跟OnPrepareReply的过程基本一致，因此就不加太多注释了。比较大的区别在于最后如果过半的Acceptor接受了该Accept请求，则说明该Value被选定（Chosen）了，就发送消息，让每个节点上的Learner学习该Value。因为Leaner不是本文的重点，这里就不详细介绍了。 AcceptorAcceptor的逻辑比Proposer更简单。同样先看它的定义： class Acceptor : public Base&#123;public: Acceptor( const Config * poConfig, const MsgTransport * poMsgTransport, const Instance * poInstance, const LogStorage * poLogStorage); ~Acceptor(); virtual void InitForNewPaxosInstance(); int Init(); AcceptorState * GetAcceptorState(); //Prepare阶段回复Prepare请求 int OnPrepare(const PaxosMsg &amp; oPaxosMsg); //Accept阶段回复Accept请求 void OnAccept(const PaxosMsg &amp; oPaxosMsg);//private: AcceptorState m_oAcceptorState;&#125;; OnPrepareOnPrepare用于处理收到的Prepare请求，逻辑如下： int Acceptor :: OnPrepare(const PaxosMsg &amp; oPaxosMsg)&#123; PLGHead(\"START Msg.InstanceID %lu Msg.from_nodeid %lu Msg.ProposalID %lu\", oPaxosMsg.instanceid(), oPaxosMsg.nodeid(), oPaxosMsg.proposalid()); BP-&gt;GetAcceptorBP()-&gt;OnPrepare(); PaxosMsg oReplyPaxosMsg; oReplyPaxosMsg.set_instanceid(GetInstanceID()); oReplyPaxosMsg.set_nodeid(m_poConfig-&gt;GetMyNodeID()); oReplyPaxosMsg.set_proposalid(oPaxosMsg.proposalid()); oReplyPaxosMsg.set_msgtype(MsgType_PaxosPrepareReply); //构造接收到的Prepare请求里的提案编号 BallotNumber oBallot(oPaxosMsg.proposalid(), oPaxosMsg.nodeid()); //提案编号大于承诺过的提案编号 if (oBallot &gt;= m_oAcceptorState.GetPromiseBallot()) &#123; PLGDebug(\"[Promise] State.PromiseID %lu State.PromiseNodeID %lu \" \"State.PreAcceptedID %lu State.PreAcceptedNodeID %lu\", m_oAcceptorState.GetPromiseBallot().m_llProposalID, m_oAcceptorState.GetPromiseBallot().m_llNodeID, m_oAcceptorState.GetAcceptedBallot().m_llProposalID, m_oAcceptorState.GetAcceptedBallot().m_llNodeID); //返回之前接受过的提案的编号 oReplyPaxosMsg.set_preacceptid(m_oAcceptorState.GetAcceptedBallot().m_llProposalID); oReplyPaxosMsg.set_preacceptnodeid(m_oAcceptorState.GetAcceptedBallot().m_llNodeID); //如果接受过的提案编号大于0（&lt;=0说明没有接受过提案），则设置接受过的提案的Value if (m_oAcceptorState.GetAcceptedBallot().m_llProposalID &gt; 0) &#123; oReplyPaxosMsg.set_value(m_oAcceptorState.GetAcceptedValue()); &#125; //更新承诺的提案编号为新的提案编号（因为新的提案编号更大） m_oAcceptorState.SetPromiseBallot(oBallot); //信息持久化 int ret = m_oAcceptorState.Persist(GetInstanceID(), GetLastChecksum()); if (ret != 0) &#123; BP-&gt;GetAcceptorBP()-&gt;OnPreparePersistFail(); PLGErr(\"Persist fail, Now.InstanceID %lu ret %d\", GetInstanceID(), ret); return -1; &#125; BP-&gt;GetAcceptorBP()-&gt;OnPreparePass(); &#125; //提案编号小于承诺过的提案编号，需要拒绝 else &#123; BP-&gt;GetAcceptorBP()-&gt;OnPrepareReject(); PLGDebug(\"[Reject] State.PromiseID %lu State.PromiseNodeID %lu\", m_oAcceptorState.GetPromiseBallot().m_llProposalID, m_oAcceptorState.GetPromiseBallot().m_llNodeID); //拒绝该Prepare请求，并返回承诺过的ProposalID oReplyPaxosMsg.set_rejectbypromiseid(m_oAcceptorState.GetPromiseBallot().m_llProposalID); &#125; nodeid_t iReplyNodeID = oPaxosMsg.nodeid(); PLGHead(\"END Now.InstanceID %lu ReplyNodeID %lu\", GetInstanceID(), oPaxosMsg.nodeid());; //向发出Prepare请求的Proposer回复消息 SendMessage(iReplyNodeID, oReplyPaxosMsg); return 0;&#125; OnAccept再来看看OnAccept： void Acceptor :: OnAccept(const PaxosMsg &amp; oPaxosMsg)&#123; PLGHead(\"START Msg.InstanceID %lu Msg.from_nodeid %lu Msg.ProposalID %lu Msg.ValueLen %zu\", oPaxosMsg.instanceid(), oPaxosMsg.nodeid(), oPaxosMsg.proposalid(), oPaxosMsg.value().size()); BP-&gt;GetAcceptorBP()-&gt;OnAccept(); PaxosMsg oReplyPaxosMsg; oReplyPaxosMsg.set_instanceid(GetInstanceID()); oReplyPaxosMsg.set_nodeid(m_poConfig-&gt;GetMyNodeID()); oReplyPaxosMsg.set_proposalid(oPaxosMsg.proposalid()); oReplyPaxosMsg.set_msgtype(MsgType_PaxosAcceptReply); BallotNumber oBallot(oPaxosMsg.proposalid(), oPaxosMsg.nodeid()); //提案编号不小于承诺过的提案编号（注意：这里是“&gt;=”，而再OnPrepare中是“&gt;”，可以先思考下为什么），需要接受该提案 if (oBallot &gt;= m_oAcceptorState.GetPromiseBallot()) &#123; PLGDebug(\"[Promise] State.PromiseID %lu State.PromiseNodeID %lu \" \"State.PreAcceptedID %lu State.PreAcceptedNodeID %lu\", m_oAcceptorState.GetPromiseBallot().m_llProposalID, m_oAcceptorState.GetPromiseBallot().m_llNodeID, m_oAcceptorState.GetAcceptedBallot().m_llProposalID, m_oAcceptorState.GetAcceptedBallot().m_llNodeID); //更新承诺的提案编号；接受的提案编号、提案值 m_oAcceptorState.SetPromiseBallot(oBallot); m_oAcceptorState.SetAcceptedBallot(oBallot); m_oAcceptorState.SetAcceptedValue(oPaxosMsg.value()); //信息持久化 int ret = m_oAcceptorState.Persist(GetInstanceID(), GetLastChecksum()); if (ret != 0) &#123; BP-&gt;GetAcceptorBP()-&gt;OnAcceptPersistFail(); PLGErr(\"Persist fail, Now.InstanceID %lu ret %d\", GetInstanceID(), ret); return; &#125; BP-&gt;GetAcceptorBP()-&gt;OnAcceptPass(); &#125; //需要拒绝该提案 else &#123; BP-&gt;GetAcceptorBP()-&gt;OnAcceptReject(); PLGDebug(\"[Reject] State.PromiseID %lu State.PromiseNodeID %lu\", m_oAcceptorState.GetPromiseBallot().m_llProposalID, m_oAcceptorState.GetPromiseBallot().m_llNodeID); //拒绝的消息中附上承诺过的ProposalID oReplyPaxosMsg.set_rejectbypromiseid(m_oAcceptorState.GetPromiseBallot().m_llProposalID); &#125; nodeid_t iReplyNodeID = oPaxosMsg.nodeid(); PLGHead(\"END Now.InstanceID %lu ReplyNodeID %lu\", GetInstanceID(), oPaxosMsg.nodeid()); //将响应发送给Proposer SendMessage(iReplyNodeID, oReplyPaxosMsg);&#125; 结语通过阅读源码可以发现，整个PhxPaxos完全基于Lamport的《Paxos Made Simple》进行工程化，没有进行任何算法变种。这对于学习Paxos算法的人来说真的是一笔宝贵的财富，所以如果对Paxos算法感兴趣，应该深入地去阅读PhxPaxos的源码，相信看完后大家对Paxos会有更深的理解。同时我们也发现，在工程实现上还是有很多细节需要注意，这比单纯理解算法要难得多。","raw":null,"content":null,"categories":[{"name":"源码分析","slug":"源码分析","permalink":"http://linbingdong.com/categories/源码分析/"}],"tags":[{"name":"PhxPaxos","slug":"PhxPaxos","permalink":"http://linbingdong.com/tags/PhxPaxos/"},{"name":"Paxos","slug":"Paxos","permalink":"http://linbingdong.com/tags/Paxos/"}]},{"title":"PhxPaxos源码分析——网络","slug":"PhxPaxos源码分析——网络","date":"2017-11-19T16:00:00.000Z","updated":"2017-11-20T10:56:44.000Z","comments":true,"path":"2017/11/20/PhxPaxos源码分析——网络/","link":"","permalink":"http://linbingdong.com/2017/11/20/PhxPaxos源码分析——网络/","excerpt":"了解分布式系统的童鞋肯定听过Paxos算法的大名。Paxos算法以晦涩难懂著称，其工程实现更难。目前，号称在工程上实现了Paxos算法的应该只有Google、阿里和腾讯。然而，只有腾讯的微信团队真正将代码开源出来，他们将Paxos算法的实现封装成了一个Paxos库，大家可以基于该库实现自己想要的功能，比如用于master选举，或者甚至利用它来实现一个分布式KV数据库等。","text":"了解分布式系统的童鞋肯定听过Paxos算法的大名。Paxos算法以晦涩难懂著称，其工程实现更难。目前，号称在工程上实现了Paxos算法的应该只有Google、阿里和腾讯。然而，只有腾讯的微信团队真正将代码开源出来，他们将Paxos算法的实现封装成了一个Paxos库，大家可以基于该库实现自己想要的功能，比如用于master选举，或者甚至利用它来实现一个分布式KV数据库等。 之前就对Paxos很感兴趣，但是一直没看过实现的代码，这次微信开源了PhxPaxos后终于有机会深入地了解Paxos的实现细节。在这里感谢微信团队。感谢PhxPaxos的作者。让我们一起来领略Paxos的魅力吧。 本次的源码分析先从网络部分开始。因为在分布式系统中不可避免会涉及到不同节点以及相同节点上不同进程之间的通信。因此网络部分也是至关重要，所以就先把网络单独拿出来看，接下来再去看Paxos算法的实现部分。 概览源码的include/phxpaxos目录下是公共头文件。include/phpaxos/network.h 是网络模块的抽象函数，如果用户想使用自己的网络协议，可以通过重写这些函数实现网络模块的自定义。 我们先来看下network.h的内容： namespace phxpaxos&#123;//You can use your own network to make paxos communicate. :)class Node;class NetWork&#123;public: NetWork(); virtual ~NetWork() &#123;&#125; //Network must not send/recieve any message before paxoslib called this funtion. virtual void RunNetWork() = 0; //If paxoslib call this function, network need to stop receive any message. virtual void StopNetWork() = 0; virtual int SendMessageTCP(const int iGroupIdx, const std::string &amp; sIp, const int iPort, const std::string &amp; sMessage) = 0; virtual int SendMessageUDP(const int iGroupIdx, const std::string &amp; sIp, const int iPort, const std::string &amp; sMessage) = 0; //When receive a message, call this funtion. //This funtion is async, just enqueue an return. int OnReceiveMessage(const char * pcMessage, const int iMessageLen);private: friend class Node; Node * m_poNode;&#125;; &#125; 这几个函数的作用从名字就可以看出来。而且都是虚函数，即需要重写这些函数。在PhxPaxos中，提供了一个默认的网络模块，就是继承了NetWork类。该类的名字叫DFNetWork，DF应该就是default的缩写了。如下： namespace phxpaxos &#123;class DFNetWork : public NetWork&#123;public: DFNetWork(); virtual ~DFNetWork(); int Init(const std::string &amp; sListenIp, const int iListenPort, const int iIOThreadCount); void RunNetWork(); void StopNetWork(); int SendMessageTCP(const int iGroupIdx, const std::string &amp; sIp, const int iPort, const std::string &amp; sMessage); int SendMessageUDP(const int iGroupIdx, const std::string &amp; sIp, const int iPort, const std::string &amp; sMessage);private: UDPRecv m_oUDPRecv; UDPSend m_oUDPSend; TcpIOThread m_oTcpIOThread;&#125;;&#125; 该类的私有成员里有UDPRecv、UDPSend和TcpIOThread三个类的对象，这三个类分别用于接收UDP消息、发送UDP消息以及收发TCP消息。 Init方法就是将UDPRecv、UDPSend和TcpIOThread分别初始化: int DFNetWork :: Init(const std::string &amp; sListenIp, const int iListenPort, const int iIOThreadCount) &#123; //初始化UDPSend int ret = m_oUDPSend.Init(); if (ret != 0) &#123; return ret; &#125; //初始化UDPRecv ret = m_oUDPRecv.Init(iListenPort); if (ret != 0) &#123; return ret; &#125; //初始化TCP ret = m_oTcpIOThread.Init(sListenIp, iListenPort, iIOThreadCount); if (ret != 0) &#123; PLErr(\"m_oTcpIOThread Init fail, ret %d\", ret); return ret; &#125; return 0;&#125; 具体的初始化过程就是调用socket的api。以UDPRecv为例，就是创建socket、设定端口、设置socket属性（如端口可重用）最后绑定端口。如下： int UDPRecv :: Init(const int iPort)&#123; //创建socket，获得socket fd if ((m_iSockFD = socket(AF_INET, SOCK_DGRAM, 0)) &lt; 0) &#123; return -1; &#125; struct sockaddr_in addr; memset(&amp;addr, 0, sizeof(addr)); addr.sin_family = AF_INET; addr.sin_port = htons(iPort); //设定端口 addr.sin_addr.s_addr = htonl(INADDR_ANY); int enable = 1; //设定socket属性，端口可重用 setsockopt(m_iSockFD, SOL_SOCKET, SO_REUSEADDR, &amp;enable, sizeof(int)); //绑定，用于监听 if (bind(m_iSockFD, (struct sockaddr *)&amp;addr, sizeof(addr)) &lt; 0) &#123; return -1; &#125; return 0;&#125; RunNetWork就是将UDPRecv、UDPSend和TcpIOThread分别运行起来： void DFNetWork :: RunNetWork()&#123; //UDPSend和UDPRecv都是调用Thread的start方法 m_oUDPSend.start(); m_oUDPRecv.start(); //TCP的Start是封装过的 m_oTcpIOThread.Start();&#125; TcpIOThread的Start()实际执行的代码如下，分别启动了TcpAcceptor、TcpWrite和TcpRead： void TcpIOThread :: Start()&#123; m_oTcpAcceptor.start(); for (auto &amp; poTcpWrite : m_vecTcpWrite) &#123; poTcpWrite-&gt;start(); &#125; for (auto &amp; poTcpRead : m_vecTcpRead) &#123; poTcpRead-&gt;start(); &#125; m_bIsStarted = true;&#125; StopNetWork就是将UDPRecv、UDPSend和TcpIOThread停止。 SendMessageTCP就是将消息用TCP发送： int DFNetWork :: SendMessageTCP(const int iGroupIdx, const std::string &amp; sIp, const int iPort, const std::string &amp; sMessage)&#123; return m_oTcpIOThread.AddMessage(iGroupIdx, sIp, iPort, sMessage);&#125; SendMessageUDP就是将消息用UDP发送：int DFNetWork :: SendMessageUDP(const int iGroupIdx, const std::string &amp; sIp, const int iPort, const std::string &amp; sMessage)&#123; return m_oUDPSend.AddMessage(sIp, iPort, sMessage);&#125; UDPUDPSend前面SendMessageUDP调用了m_oUDPSend.AddMessage。这里的UDPSend维护了一个发送队列，如下： Queue&lt;QueueData *&gt; m_oSendQueue; m_oUDPSend.AddMessage就是将消息加入到UDP的m_oSendQueue中。 然后UDPSend在run方法中一直循环将m_oSendQueue中的消息发送出去： void UDPSend :: run()&#123; m_bIsStarted = true; while(true) &#123; QueueData * poData = nullptr; //同步，线程安全 m_oSendQueue.lock(); bool bSucc = m_oSendQueue.peek(poData, 1000); if (bSucc) &#123; //取出队头消息 m_oSendQueue.pop(); &#125; m_oSendQueue.unlock(); if (poData != nullptr) &#123; //将消息发送出去 SendMessage(poData-&gt;m_sIP, poData-&gt;m_iPort, poData-&gt;m_sMessage); delete poData; &#125; if (m_bIsEnd) &#123; PLHead(\"UDPSend [END]\"); return; &#125; &#125;&#125; 因此UDPSend就是把消息加入到消息队列，然后循环将消息队列里的消息发送出去。 UDPRecv接下来看看UDPRecv。UDPRecv的初始化前面已经看过了，就是简单的获得socket fd，设定sockaddr_in，设置socket属性最后将socket fd和sockaddr_in绑定用于监听。 主要来看看UDPRecv的run方法。这里主要用了I/O多路复用中的poll，注册了一个pollfd，该pollfd的fd即之前创建的绑定了端口的socket fd，events为POLLIN，表示监听数据可读事件，如果有数据可读了，则调用recvfrom读入数据。最后调用OnReceiveMessage将消息添加到当前instance的IoLoop中： void UDPRecv :: run()&#123; m_bIsStarted = true; char sBuffer[65536] = &#123;0&#125;; struct sockaddr_in addr; socklen_t addr_len = sizeof(struct sockaddr_in); memset(&amp;addr, 0, sizeof(addr)); while(true) &#123; if (m_bIsEnd) &#123; PLHead(\"UDPRecv [END]\"); return; &#125; struct pollfd fd; int ret; fd.fd = m_iSockFD; //注册POLLIN事件 fd.events = POLLIN; //调用poll检查是否有数据可读 ret = poll(&amp;fd, 1, 500); if (ret == 0 || ret == -1) &#123; continue; &#125; //将接收到的数据放入sBuffer中 int iRecvLen = recvfrom(m_iSockFD, sBuffer, sizeof(sBuffer), 0, (struct sockaddr *)&amp;addr, &amp;addr_len); BP-&gt;GetNetworkBP()-&gt;UDPReceive(iRecvLen); if (iRecvLen &gt; 0) &#123; //这里会依次调用Node和Instance的OnReceiveMessage方法，最后将消息加入到Instance的IoLoop中 m_poDFNetWork-&gt;OnReceiveMessage(sBuffer, iRecvLen); &#125; &#125;&#125; TCPTcpIOThread接下来看看收发TCP消息的TcpIOThread： class TcpIOThread &#123;public: TcpIOThread(NetWork * poNetWork); ~TcpIOThread(); //用于初始化TcpAcceptor以及iIOThreadCount个m_vecTcpRead和m_vecTcpWrite int Init(const std::string &amp; sListenIp, const int iListenPort, const int iIOThreadCount); //启动TcpAcceptor用于监听以及所有的m_vecTcpRead和m_vecTcpWrite用于读写消息 void Start(); //停止TcpAcceptor和所有的m_vecTcpRead及m_vecTcpWrite void Stop(); //将消息加入到特定TcpWrite的消息队列中 int AddMessage(const int iGroupIdx, const std::string &amp; sIP, const int iPort, const std::string &amp; sMessage);private: NetWork * m_poNetWork; TcpAcceptor m_oTcpAcceptor; std::vector&lt;TcpRead *&gt; m_vecTcpRead; std::vector&lt;TcpWrite *&gt; m_vecTcpWrite; bool m_bIsStarted;&#125;; TcpRead类似于前面讲的UDPRecv，TcpWrite类似于于UDPSend。严格来讲，TcpAcceptor + TcpRead才是UDPRecv。这里把TcpAcceptor单独抽出来，专门用于监听连接请求并建立连接。TcpRead只需要负责读消息就行。 TcpAcceptor我们来看看TcpAcceptor： class TcpAcceptor : public Thread&#123;public: TcpAcceptor(); ~TcpAcceptor(); //监听端口 void Listen(const std::string &amp; sListenIP, const int iListenPort); //一直while循环，监听连接事件并建立连接获得fd，然后添加事件到EventLoop中 void run(); void Stop(); void AddEventLoop(EventLoop * poEventLoop); void AddEvent(int iFD, SocketAddress oAddr);private: //服务端的socket，用于监听 ServerSocket m_oSocket; std::vector&lt;EventLoop *&gt; m_vecEventLoop;private: bool m_bIsEnd; bool m_bIsStarted;&#125;; 这里主要来看下run方法： void TcpAcceptor :: run()&#123; m_bIsStarted = true; PLHead(\"start accept...\"); m_oSocket.setAcceptTimeout(500); m_oSocket.setNonBlocking(true); while (true) &#123; struct pollfd pfd; int ret; pfd.fd = m_oSocket.getSocketHandle(); //注册事件 pfd.events = POLLIN; //等待事件到来 ret = poll(&amp;pfd, 1, 500); if (ret != 0 &amp;&amp; ret != -1) &#123; SocketAddress oAddr; int fd = -1; try &#123; //建立连接，获得fd。这里的acceptfd对accept进行了简单的封装 fd = m_oSocket.acceptfd(&amp;oAddr); &#125; catch(...) &#123; fd = -1; &#125; if (fd &gt;= 0) &#123; BP-&gt;GetNetworkBP()-&gt;TcpAcceptFd(); PLImp(\"accepted!, fd %d ip %s port %d\", fd, oAddr.getHost().c_str(), oAddr.getPort()); //添加事件 AddEvent(fd, oAddr); &#125; &#125; if (m_bIsEnd) &#123; PLHead(\"TCP.Acceptor [END]\"); return; &#125; &#125;&#125; 再看看AddEvent方法： void TcpAcceptor :: AddEvent(int iFD, SocketAddress oAddr)&#123; EventLoop * poMinActiveEventLoop = nullptr; int iMinActiveEventCount = 1 &lt;&lt; 30; for (auto &amp; poEventLoop : m_vecEventLoop) &#123; int iActiveCount = poEventLoop-&gt;GetActiveEventCount(); if (iActiveCount &lt; iMinActiveEventCount) &#123; iMinActiveEventCount = iActiveCount; poMinActiveEventLoop = poEventLoop; &#125; &#125;oAddr.getPort()); poMinActiveEventLoop-&gt;AddEvent(iFD, oAddr);&#125; 即找到活跃数最少的EventLoop，将事件添加到该EventLoop中。这里应该是为了负载均衡，防止有些线程工作量很大，有些则很空闲。 具体EventLoop的AddEvent就是将事件加入到FDQueue中，如下： void EventLoop :: AddEvent(int iFD, SocketAddress oAddr)&#123; std::lock_guard&lt;std::mutex&gt; oLockGuard(m_oMutex); m_oFDQueue.push(make_pair(iFD, oAddr));&#125; 到这里TcpAcceptor的作用及实现基本就很清晰了。 TcpRead先来看看TcpRead类的定义： class TcpRead : public Thread&#123;public: TcpRead(NetWork * poNetWork); ~TcpRead(); int Init(); void run(); void Stop(); EventLoop * GetEventLoop();private: EventLoop m_oEventLoop;&#125;; 这里的成员变量是一个EventLoop对象。通过源码发现，Init、run、Stop方法其实都是调用了m_oEventLoop相应的方法，如下： int TcpRead :: Init()&#123; return m_oEventLoop.Init(20480);&#125;void TcpRead :: run()&#123; m_oEventLoop.StartLoop();&#125;void TcpRead :: Stop()&#123; m_oEventLoop.Stop(); join(); PLHead(\"TcpReadThread [END]\");&#125; 因此主要来看下EventLoop。 首先说下Event。PhxPaxos在TCP这块主要用了I/O多路复用中的epoll。这里主要将数据和通知等都封装成Event，然后由TcpWrite和TcpRead的EventLoop去执行。PhxPaxos中的Event包含两个子类，分别是MessageEvent和Notify。其中MessageEvent主要用于数据的读写；而Notify主要用于通知事件发生。这里的Notify基于管道pipe和EPOLLIN事件来实现，可以通过Notify的Init方法看出： int Notify :: Init()&#123; //m_iPipeFD是一个长度为2的int数组，用于存放管道两端的socket fd int ret = pipe(m_iPipeFD); if (ret != 0) &#123; PLErr(\"create pipe fail, ret %d\", ret); return ret; &#125; fcntl(m_iPipeFD[0], F_SETFL, O_NONBLOCK); fcntl(m_iPipeFD[1], F_SETFL, O_NONBLOCK); AddEvent(EPOLLIN); return 0;&#125; 继续回到EventLoop。首先看下EventLoop的Init方法： int EventLoop :: Init(const int iEpollLength)&#123; //创建epoll句柄，iEpollLength为监听的fd数 m_iEpollFd = epoll_create(iEpollLength); if (m_iEpollFd == -1) &#123; PLErr(\"epoll_create fail, ret %d\", m_iEpollFd); return -1; &#125; m_poNotify = new Notify(this); assert(m_poNotify != nullptr); //初始化Notify：创建pipe，设置m_iPipeFD并添加EPOLLIN事件 int ret = m_poNotify-&gt;Init(); if (ret != 0) &#123; return ret; &#125; return 0;&#125; 接着来看下最重要的StartLoop： void EventLoop :: StartLoop()&#123; m_bIsEnd = false; while(true) &#123; BP-&gt;GetNetworkBP()-&gt;TcpEpollLoop(); int iNextTimeout = 1000; DealwithTimeout(iNextTimeout); //PLHead(\"nexttimeout %d\", iNextTimeout); OneLoop(iNextTimeout); CreateEvent(); if (m_poTcpClient != nullptr) &#123; m_poTcpClient-&gt;DealWithWrite(); &#125; if (m_bIsEnd) &#123; PLHead(\"TCP.EventLoop [END]\"); break; &#125; &#125;&#125; 主循环是OneLoop： void EventLoop :: OneLoop(const int iTimeoutMs)&#123; //调用epoll_wait等待事件发生 int n = epoll_wait(m_iEpollFd, m_EpollEvents, MAX_EVENTS, 1); if (n == -1) &#123; if (errno != EINTR) &#123; PLErr(\"epoll_wait fail, errno %d\", errno); return; &#125; &#125; //逐一处理发生的epoll事件 for (int i = 0; i &lt; n; i++) &#123; int iFd = m_EpollEvents[i].data.fd; auto it = m_mapEvent.find(iFd); if (it == end(m_mapEvent)) &#123; continue; &#125; int iEvents = m_EpollEvents[i].events; Event * poEvent = it-&gt;second.m_poEvent; int ret = 0; if (iEvents &amp; EPOLLERR) &#123; OnError(iEvents, poEvent); continue; &#125; try &#123; //如果是EPOLLIN事件，表明由数据可读，则调用poEvent的OnRead方法处理 if (iEvents &amp; EPOLLIN) &#123; ret = poEvent-&gt;OnRead(); &#125; //如果是EPOLLOUT事件，表明由数据可写，则调用poEvent的OnWrite方法处理 if (iEvents &amp; EPOLLOUT) &#123; ret = poEvent-&gt;OnWrite(); &#125; &#125; catch (...) &#123; ret = -1; &#125; if (ret != 0) &#123; OnError(iEvents, poEvent); &#125; &#125;&#125; 其他具体的细节这里就不再赘述了，有兴趣的可以自己去看看源码。 TcpWrite看完了TcpRead，再来看看TcpWrite。首先还是看它的定义： class TcpWrite : public Thread&#123;public: TcpWrite(NetWork * poNetWork); ~TcpWrite(); int Init(); void run(); void Stop(); int AddMessage(const std::string &amp; sIP, const int iPort, const std::string &amp; sMessage);private: TcpClient m_oTcpClient; EventLoop m_oEventLoop;&#125;; Init、run、Stop跟TcpRead中对应方法的作用一致。AddMessage则是调用了m_oTcpClient的AddMessage方法。发现TcpWrite的成员变量比TcpRead多了一个TcpClient对象，因此主要来看看这个TcpClient是干嘛的。 刚刚说TcpWrite的AddMessage调用了m_oTcpClient的AddMessage方法。在m_oTcpClient的AddMessage方法中，则是先创建了一个指向MessageEvent对象的指针poEvent，然后再调用poEvent的AddMessage方法： int TcpClient :: AddMessage(const std::string &amp; sIP, const int iPort, const std::string &amp; sMessage)&#123; //PLImp(\"ok\"); MessageEvent * poEvent = GetEvent(sIP, iPort); if (poEvent == nullptr) &#123; PLErr(\"no event created for this ip %s port %d\", sIP.c_str(), iPort); return -1; &#125; return poEvent-&gt;AddMessage(sMessage);&#125; 因此继续看看MessageEvent的AddMessage方法： int MessageEvent :: AddMessage(const std::string &amp; sMessage)&#123; m_llLastActiveTime = Time::GetSteadyClockMS(); std::unique_lock&lt;std::mutex&gt; oLock(m_oMutex); if ((int)m_oInQueue.size() &gt; TCP_QUEUE_MAXLEN) &#123; BP-&gt;GetNetworkBP()-&gt;TcpQueueFull(); //PLErr(\"queue length %d too long, can't enqueue\", m_oInQueue.size()); return -2; &#125; if (m_iQueueMemSize &gt; MAX_QUEUE_MEM_SIZE) &#123; //PLErr(\"queue memsize %d too large, can't enqueue\", m_iQueueMemSize); return -2; &#125; QueueData tData; //将消息封装成QueueData后放入队列 tData.llEnqueueAbsTime = Time::GetSteadyClockMS(); tData.psValue = new string(sMessage); m_oInQueue.push(tData); m_iQueueMemSize += sMessage.size(); oLock.unlock(); //退出EpollWait，实际是调用SendNotify发送了一个通知 JumpoutEpollWait(); return 0;&#125; 可以看到这里将消息加上入队时间后封装成一个QueueDate，然后放入m_oInQueue队列中。最后调用EventLoop的SendNotify发送了一个通知（利用之前创建的pipe）退出EpollWait。 说完了消息怎么入队，那消息是怎么发送出去的呢？ 这里主要涉及到MessageEvent的OnWrite函数： int MessageEvent :: OnWrite()&#123; int ret = 0; //只要发送队列不为空或者还有上次未发送完的数据，就调用DoOnWrite执行真正的发送操作 while (!m_oInQueue.empty() || m_iLeftWriteLen &gt; 0) &#123; ret = DoOnWrite(); if (ret != 0 &amp;&amp; ret != 1) &#123; return ret; &#125; else if (ret == 1) &#123; //need break, wait next write return 0; &#125; &#125; WriteDone(); return 0;&#125; DoOnWrite: int MessageEvent :: DoOnWrite()&#123; //上一次的消息还未发送完毕，将剩下的发送完 if (m_iLeftWriteLen &gt; 0) &#123; return WriteLeft(); &#125; m_oMutex.lock(); if (m_oInQueue.empty()) &#123; m_oMutex.unlock(); return 0; &#125; //从队列中取出一条新消息，准备发送 QueueData tData = m_oInQueue.front(); m_oInQueue.pop(); m_iQueueMemSize -= tData.psValue-&gt;size(); m_oMutex.unlock(); std::string * poMessage = tData.psValue; //如果该消息入队太久没有被处理，则抛弃，不发送 uint64_t llNowTime = Time::GetSteadyClockMS(); int iDelayMs = llNowTime &gt; tData.llEnqueueAbsTime ? (int)(llNowTime - tData.llEnqueueAbsTime) : 0; BP-&gt;GetNetworkBP()-&gt;TcpOutQueue(iDelayMs); if (iDelayMs &gt; TCP_OUTQUEUE_DROP_TIMEMS) &#123; //PLErr(\"drop request because enqueue timeout, nowtime %lu unqueuetime %lu\", //llNowTime, tData.llEnqueueAbsTime); delete poMessage; return 0; &#125; //计算发送缓冲区长度，需要加上4字节用于表示消息长度 int iBuffLen = poMessage-&gt;size(); int niBuffLen = htonl(iBuffLen + 4); int iLen = iBuffLen + 4; //申请缓冲区 m_oWriteCacheBuffer.Ready(iLen); //将消息长度及消息内容拷贝到缓冲区 memcpy(m_oWriteCacheBuffer.GetPtr(), &amp;niBuffLen, 4); memcpy(m_oWriteCacheBuffer.GetPtr() + 4, poMessage-&gt;c_str(), iBuffLen); m_iLeftWriteLen = iLen; m_iLastWritePos = 0; delete poMessage; //PLImp(\"write len %d ip %s port %d\", iLen, m_oAddr.getHost().c_str(), m_oAddr.getPort()); //开始发送消息，有可能消息太大一次发送不完 int iWriteLen = m_oSocket.send(m_oWriteCacheBuffer.GetPtr(), iLen); if (iWriteLen &lt; 0) &#123; PLErr(\"fail, write len %d ip %s port %d\", iWriteLen, m_oAddr.getHost().c_str(), m_oAddr.getPort()); return -1; &#125; //需要下次再发送 if (iWriteLen == 0) &#123; //need wait next write AddEvent(EPOLLOUT); return 1; &#125; //PLImp(\"real write len %d\", iWriteLen); //发送成功 if (iWriteLen == iLen) &#123; m_iLeftWriteLen = 0; m_iLastWritePos = 0; //write done &#125; //没有一次性全部发送完，剩下的需要下次发送 else if (iWriteLen &lt; iLen) &#123; //m_iLastWritePos和m_iLeftWriteLen分别用来表示上次写的位置以及剩下需要发送的长度 m_iLastWritePos = iWriteLen; m_iLeftWriteLen = iLen - iWriteLen; PLImp(\"write buflen %d smaller than expectlen %d\", iWriteLen, iLen); &#125; else &#123; PLErr(\"write buflen %d large than expectlen %d\", iWriteLen, iLen); &#125; return 0;&#125; 结语先介绍这么多吧，接下去会有更多相关的文章，特别是PhxPaxos中实现Paxos算法的那部分，相信看过Paxos相关论文的童鞋会对这块很感兴趣。 最后，附上PhxPaxos源码的地址：https://github.com/Tencent/phxpaxos","raw":null,"content":null,"categories":[{"name":"源码分析","slug":"源码分析","permalink":"http://linbingdong.com/categories/源码分析/"}],"tags":[{"name":"PhxPaxos","slug":"PhxPaxos","permalink":"http://linbingdong.com/tags/PhxPaxos/"},{"name":"Paxos","slug":"Paxos","permalink":"http://linbingdong.com/tags/Paxos/"}]},{"title":"Java回调机制","slug":"Java回调机制","date":"2017-11-12T16:00:00.000Z","updated":"2017-11-13T14:13:50.000Z","comments":true,"path":"2017/11/13/Java回调机制/","link":"","permalink":"http://linbingdong.com/2017/11/13/Java回调机制/","excerpt":"对Java回调机制进行简单的介绍，并给出简单的示例。","text":"对Java回调机制进行简单的介绍，并给出简单的示例。 什么是回调回调，即callback。简单来讲，回调就是A包含B，A的函数Af1调用B的函数Bf1，Bf1又调用A的另一个函数Af2。这个Af2是回调接口中的方法，即A实现了回调接口。那么，Bf1为什么能调用Af2呢，是因为A将自身作为参数传给了Bf1。回调一般用在异步调用中。 举个通俗点的例子：A是Boss，B是worker。Boss让Worker去干活，并且让worker干完活后向Boss报告。Boss分发任务（dispatchWork），让woker去执行（startWork）。worker执行完后调用回调接口的report()函数报告任务完成。 示例 CallbackReport.java public interface CallbackReport &#123; void report(String msg);&#125; Boss.java public class Boss implements CallbackReport&#123; Worker worker; public Boss(Worker worker) &#123; this.worker = worker; &#125; public void dispatchWork() &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; worker.startWork(Boss.this); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;).start(); &#125; @Override public void report(String msg) &#123; System.out.println(\"worker reported: \" + msg); &#125;&#125; Worker.java public class Worker &#123; public void startWork(CallbackReport callback) throws InterruptedException &#123; System.out.println(\"worker starts working...\"); Thread.sleep(3000); callback.report(\"work is done!\"); &#125;&#125; CallbackTest.java public class CallbackTest &#123; public static void main(String[] args) throws InterruptedException &#123; Worker worker = new Worker(); Boss boss = new Boss(worker); boss.dispatchWork(); System.out.println(\"dispatched work,wait for done!\"); &#125;&#125; 输出worker starts working...dispatched work,wait for done!worker reported: work is done! //等待3秒后才出现这句Process finished with exit code 0","raw":null,"content":null,"categories":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/tags/Java/"}]},{"title":"设计模式-适配器模式","slug":"设计模式-适配器模式","date":"2017-11-10T16:00:00.000Z","updated":"2017-11-11T03:53:34.000Z","comments":true,"path":"2017/11/11/设计模式-适配器模式/","link":"","permalink":"http://linbingdong.com/2017/11/11/设计模式-适配器模式/","excerpt":"本文介绍适配器模式。","text":"本文介绍适配器模式。 定义适配器模式将一个类的接口，转换成客户端期待的另一个接口。 比如我们想用苹果的充电线给安卓充电。但是安卓的充电接口（type-c）跟苹果（lightning）的不一样，所以就需要一个适配器，将安卓的type-c接口转换成苹果的lightning接口，这样就能用苹果的充电线给安卓充电了。 图中玫瑰金色的就是适配器。 角色 目标（Target）：即期望的接口。 适配器（Adapter）：用于将源接口转换成目标接口。 被适配者（Adaptee）：即源接口。 类图 示例public class AdapterDP &#123; public static void main(String[] args) &#123; AppleLightning appleLighting = new AppleLightning(); System.out.println(\"use lightning to charge\"); appleLighting.chargeWithLightning(); System.out.println('\\n' + \"use type-c to charge\"); AndroidTypeC androidTypeC = new AndroidTypeC(); androidTypeC.chargeWithTypeC(); System.out.println('\\n' + \"use lightning to charge\"); Lightning adapter = new Adapter(androidTypeC); adapter.chargeWithLightning(); &#125;&#125;interface Lightning &#123; void chargeWithLightning();&#125;class AppleLightning implements Lightning &#123; public void chargeWithLightning() &#123; System.out.println(\"charging iPhone...\"); &#125;&#125;class AndroidTypeC &#123; public void chargeWithTypeC() &#123; System.out.println(\"charging android...\"); &#125;&#125;class Adapter implements Lightning &#123; public AndroidTypeC androidTypeC; public Adapter(AndroidTypeC androidTypeC) &#123; this.androidTypeC = androidTypeC; &#125; public void chargeWithLightning() &#123; androidTypeC.chargeWithTypeC(); &#125;&#125; 输出 use lightning to chargecharging iPhone...use type-c to chargecharging android...use lightning to chargecharging android... 注：以上的例子是对象适配器模式，还有另一种适配器模式叫类适配器模式，这里不再赘述。 适配器模式在Hadoop源码中的应用Hadoop作为广泛应用的大数据组件，其本质是一个分布式系统，在分布式系统中，各个节点之间的通信和交互是必不可少的，为此，Hadoop实现了一套自己的RPC框架，该RPC框架默认使用Protocol Buffer作为序列化工具。 ClientProtocol协议定义了HDFS Client和NameNode交互的所有方法，但是ClientProtocol协议中方法的参数是无法在网络中传输的，需要对参数进行序列化操作，所以HDFS又定义了ClientNamenodeProtocolPB协议，该协议包含了ClientProtocol定义的所有方法，但是参数却是使用protobuf序列化后的格式。 ClientNamenodeProtocolTranslatorPB类作为Client侧的适配器类，实现了ClientProtocol接口，它内部拥有一个实现了ClientNamenodeProtocolPB接口的对象，可以将ClientProtocol调用适配成ClientNamenodeProtocolPB调用。以rename()调用为例，ClientNamenodeProtocolPB将rename(String, String)调用中的两个String参数序列化成一个RenameRequestProto对象，然后调用ClientNamenodeProtocolPB对象的rename(RenameRequestProto)方法，这样就完成了ClientProtocol接口到ClientNamenodeProtocolPB接口的适配。 在该例子中，ClientNamenodeProtocolTranslatorPB类为适配器，ClientProtocol为目标接口（这里的目标是对客户端来说的），ClientNamenodeProtocolPB为源接口。","raw":null,"content":null,"categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://linbingdong.com/categories/设计模式/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/tags/Java/"},{"name":"设计模式","slug":"设计模式","permalink":"http://linbingdong.com/tags/设计模式/"}]},{"title":"分布式系列文章——Paxos算法原理与推导","slug":"分布式系列文章——Paxos算法原理与推导","date":"2017-04-16T16:00:00.000Z","updated":"2017-09-03T02:12:37.000Z","comments":true,"path":"2017/04/17/分布式系列文章——Paxos算法原理与推导/","link":"","permalink":"http://linbingdong.com/2017/04/17/分布式系列文章——Paxos算法原理与推导/","excerpt":"Paxos算法在分布式领域具有非常重要的地位。但是Paxos算法有两个比较明显的缺点：1.难以理解 2.工程实现更难。\n网上有很多讲解Paxos算法的文章，但是质量参差不齐。看了很多关于Paxos的资料后发现，学习Paxos最好的资料是论文《Paxos Made Simple》，其次是中、英文版维基百科对Paxos的介绍。本文试图带大家一步步揭开Paxos神秘的面纱。","text":"Paxos算法在分布式领域具有非常重要的地位。但是Paxos算法有两个比较明显的缺点：1.难以理解 2.工程实现更难。 网上有很多讲解Paxos算法的文章，但是质量参差不齐。看了很多关于Paxos的资料后发现，学习Paxos最好的资料是论文《Paxos Made Simple》，其次是中、英文版维基百科对Paxos的介绍。本文试图带大家一步步揭开Paxos神秘的面纱。 Paxos是什么 Paxos算法是基于消息传递且具有高度容错特性的一致性算法，是目前公认的解决分布式一致性问题最有效的算法之一。 Google Chubby的作者Mike Burrows说过这个世界上只有一种一致性算法，那就是Paxos，其它的算法都是残次品。 虽然Mike Burrows说得有点夸张，但是至少说明了Paxos算法的地位。然而，Paxos算法也因为晦涩难懂而臭名昭著。本文的目的就是带领大家深入浅出理解Paxos算法，不仅理解它的执行流程，还要理解算法的推导过程，作者是怎么一步步想到最终的方案的。只有理解了推导过程，才能深刻掌握该算法的精髓。而且理解推导过程对于我们的思维也是非常有帮助的，可能会给我们带来一些解决问题的思路，对我们有所启发。 问题产生的背景在常见的分布式系统中，总会发生诸如机器宕机或网络异常（包括消息的延迟、丢失、重复、乱序，还有网络分区）等情况。Paxos算法需要解决的问题就是如何在一个可能发生上述异常的分布式系统中，快速且正确地在集群内部对某个数据的值达成一致，并且保证不论发生以上任何异常，都不会破坏整个系统的一致性。 注：这里某个数据的值并不只是狭义上的某个数，它可以是一条日志，也可以是一条命令（command）。。。根据应用场景不同，某个数据的值有不同的含义。 相关概念在Paxos算法中，有三种角色： Proposer Acceptor Learners 在具体的实现中，一个进程可能同时充当多种角色。比如一个进程可能既是Proposer又是Acceptor又是Learner。 还有一个很重要的概念叫提案（Proposal）。最终要达成一致的value就在提案里。 注： 暂且认为『提案=value』，即提案只包含value。在我们接下来的推导过程中会发现如果提案只包含value，会有问题，于是我们再对提案重新设计。 暂且认为『Proposer可以直接提出提案』。在我们接下来的推导过程中会发现如果Proposer直接提出提案会有问题，需要增加一个学习提案的过程。 Proposer可以提出（propose）提案；Acceptor可以接受（accept）提案；如果某个提案被选定（chosen），那么该提案里的value就被选定了。 回到刚刚说的『对某个数据的值达成一致』，指的是Proposer、Acceptor、Learner都认为同一个value被选定（chosen）。那么，Proposer、Acceptor、Learner分别在什么情况下才能认为某个value被选定呢？ Proposer：只要Proposer发的提案被Acceptor接受（刚开始先认为只需要一个Acceptor接受即可，在推导过程中会发现需要半数以上的Acceptor同意才行），Proposer就认为该提案里的value被选定了。 Acceptor：只要Acceptor接受了某个提案，Acceptor就认为该提案里的value被选定了。 Learner：Acceptor告诉Learner哪个value被选定，Learner就认为那个value被选定。 问题描述假设有一组可以提出（propose）value（value在提案Proposal里）的进程集合。一个一致性算法需要保证提出的这么多value中，只有一个value被选定（chosen）。如果没有value被提出，就不应该有value被选定。如果一个value被选定，那么所有进程都应该能学习（learn）到这个被选定的value。对于一致性算法，安全性（safaty）要求如下： 只有被提出的value才能被选定。 只有一个value被选定，并且 如果某个进程认为某个value被选定了，那么这个value必须是真的被选定的那个。 我们不去精确地定义其活性（liveness）要求。我们的目标是保证最终有一个提出的value被选定。当一个value被选定后，进程最终也能学习到这个value。 Paxos的目标：保证最终有一个value会被选定，当value被选定后，进程最终也能获取到被选定的value。 假设不同角色之间可以通过发送消息来进行通信，那么： 每个角色以任意的速度执行，可能因出错而停止，也可能会重启。一个value被选定后，所有的角色可能失败然后重启，除非那些失败后重启的角色能记录某些信息，否则等他们重启后无法确定被选定的值。 消息在传递过程中可能出现任意时长的延迟，可能会重复，也可能丢失。但是消息不会被损坏，即消息内容不会被篡改（拜占庭将军问题）。 推导过程最简单的方案——只有一个Acceptor假设只有一个Acceptor（可以有多个Proposer），只要Acceptor接受它收到的第一个提案，则该提案被选定，该提案里的value就是被选定的value。这样就保证只有一个value会被选定。 但是，如果这个唯一的Acceptor宕机了，那么整个系统就无法工作了！ 因此，必须要有多个Acceptor！ 多个Acceptor多个Acceptor的情况如下图。那么，如何保证在多个Proposer和多个Acceptor的情况下选定一个value呢？ 下面开始寻找解决方案。 如果我们希望即使只有一个Proposer提出了一个value，该value也最终被选定。 那么，就得到下面的约束： P1：一个Acceptor必须接受它收到的第一个提案。 但是，这又会引出另一个问题：如果每个Proposer分别提出不同的value，发给不同的Acceptor。根据P1，Acceptor分别接受自己收到的value，就导致不同的value被选定。出现了不一致。如下图： 刚刚是因为『一个提案只要被一个Acceptor接受，则该提案的value就被选定了』才导致了出现上面不一致的问题。因此，我们需要加一个规定： 规定：一个提案被选定需要被半数以上的Acceptor接受 这个规定又暗示了：『一个Acceptor必须能够接受不止一个提案！』不然可能导致最终没有value被选定。比如上图的情况。v1、v2、v3都没有被选定，因为它们都只被一个Acceptor的接受。 最开始讲的『提案=value』已经不能满足需求了，于是重新设计提案，给每个提案加上一个提案编号，表示提案被提出的顺序。令『提案=提案编号+value』。 虽然允许多个提案被选定，但必须保证所有被选定的提案都具有相同的value值。否则又会出现不一致。 于是有了下面的约束： P2：如果某个value为v的提案被选定了，那么每个编号更高的被选定提案的value必须也是v。 一个提案只有被Acceptor接受才可能被选定，因此我们可以把P2约束改写成对Acceptor接受的提案的约束P2a。 P2a：如果某个value为v的提案被选定了，那么每个编号更高的被Acceptor接受的提案的value必须也是v。 只要满足了P2a，就能满足P2。 但是，考虑如下的情况：假设总的有5个Acceptor。Proposer2提出[M1,V1]的提案，Acceptor2~5（半数以上）均接受了该提案，于是对于Acceptor2~5和Proposer2来讲，它们都认为V1被选定。Acceptor1刚刚从宕机状态恢复过来（之前Acceptor1没有收到过任何提案），此时Proposer1向Acceptor1发送了[M2,V2]的提案（V2≠V1且M2&gt;M1），对于Acceptor1来讲，这是它收到的第一个提案。根据P1（一个Acceptor必须接受它收到的第一个提案。）,Acceptor1必须接受该提案！同时Acceptor1认为V2被选定。这就出现了两个问题： Acceptor1认为V2被选定，Acceptor2~5和Proposer2认为V1被选定。出现了不一致。 V1被选定了，但是编号更高的被Acceptor1接受的提案[M2,V2]的value为V2，且V2≠V1。这就跟P2a（如果某个value为v的提案被选定了，那么每个编号更高的被Acceptor接受的提案的value必须也是v）矛盾了。 所以我们要对P2a约束进行强化！ P2a是对Acceptor接受的提案约束，但其实提案是Proposer提出来的，所有我们可以对Proposer提出的提案进行约束。得到P2b： P2b：如果某个value为v的提案被选定了，那么之后任何Proposer提出的编号更高的提案的value必须也是v。 由P2b可以推出P2a进而推出P2。 那么，如何确保在某个value为v的提案被选定后，Proposer提出的编号更高的提案的value都是v呢？ 只要满足P2c即可： P2c：对于任意的N和V，如果提案[N, V]被提出，那么存在一个半数以上的Acceptor组成的集合S，满足以下两个条件中的任意一个： S中每个Acceptor都没有接受过编号小于N的提案。 S中Acceptor接受过的最大编号的提案的value为V。 Proposer生成提案为了满足P2b，这里有个比较重要的思想：Proposer生成提案之前，应该先去『学习』已经被选定或者可能被选定的value，然后以该value作为自己提出的提案的value。如果没有value被选定，Proposer才可以自己决定value的值。这样才能达成一致。这个学习的阶段是通过一个『Prepare请求』实现的。 于是我们得到了如下的提案生成算法： Proposer选择一个新的提案编号N，然后向某个Acceptor集合（半数以上）发送请求，要求该集合中的每个Acceptor做出如下响应（response）。(a) 向Proposer承诺保证不再接受任何编号小于N的提案。(b) 如果Acceptor已经接受过提案，那么就向Proposer响应已经接受过的编号小于N的最大编号的提案。 我们将该请求称为编号为N的Prepare请求。 如果Proposer收到了半数以上的Acceptor的响应，那么它就可以生成编号为N，Value为V的提案[N,V]。这里的V是所有的响应中编号最大的提案的Value。如果所有的响应中都没有提案，那 么此时V就可以由Proposer自己选择。生成提案后，Proposer将该提案发送给半数以上的Acceptor集合，并期望这些Acceptor能接受该提案。我们称该请求为Accept请求。（注意：此时接受Accept请求的Acceptor集合不一定是之前响应Prepare请求的Acceptor集合） Acceptor接受提案Acceptor可以忽略任何请求（包括Prepare请求和Accept请求）而不用担心破坏算法的安全性。因此，我们这里要讨论的是什么时候Acceptor可以响应一个请求。 我们对Acceptor接受提案给出如下约束： P1a：一个Acceptor只要尚未响应过任何编号大于N的Prepare请求，那么他就可以接受这个编号为N的提案。 如果Acceptor收到一个编号为N的Prepare请求，在此之前它已经响应过编号大于N的Prepare请求。根据P1a，该Acceptor不可能接受编号为N的提案。因此，该Acceptor可以忽略编号为N的Prepare请求。当然，也可以回复一个error，让Proposer尽早知道自己的提案不会被接受。 因此，一个Acceptor只需记住：1. 已接受的编号最大的提案 2. 已响应的请求的最大编号。 Paxos算法描述经过上面的推导，我们总结下Paxos算法的流程。 Paxos算法分为两个阶段。具体如下： 阶段一： (a) Proposer选择一个提案编号N，然后向半数以上的Acceptor发送编号为N的Prepare请求。 (b) 如果一个Acceptor收到一个编号为N的Prepare请求，且N大于该Acceptor已经响应过的所有Prepare请求的编号，那么它就会将它已经接受过的编号最大的提案（如果有的话）作为响应反馈给Proposer，同时该Acceptor承诺不再接受任何编号小于N的提案。 阶段二： (a) 如果Proposer收到半数以上Acceptor对其发出的编号为N的Prepare请求的响应，那么它就会发送一个针对[N,V]提案的Accept请求给半数以上的Acceptor。注意：V就是收到的响应中编号最大的提案的value，如果响应中不包含任何提案，那么V就由Proposer自己决定。 (b) 如果Acceptor收到一个针对编号为N的提案的Accept请求，只要该Acceptor没有对编号大于N的Prepare请求做出过响应，它就接受该提案。 Learner学习被选定的valueLearner学习（获取）被选定的value有如下三种方案： 如何保证Paxos算法的活性 通过选取主Proposer，就可以保证Paxos算法的活性。至此，我们得到一个既能保证安全性，又能保证活性的分布式一致性算法——Paxos算法。 参考资料 论文《Paxos Made Simple》 论文《The Part-Time Parliament》 英文版维基百科的Paxos 中文版维基百科的Paxos 书籍《从Paxos到ZooKeeper》","raw":null,"content":null,"categories":[{"name":"分布式一致性算法","slug":"分布式一致性算法","permalink":"http://linbingdong.com/categories/分布式一致性算法/"},{"name":"Paxos","slug":"分布式一致性算法/Paxos","permalink":"http://linbingdong.com/categories/分布式一致性算法/Paxos/"}],"tags":[{"name":"分布式系统","slug":"分布式系统","permalink":"http://linbingdong.com/tags/分布式系统/"},{"name":"Paxos","slug":"Paxos","permalink":"http://linbingdong.com/tags/Paxos/"},{"name":"分布式一致性算法","slug":"分布式一致性算法","permalink":"http://linbingdong.com/tags/分布式一致性算法/"}]},{"title":"深入理解HashMap","slug":"深入理解HashMap","date":"2017-04-16T12:47:53.000Z","updated":"2017-04-16T12:47:53.000Z","comments":true,"path":"2017/04/16/深入理解HashMap/","link":"","permalink":"http://linbingdong.com/2017/04/16/深入理解HashMap/","excerpt":"JDK1.7和JDK1.8中HashMap的实现不尽相同，JDK1.8中做了一些优化，比如当链表多长时转化成红黑树，提高插入和查找的效率；扩容时不再重新哈希。","text":"JDK1.7和JDK1.8中HashMap的实现不尽相同，JDK1.8中做了一些优化，比如当链表多长时转化成红黑树，提高插入和查找的效率；扩容时不再重新哈希。 HashMap的源码比较长，加上注释有2000行左右，会在文末贴出。这里主要以提问题的形式来加深对HashMap的理解，读者可以先独立思考这些问题，再参照我的回答和文末的源码进行学习。 实现原理数组+链表+红黑树 数组是一个Node数组，Node是一个Key-Value对，Node实现了Map.Entry。链表/红黑树是用来解决hash冲突的。当链表的长度大于8时会转化成红黑树（Node-&gt;TreeNode），提高插入和查询的效率。 HashMap继承了哪些类，实现了哪些接口继承了AbstractMap类，实现了Serializable/Cloneable/Map接口 如何put和getget 确定key对应的Node的索引： h = key.hashCode(); hash = h ^ (h &gt;&gt;&gt; 16); index = hash &amp; (length - 1) 如果第一个Node的key跟该key相等（hash值相同且equals），则返回第一个Node的value。 如果第一个是TreeNode，则用红黑树的方法进行搜索，找到key相同的返回该Node的value；如果第一个是Node，则按链表逐个搜索，找到key相同的返回该Node的value。 返回null put 如果数组长度为0，先扩容。如果已经是最大容量，则不再扩。 计算放置的位置（数组下标）。h = key.hashCode(); hash = h ^ h &gt;&gt;&gt; 16; index = hash &amp; (length - 1) 若该位置为null，则直接将新节点放入该位置 若该位置不为null，如果第一个节点的key跟要put的key相同（hashCode和equals），直接覆盖value 否则，判断是否为TreeNode，若是，则直接在红黑树中插入键值对。 若是链表，判断链表的长度是否大于8，如果大于8则转为红黑树，并在红黑树中插入。否则执行链表中的插入操作。遍历链表时若发现key已经存在则直接覆盖value。 如果超过阈值，就扩容。 初始容量和最大容量分别是多少默认初始容量为16（1&lt;&lt;4），可以在初始化时自己指定。最大容量为2的30次方（1&lt;&lt;30）,容量一定是2的整数幂 加载因子（loadFactor）是多少，为什么0.75 加载因子默认为0.75是对时间和空间的折中。加载因子越大，空间利用率越高，但是产生冲突的概率就越大，会导致put和get效率降低。 如何确定Node在数组中的位置（如何构造哈希函数） 通过key.hashCode()获得 h = key.hashCode()， 将h的高16位和低16位异或。 hash = h ^ (h &gt;&gt;&gt; 16) index = hash &amp; (length - 1) 等价于hash % length。前提是length必须为2的整数幂 如何减少冲突 合理的加载因子，如果加载因子设置得过大会增加冲突的概率 合理的hash函数。比如将hashCode的高16位和低16位异或。 如何解决冲突采用拉链法解决冲突（链表的插入采用头插法）。 什么时候扩容，如何扩容当Node数（size）超过阈值（threshold = 容量*加载因子）时会扩容。扩容为原来的两倍。 新建一个两倍大的数组 把旧数组中的Node全部放入新数组。JDK1.7中所有的Node都会重新hash来确定在新数组中的位置，效率很低；但其实扩容为两倍后，新的Node下标要么跟原来相等，要么比原来大length。JDK1.8中就采取后面的方法，效率更高. 构造函数 包含 初始容量和加载因子 两个参数的构造函数如果指定的初始容量大于最大容量，会以最大容量作为初始容量；如果指定的初始容量不是2的整数幂，会找到大于该值的最小的2的整数幂作为初始容量。 默认构造函数初始容量为16；加载因子为0.75 包含子map的构造函数 为什么数组容量要是2的整数幂 这样hash % length 可以转化为 hash &amp; (length -1) 位运算效率更高 保证length - 1二进制表示的最低位为1。 如果最低位为 0 ，则按位与之后一定得到偶数（下标都是偶数），这样浪费了一大半的空间。 什么情况下HashMap会线程不安全 如线程A和线程B同时put并写入相同的位置。两个线程都会得到该位置当前的头节点。如果A先写入新的头节点，然后B也写入新的头节点，那么B的操作就会覆盖A的操作造成A的写入操作丢失。 如多个线程同时put并刚好都达到门限值，然后都进行了resize（扩容）。此时每个线程都会生成一个新的数组并将table指针指向新的数组，结果最终只有最后一个线程的生成的新数组被赋给table变量，其他线程的均会丢失。 HashMap源码（JDK1.8） package java.util;import java.io.IOException;import java.io.InvalidObjectException;import java.io.Serializable;import java.lang.reflect.ParameterizedType;import java.lang.reflect.Type;import java.util.function.BiConsumer;import java.util.function.BiFunction;import java.util.function.Consumer;import java.util.function.Function;public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable &#123; private static final long serialVersionUID = 362498820763181265L; /** * The default initial capacity - MUST be a power of two. */ static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16 /** * The maximum capacity, used if a higher value is implicitly specified * by either of the constructors with arguments. * MUST be a power of two &lt;= 1&lt;&lt;30. */ static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; /** * The load factor used when none specified in constructor. */ static final float DEFAULT_LOAD_FACTOR = 0.75f; /** * The bin count threshold for using a tree rather than list for a * bin. Bins are converted to trees when adding an element to a * bin with at least this many nodes. The value must be greater * than 2 and should be at least 8 to mesh with assumptions in * tree removal about conversion back to plain bins upon * shrinkage. */ static final int TREEIFY_THRESHOLD = 8; /** * The bin count threshold for untreeifying a (split) bin during a * resize operation. Should be less than TREEIFY_THRESHOLD, and at * most 6 to mesh with shrinkage detection under removal. */ static final int UNTREEIFY_THRESHOLD = 6; /** * The smallest table capacity for which bins may be treeified. * (Otherwise the table is resized if too many nodes in a bin.) * Should be at least 4 * TREEIFY_THRESHOLD to avoid conflicts * between resizing and treeification thresholds. */ static final int MIN_TREEIFY_CAPACITY = 64; /** * Basic hash bin node, used for most entries. (See below for * TreeNode subclass, and in LinkedHashMap for its Entry subclass.) */ static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; V value; Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + \"=\" + value; &#125; public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; public final boolean equals(Object o) &#123; if (o == this) return true; if (o instanceof Map.Entry) &#123; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; &#125; return false; &#125; &#125; /* ---------------- Static utilities -------------- */ /** * Computes key.hashCode() and spreads (XORs) higher bits of hash * to lower. Because the table uses power-of-two masking, sets of * hashes that vary only in bits above the current mask will * always collide. (Among known examples are sets of Float keys * holding consecutive whole numbers in small tables.) So we * apply a transform that spreads the impact of higher bits * downward. There is a tradeoff between speed, utility, and * quality of bit-spreading. Because many common sets of hashes * are already reasonably distributed (so don't benefit from * spreading), and because we use trees to handle large sets of * collisions in bins, we just XOR some shifted bits in the * cheapest possible way to reduce systematic lossage, as well as * to incorporate impact of the highest bits that would otherwise * never be used in index calculations because of table bounds. */ static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16); &#125; /** * Returns x's Class if it is of the form \"class C implements * Comparable&lt;C&gt;\", else null. */ static Class&lt;?&gt; comparableClassFor(Object x) &#123; if (x instanceof Comparable) &#123; Class&lt;?&gt; c; Type[] ts, as; Type t; ParameterizedType p; if ((c = x.getClass()) == String.class) // bypass checks return c; if ((ts = c.getGenericInterfaces()) != null) &#123; for (int i = 0; i &lt; ts.length; ++i) &#123; if (((t = ts[i]) instanceof ParameterizedType) &amp;&amp; ((p = (ParameterizedType)t).getRawType() == Comparable.class) &amp;&amp; (as = p.getActualTypeArguments()) != null &amp;&amp; as.length == 1 &amp;&amp; as[0] == c) // type arg is c return c; &#125; &#125; &#125; return null; &#125; /** * Returns k.compareTo(x) if x matches kc (k's screened comparable * class), else 0. */ @SuppressWarnings(&#123;\"rawtypes\",\"unchecked\"&#125;) // for cast to Comparable static int compareComparables(Class&lt;?&gt; kc, Object k, Object x) &#123; return (x == null || x.getClass() != kc ? 0 : ((Comparable)k).compareTo(x)); &#125; /** * Returns a power of two size for the given target capacity. */ static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; &#125; /* ---------------- Fields -------------- */ /** * The table, initialized on first use, and resized as * necessary. When allocated, length is always a power of two. * (We also tolerate length zero in some operations to allow * bootstrapping mechanics that are currently not needed.) */ transient Node&lt;K,V&gt;[] table; /** * Holds cached entrySet(). Note that AbstractMap fields are used * for keySet() and values(). */ transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet; /** * The number of key-value mappings contained in this map. */ transient int size; /** * The number of times this HashMap has been structurally modified * Structural modifications are those that change the number of mappings in * the HashMap or otherwise modify its internal structure (e.g., * rehash). This field is used to make iterators on Collection-views of * the HashMap fail-fast. (See ConcurrentModificationException). */ transient int modCount; /** * The next size value at which to resize (capacity * load factor). * * @serial */ // (The javadoc description is true upon serialization. // Additionally, if the table array has not been allocated, this // field holds the initial array capacity, or zero signifying // DEFAULT_INITIAL_CAPACITY.) int threshold; /** * The load factor for the hash table. * * @serial */ final float loadFactor; /* ---------------- Public operations -------------- */ /** * Constructs an empty &lt;tt&gt;HashMap&lt;/tt&gt; with the specified initial * capacity and load factor. * * @param initialCapacity the initial capacity * @param loadFactor the load factor * @throws IllegalArgumentException if the initial capacity is negative * or the load factor is nonpositive */ public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(\"Illegal initial capacity: \" + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(\"Illegal load factor: \" + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity); &#125; /** * Constructs an empty &lt;tt&gt;HashMap&lt;/tt&gt; with the specified initial * capacity and the default load factor (0.75). * * @param initialCapacity the initial capacity. * @throws IllegalArgumentException if the initial capacity is negative. */ public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR); &#125; /** * Constructs an empty &lt;tt&gt;HashMap&lt;/tt&gt; with the default initial capacity * (16) and the default load factor (0.75). */ public HashMap() &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted &#125; /** * Constructs a new &lt;tt&gt;HashMap&lt;/tt&gt; with the same mappings as the * specified &lt;tt&gt;Map&lt;/tt&gt;. The &lt;tt&gt;HashMap&lt;/tt&gt; is created with * default load factor (0.75) and an initial capacity sufficient to * hold the mappings in the specified &lt;tt&gt;Map&lt;/tt&gt;. * * @param m the map whose mappings are to be placed in this map * @throws NullPointerException if the specified map is null */ public HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false); &#125; /** * Implements Map.putAll and Map constructor * * @param m the map * @param evict false when initially constructing this map, else * true (relayed to method afterNodeInsertion). */ final void putMapEntries(Map&lt;? extends K, ? extends V&gt; m, boolean evict) &#123; int s = m.size(); if (s &gt; 0) &#123; if (table == null) &#123; // pre-size float ft = ((float)s / loadFactor) + 1.0F; int t = ((ft &lt; (float)MAXIMUM_CAPACITY) ? (int)ft : MAXIMUM_CAPACITY); if (t &gt; threshold) threshold = tableSizeFor(t); &#125; else if (s &gt; threshold) resize(); for (Map.Entry&lt;? extends K, ? extends V&gt; e : m.entrySet()) &#123; K key = e.getKey(); V value = e.getValue(); putVal(hash(key), key, value, false, evict); &#125; &#125; &#125; /** * Returns the number of key-value mappings in this map. * * @return the number of key-value mappings in this map */ public int size() &#123; return size; &#125; /** * Returns &lt;tt&gt;true&lt;/tt&gt; if this map contains no key-value mappings. * * @return &lt;tt&gt;true&lt;/tt&gt; if this map contains no key-value mappings */ public boolean isEmpty() &#123; return size == 0; &#125; /** * Returns the value to which the specified key is mapped, * or &#123;@code null&#125; if this map contains no mapping for the key. * * &lt;p&gt;More formally, if this map contains a mapping from a key * &#123;@code k&#125; to a value &#123;@code v&#125; such that &#123;@code (key==null ? k==null : * key.equals(k))&#125;, then this method returns &#123;@code v&#125;; otherwise * it returns &#123;@code null&#125;. (There can be at most one such mapping.) * * &lt;p&gt;A return value of &#123;@code null&#125; does not &lt;i&gt;necessarily&lt;/i&gt; * indicate that the map contains no mapping for the key; it's also * possible that the map explicitly maps the key to &#123;@code null&#125;. * The &#123;@link #containsKey containsKey&#125; operation may be used to * distinguish these two cases. * * @see #put(Object, Object) */ public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value; &#125; /** * Implements Map.get and related methods * * @param hash hash for key * @param key the key * @return the node, or null if none */ final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) &#123; if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null; &#125; /** * Returns &lt;tt&gt;true&lt;/tt&gt; if this map contains a mapping for the * specified key. * * @param key The key whose presence in this map is to be tested * @return &lt;tt&gt;true&lt;/tt&gt; if this map contains a mapping for the specified * key. */ public boolean containsKey(Object key) &#123; return getNode(hash(key), key) != null; &#125; /** * Associates the specified value with the specified key in this map. * If the map previously contained a mapping for the key, the old * value is replaced. * * @param key key with which the specified value is to be associated * @param value value to be associated with the specified key * @return the previous value associated with &lt;tt&gt;key&lt;/tt&gt;, or * &lt;tt&gt;null&lt;/tt&gt; if there was no mapping for &lt;tt&gt;key&lt;/tt&gt;. * (A &lt;tt&gt;null&lt;/tt&gt; return can also indicate that the map * previously associated &lt;tt&gt;null&lt;/tt&gt; with &lt;tt&gt;key&lt;/tt&gt;.) */ public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true); &#125; /** * Implements Map.put and related methods * * @param hash hash for key * @param key the key * @param value the value to put * @param onlyIfAbsent if true, don't change existing value * @param evict if false, the table is in creation mode. * @return previous value, or null if none */ final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null; &#125; /** * Initializes or doubles table size. If null, allocates in * accord with initial capacity target held in field threshold. * Otherwise, because we are using power-of-two expansion, the * elements from each bin must either stay at same index, or move * with a power of two offset in the new table. * * @return the table */ final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;\"rawtypes\",\"unchecked\"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab; &#125; /** * Replaces all linked nodes in bin at index for given hash unless * table is too small, in which case resizes instead. */ final void treeifyBin(Node&lt;K,V&gt;[] tab, int hash) &#123; int n, index; Node&lt;K,V&gt; e; if (tab == null || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY) resize(); else if ((e = tab[index = (n - 1) &amp; hash]) != null) &#123; TreeNode&lt;K,V&gt; hd = null, tl = null; do &#123; TreeNode&lt;K,V&gt; p = replacementTreeNode(e, null); if (tl == null) hd = p; else &#123; p.prev = tl; tl.next = p; &#125; tl = p; &#125; while ((e = e.next) != null); if ((tab[index] = hd) != null) hd.treeify(tab); &#125; &#125; /** * Copies all of the mappings from the specified map to this map. * These mappings will replace any mappings that this map had for * any of the keys currently in the specified map. * * @param m mappings to be stored in this map * @throws NullPointerException if the specified map is null */ public void putAll(Map&lt;? extends K, ? extends V&gt; m) &#123; putMapEntries(m, true); &#125; /** * Removes the mapping for the specified key from this map if present. * * @param key key whose mapping is to be removed from the map * @return the previous value associated with &lt;tt&gt;key&lt;/tt&gt;, or * &lt;tt&gt;null&lt;/tt&gt; if there was no mapping for &lt;tt&gt;key&lt;/tt&gt;. * (A &lt;tt&gt;null&lt;/tt&gt; return can also indicate that the map * previously associated &lt;tt&gt;null&lt;/tt&gt; with &lt;tt&gt;key&lt;/tt&gt;.) */ public V remove(Object key) &#123; Node&lt;K,V&gt; e; return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value; &#125; /** * Implements Map.remove and related methods * * @param hash hash for key * @param key the key * @param value the value to match if matchValue, else ignored * @param matchValue if true only remove if value is equal * @param movable if false do not move other nodes while removing * @return the node, or null if none */ final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) &#123; Node&lt;K,V&gt; node = null, e; K k; V v; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) node = p; else if ((e = p.next) != null) &#123; if (p instanceof TreeNode) node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key); else &#123; do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; node = e; break; &#125; p = e; &#125; while ((e = e.next) != null); &#125; &#125; if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) &#123; if (node instanceof TreeNode) ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable); else if (node == p) tab[index] = node.next; else p.next = node.next; ++modCount; --size; afterNodeRemoval(node); return node; &#125; &#125; return null; &#125; /** * Removes all of the mappings from this map. * The map will be empty after this call returns. */ public void clear() &#123; Node&lt;K,V&gt;[] tab; modCount++; if ((tab = table) != null &amp;&amp; size &gt; 0) &#123; size = 0; for (int i = 0; i &lt; tab.length; ++i) tab[i] = null; &#125; &#125; /** * Returns &lt;tt&gt;true&lt;/tt&gt; if this map maps one or more keys to the * specified value. * * @param value value whose presence in this map is to be tested * @return &lt;tt&gt;true&lt;/tt&gt; if this map maps one or more keys to the * specified value */ public boolean containsValue(Object value) &#123; Node&lt;K,V&gt;[] tab; V v; if ((tab = table) != null &amp;&amp; size &gt; 0) &#123; for (int i = 0; i &lt; tab.length; ++i) &#123; for (Node&lt;K,V&gt; e = tab[i]; e != null; e = e.next) &#123; if ((v = e.value) == value || (value != null &amp;&amp; value.equals(v))) return true; &#125; &#125; &#125; return false; &#125; /** * Returns a &#123;@link Set&#125; view of the keys contained in this map. * The set is backed by the map, so changes to the map are * reflected in the set, and vice-versa. If the map is modified * while an iteration over the set is in progress (except through * the iterator's own &lt;tt&gt;remove&lt;/tt&gt; operation), the results of * the iteration are undefined. The set supports element removal, * which removes the corresponding mapping from the map, via the * &lt;tt&gt;Iterator.remove&lt;/tt&gt;, &lt;tt&gt;Set.remove&lt;/tt&gt;, * &lt;tt&gt;removeAll&lt;/tt&gt;, &lt;tt&gt;retainAll&lt;/tt&gt;, and &lt;tt&gt;clear&lt;/tt&gt; * operations. It does not support the &lt;tt&gt;add&lt;/tt&gt; or &lt;tt&gt;addAll&lt;/tt&gt; * operations. * * @return a set view of the keys contained in this map */ public Set&lt;K&gt; keySet() &#123; Set&lt;K&gt; ks; return (ks = keySet) == null ? (keySet = new KeySet()) : ks; &#125; final class KeySet extends AbstractSet&lt;K&gt; &#123; public final int size() &#123; return size; &#125; public final void clear() &#123; HashMap.this.clear(); &#125; public final Iterator&lt;K&gt; iterator() &#123; return new KeyIterator(); &#125; public final boolean contains(Object o) &#123; return containsKey(o); &#125; public final boolean remove(Object key) &#123; return removeNode(hash(key), key, null, false, true) != null; &#125; public final Spliterator&lt;K&gt; spliterator() &#123; return new KeySpliterator&lt;&gt;(HashMap.this, 0, -1, 0, 0); &#125; public final void forEach(Consumer&lt;? super K&gt; action) &#123; Node&lt;K,V&gt;[] tab; if (action == null) throw new NullPointerException(); if (size &gt; 0 &amp;&amp; (tab = table) != null) &#123; int mc = modCount; for (int i = 0; i &lt; tab.length; ++i) &#123; for (Node&lt;K,V&gt; e = tab[i]; e != null; e = e.next) action.accept(e.key); &#125; if (modCount != mc) throw new ConcurrentModificationException(); &#125; &#125; &#125; /** * Returns a &#123;@link Collection&#125; view of the values contained in this map. * The collection is backed by the map, so changes to the map are * reflected in the collection, and vice-versa. If the map is * modified while an iteration over the collection is in progress * (except through the iterator's own &lt;tt&gt;remove&lt;/tt&gt; operation), * the results of the iteration are undefined. The collection * supports element removal, which removes the corresponding * mapping from the map, via the &lt;tt&gt;Iterator.remove&lt;/tt&gt;, * &lt;tt&gt;Collection.remove&lt;/tt&gt;, &lt;tt&gt;removeAll&lt;/tt&gt;, * &lt;tt&gt;retainAll&lt;/tt&gt; and &lt;tt&gt;clear&lt;/tt&gt; operations. It does not * support the &lt;tt&gt;add&lt;/tt&gt; or &lt;tt&gt;addAll&lt;/tt&gt; operations. * * @return a view of the values contained in this map */ public Collection&lt;V&gt; values() &#123; Collection&lt;V&gt; vs; return (vs = values) == null ? (values = new Values()) : vs; &#125; final class Values extends AbstractCollection&lt;V&gt; &#123; public final int size() &#123; return size; &#125; public final void clear() &#123; HashMap.this.clear(); &#125; public final Iterator&lt;V&gt; iterator() &#123; return new ValueIterator(); &#125; public final boolean contains(Object o) &#123; return containsValue(o); &#125; public final Spliterator&lt;V&gt; spliterator() &#123; return new ValueSpliterator&lt;&gt;(HashMap.this, 0, -1, 0, 0); &#125; public final void forEach(Consumer&lt;? super V&gt; action) &#123; Node&lt;K,V&gt;[] tab; if (action == null) throw new NullPointerException(); if (size &gt; 0 &amp;&amp; (tab = table) != null) &#123; int mc = modCount; for (int i = 0; i &lt; tab.length; ++i) &#123; for (Node&lt;K,V&gt; e = tab[i]; e != null; e = e.next) action.accept(e.value); &#125; if (modCount != mc) throw new ConcurrentModificationException(); &#125; &#125; &#125; /** * Returns a &#123;@link Set&#125; view of the mappings contained in this map. * The set is backed by the map, so changes to the map are * reflected in the set, and vice-versa. If the map is modified * while an iteration over the set is in progress (except through * the iterator's own &lt;tt&gt;remove&lt;/tt&gt; operation, or through the * &lt;tt&gt;setValue&lt;/tt&gt; operation on a map entry returned by the * iterator) the results of the iteration are undefined. The set * supports element removal, which removes the corresponding * mapping from the map, via the &lt;tt&gt;Iterator.remove&lt;/tt&gt;, * &lt;tt&gt;Set.remove&lt;/tt&gt;, &lt;tt&gt;removeAll&lt;/tt&gt;, &lt;tt&gt;retainAll&lt;/tt&gt; and * &lt;tt&gt;clear&lt;/tt&gt; operations. It does not support the * &lt;tt&gt;add&lt;/tt&gt; or &lt;tt&gt;addAll&lt;/tt&gt; operations. * * @return a set view of the mappings contained in this map */ public Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet() &#123; Set&lt;Map.Entry&lt;K,V&gt;&gt; es; return (es = entrySet) == null ? (entrySet = new EntrySet()) : es; &#125; final class EntrySet extends AbstractSet&lt;Map.Entry&lt;K,V&gt;&gt; &#123; public final int size() &#123; return size; &#125; public final void clear() &#123; HashMap.this.clear(); &#125; public final Iterator&lt;Map.Entry&lt;K,V&gt;&gt; iterator() &#123; return new EntryIterator(); &#125; public final boolean contains(Object o) &#123; if (!(o instanceof Map.Entry)) return false; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;) o; Object key = e.getKey(); Node&lt;K,V&gt; candidate = getNode(hash(key), key); return candidate != null &amp;&amp; candidate.equals(e); &#125; public final boolean remove(Object o) &#123; if (o instanceof Map.Entry) &#123; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;) o; Object key = e.getKey(); Object value = e.getValue(); return removeNode(hash(key), key, value, true, true) != null; &#125; return false; &#125; public final Spliterator&lt;Map.Entry&lt;K,V&gt;&gt; spliterator() &#123; return new EntrySpliterator&lt;&gt;(HashMap.this, 0, -1, 0, 0); &#125; public final void forEach(Consumer&lt;? super Map.Entry&lt;K,V&gt;&gt; action) &#123; Node&lt;K,V&gt;[] tab; if (action == null) throw new NullPointerException(); if (size &gt; 0 &amp;&amp; (tab = table) != null) &#123; int mc = modCount; for (int i = 0; i &lt; tab.length; ++i) &#123; for (Node&lt;K,V&gt; e = tab[i]; e != null; e = e.next) action.accept(e); &#125; if (modCount != mc) throw new ConcurrentModificationException(); &#125; &#125; &#125; // Overrides of JDK8 Map extension methods @Override public V getOrDefault(Object key, V defaultValue) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? defaultValue : e.value; &#125; @Override public V putIfAbsent(K key, V value) &#123; return putVal(hash(key), key, value, true, true); &#125; @Override public boolean remove(Object key, Object value) &#123; return removeNode(hash(key), key, value, true, true) != null; &#125; @Override public boolean replace(K key, V oldValue, V newValue) &#123; Node&lt;K,V&gt; e; V v; if ((e = getNode(hash(key), key)) != null &amp;&amp; ((v = e.value) == oldValue || (v != null &amp;&amp; v.equals(oldValue)))) &#123; e.value = newValue; afterNodeAccess(e); return true; &#125; return false; &#125; @Override public V replace(K key, V value) &#123; Node&lt;K,V&gt; e; if ((e = getNode(hash(key), key)) != null) &#123; V oldValue = e.value; e.value = value; afterNodeAccess(e); return oldValue; &#125; return null; &#125; @Override public V computeIfAbsent(K key, Function&lt;? super K, ? extends V&gt; mappingFunction) &#123; if (mappingFunction == null) throw new NullPointerException(); int hash = hash(key); Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first; int n, i; int binCount = 0; TreeNode&lt;K,V&gt; t = null; Node&lt;K,V&gt; old = null; if (size &gt; threshold || (tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; if ((first = tab[i = (n - 1) &amp; hash]) != null) &#123; if (first instanceof TreeNode) old = (t = (TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); else &#123; Node&lt;K,V&gt; e = first; K k; do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; old = e; break; &#125; ++binCount; &#125; while ((e = e.next) != null); &#125; V oldValue; if (old != null &amp;&amp; (oldValue = old.value) != null) &#123; afterNodeAccess(old); return oldValue; &#125; &#125; V v = mappingFunction.apply(key); if (v == null) &#123; return null; &#125; else if (old != null) &#123; old.value = v; afterNodeAccess(old); return v; &#125; else if (t != null) t.putTreeVal(this, tab, hash, key, v); else &#123; tab[i] = newNode(hash, key, v, first); if (binCount &gt;= TREEIFY_THRESHOLD - 1) treeifyBin(tab, hash); &#125; ++modCount; ++size; afterNodeInsertion(true); return v; &#125; public V computeIfPresent(K key, BiFunction&lt;? super K, ? super V, ? extends V&gt; remappingFunction) &#123; if (remappingFunction == null) throw new NullPointerException(); Node&lt;K,V&gt; e; V oldValue; int hash = hash(key); if ((e = getNode(hash, key)) != null &amp;&amp; (oldValue = e.value) != null) &#123; V v = remappingFunction.apply(key, oldValue); if (v != null) &#123; e.value = v; afterNodeAccess(e); return v; &#125; else removeNode(hash, key, null, false, true); &#125; return null; &#125; @Override public V compute(K key, BiFunction&lt;? super K, ? super V, ? extends V&gt; remappingFunction) &#123; if (remappingFunction == null) throw new NullPointerException(); int hash = hash(key); Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first; int n, i; int binCount = 0; TreeNode&lt;K,V&gt; t = null; Node&lt;K,V&gt; old = null; if (size &gt; threshold || (tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; if ((first = tab[i = (n - 1) &amp; hash]) != null) &#123; if (first instanceof TreeNode) old = (t = (TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); else &#123; Node&lt;K,V&gt; e = first; K k; do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; old = e; break; &#125; ++binCount; &#125; while ((e = e.next) != null); &#125; &#125; V oldValue = (old == null) ? null : old.value; V v = remappingFunction.apply(key, oldValue); if (old != null) &#123; if (v != null) &#123; old.value = v; afterNodeAccess(old); &#125; else removeNode(hash, key, null, false, true); &#125; else if (v != null) &#123; if (t != null) t.putTreeVal(this, tab, hash, key, v); else &#123; tab[i] = newNode(hash, key, v, first); if (binCount &gt;= TREEIFY_THRESHOLD - 1) treeifyBin(tab, hash); &#125; ++modCount; ++size; afterNodeInsertion(true); &#125; return v; &#125; @Override public V merge(K key, V value, BiFunction&lt;? super V, ? super V, ? extends V&gt; remappingFunction) &#123; if (value == null) throw new NullPointerException(); if (remappingFunction == null) throw new NullPointerException(); int hash = hash(key); Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first; int n, i; int binCount = 0; TreeNode&lt;K,V&gt; t = null; Node&lt;K,V&gt; old = null; if (size &gt; threshold || (tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; if ((first = tab[i = (n - 1) &amp; hash]) != null) &#123; if (first instanceof TreeNode) old = (t = (TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); else &#123; Node&lt;K,V&gt; e = first; K k; do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; old = e; break; &#125; ++binCount; &#125; while ((e = e.next) != null); &#125; &#125; if (old != null) &#123; V v; if (old.value != null) v = remappingFunction.apply(old.value, value); else v = value; if (v != null) &#123; old.value = v; afterNodeAccess(old); &#125; else removeNode(hash, key, null, false, true); return v; &#125; if (value != null) &#123; if (t != null) t.putTreeVal(this, tab, hash, key, value); else &#123; tab[i] = newNode(hash, key, value, first); if (binCount &gt;= TREEIFY_THRESHOLD - 1) treeifyBin(tab, hash); &#125; ++modCount; ++size; afterNodeInsertion(true); &#125; return value; &#125; @Override public void forEach(BiConsumer&lt;? super K, ? super V&gt; action) &#123; Node&lt;K,V&gt;[] tab; if (action == null) throw new NullPointerException(); if (size &gt; 0 &amp;&amp; (tab = table) != null) &#123; int mc = modCount; for (int i = 0; i &lt; tab.length; ++i) &#123; for (Node&lt;K,V&gt; e = tab[i]; e != null; e = e.next) action.accept(e.key, e.value); &#125; if (modCount != mc) throw new ConcurrentModificationException(); &#125; &#125; @Override public void replaceAll(BiFunction&lt;? super K, ? super V, ? extends V&gt; function) &#123; Node&lt;K,V&gt;[] tab; if (function == null) throw new NullPointerException(); if (size &gt; 0 &amp;&amp; (tab = table) != null) &#123; int mc = modCount; for (int i = 0; i &lt; tab.length; ++i) &#123; for (Node&lt;K,V&gt; e = tab[i]; e != null; e = e.next) &#123; e.value = function.apply(e.key, e.value); &#125; &#125; if (modCount != mc) throw new ConcurrentModificationException(); &#125; &#125; /* ------------------------------------------------------------ */ // Cloning and serialization /** * Returns a shallow copy of this &lt;tt&gt;HashMap&lt;/tt&gt; instance: the keys and * values themselves are not cloned. * * @return a shallow copy of this map */ @SuppressWarnings(\"unchecked\") @Override public Object clone() &#123; HashMap&lt;K,V&gt; result; try &#123; result = (HashMap&lt;K,V&gt;)super.clone(); &#125; catch (CloneNotSupportedException e) &#123; // this shouldn't happen, since we are Cloneable throw new InternalError(e); &#125; result.reinitialize(); result.putMapEntries(this, false); return result; &#125; // These methods are also used when serializing HashSets final float loadFactor() &#123; return loadFactor; &#125; final int capacity() &#123; return (table != null) ? table.length : (threshold &gt; 0) ? threshold : DEFAULT_INITIAL_CAPACITY; &#125; /** * Save the state of the &lt;tt&gt;HashMap&lt;/tt&gt; instance to a stream (i.e., * serialize it). * * @serialData The &lt;i&gt;capacity&lt;/i&gt; of the HashMap (the length of the * bucket array) is emitted (int), followed by the * &lt;i&gt;size&lt;/i&gt; (an int, the number of key-value * mappings), followed by the key (Object) and value (Object) * for each key-value mapping. The key-value mappings are * emitted in no particular order. */ private void writeObject(java.io.ObjectOutputStream s) throws IOException &#123; int buckets = capacity(); // Write out the threshold, loadfactor, and any hidden stuff s.defaultWriteObject(); s.writeInt(buckets); s.writeInt(size); internalWriteEntries(s); &#125; /** * Reconstitute the &#123;@code HashMap&#125; instance from a stream (i.e., * deserialize it). */ private void readObject(java.io.ObjectInputStream s) throws IOException, ClassNotFoundException &#123; // Read in the threshold (ignored), loadfactor, and any hidden stuff s.defaultReadObject(); reinitialize(); if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new InvalidObjectException(\"Illegal load factor: \" + loadFactor); s.readInt(); // Read and ignore number of buckets int mappings = s.readInt(); // Read number of mappings (size) if (mappings &lt; 0) throw new InvalidObjectException(\"Illegal mappings count: \" + mappings); else if (mappings &gt; 0) &#123; // (if zero, use defaults) // Size the table using given load factor only if within // range of 0.25...4.0 float lf = Math.min(Math.max(0.25f, loadFactor), 4.0f); float fc = (float)mappings / lf + 1.0f; int cap = ((fc &lt; DEFAULT_INITIAL_CAPACITY) ? DEFAULT_INITIAL_CAPACITY : (fc &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : tableSizeFor((int)fc)); float ft = (float)cap * lf; threshold = ((cap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; MAXIMUM_CAPACITY) ? (int)ft : Integer.MAX_VALUE); @SuppressWarnings(&#123;\"rawtypes\",\"unchecked\"&#125;) Node&lt;K,V&gt;[] tab = (Node&lt;K,V&gt;[])new Node[cap]; table = tab; // Read the keys and values, and put the mappings in the HashMap for (int i = 0; i &lt; mappings; i++) &#123; @SuppressWarnings(\"unchecked\") K key = (K) s.readObject(); @SuppressWarnings(\"unchecked\") V value = (V) s.readObject(); putVal(hash(key), key, value, false, false); &#125; &#125; &#125; /* ------------------------------------------------------------ */ // iterators abstract class HashIterator &#123; Node&lt;K,V&gt; next; // next entry to return Node&lt;K,V&gt; current; // current entry int expectedModCount; // for fast-fail int index; // current slot HashIterator() &#123; expectedModCount = modCount; Node&lt;K,V&gt;[] t = table; current = next = null; index = 0; if (t != null &amp;&amp; size &gt; 0) &#123; // advance to first entry do &#123;&#125; while (index &lt; t.length &amp;&amp; (next = t[index++]) == null); &#125; &#125; public final boolean hasNext() &#123; return next != null; &#125; final Node&lt;K,V&gt; nextNode() &#123; Node&lt;K,V&gt;[] t; Node&lt;K,V&gt; e = next; if (modCount != expectedModCount) throw new ConcurrentModificationException(); if (e == null) throw new NoSuchElementException(); if ((next = (current = e).next) == null &amp;&amp; (t = table) != null) &#123; do &#123;&#125; while (index &lt; t.length &amp;&amp; (next = t[index++]) == null); &#125; return e; &#125; public final void remove() &#123; Node&lt;K,V&gt; p = current; if (p == null) throw new IllegalStateException(); if (modCount != expectedModCount) throw new ConcurrentModificationException(); current = null; K key = p.key; removeNode(hash(key), key, null, false, false); expectedModCount = modCount; &#125; &#125; final class KeyIterator extends HashIterator implements Iterator&lt;K&gt; &#123; public final K next() &#123; return nextNode().key; &#125; &#125; final class ValueIterator extends HashIterator implements Iterator&lt;V&gt; &#123; public final V next() &#123; return nextNode().value; &#125; &#125; final class EntryIterator extends HashIterator implements Iterator&lt;Map.Entry&lt;K,V&gt;&gt; &#123; public final Map.Entry&lt;K,V&gt; next() &#123; return nextNode(); &#125; &#125; /* ------------------------------------------------------------ */ // spliterators static class HashMapSpliterator&lt;K,V&gt; &#123; final HashMap&lt;K,V&gt; map; Node&lt;K,V&gt; current; // current node int index; // current index, modified on advance/split int fence; // one past last index int est; // size estimate int expectedModCount; // for comodification checks HashMapSpliterator(HashMap&lt;K,V&gt; m, int origin, int fence, int est, int expectedModCount) &#123; this.map = m; this.index = origin; this.fence = fence; this.est = est; this.expectedModCount = expectedModCount; &#125; final int getFence() &#123; // initialize fence and size on first use int hi; if ((hi = fence) &lt; 0) &#123; HashMap&lt;K,V&gt; m = map; est = m.size; expectedModCount = m.modCount; Node&lt;K,V&gt;[] tab = m.table; hi = fence = (tab == null) ? 0 : tab.length; &#125; return hi; &#125; public final long estimateSize() &#123; getFence(); // force init return (long) est; &#125; &#125; static final class KeySpliterator&lt;K,V&gt; extends HashMapSpliterator&lt;K,V&gt; implements Spliterator&lt;K&gt; &#123; KeySpliterator(HashMap&lt;K,V&gt; m, int origin, int fence, int est, int expectedModCount) &#123; super(m, origin, fence, est, expectedModCount); &#125; public KeySpliterator&lt;K,V&gt; trySplit() &#123; int hi = getFence(), lo = index, mid = (lo + hi) &gt;&gt;&gt; 1; return (lo &gt;= mid || current != null) ? null : new KeySpliterator&lt;&gt;(map, lo, index = mid, est &gt;&gt;&gt;= 1, expectedModCount); &#125; public void forEachRemaining(Consumer&lt;? super K&gt; action) &#123; int i, hi, mc; if (action == null) throw new NullPointerException(); HashMap&lt;K,V&gt; m = map; Node&lt;K,V&gt;[] tab = m.table; if ((hi = fence) &lt; 0) &#123; mc = expectedModCount = m.modCount; hi = fence = (tab == null) ? 0 : tab.length; &#125; else mc = expectedModCount; if (tab != null &amp;&amp; tab.length &gt;= hi &amp;&amp; (i = index) &gt;= 0 &amp;&amp; (i &lt; (index = hi) || current != null)) &#123; Node&lt;K,V&gt; p = current; current = null; do &#123; if (p == null) p = tab[i++]; else &#123; action.accept(p.key); p = p.next; &#125; &#125; while (p != null || i &lt; hi); if (m.modCount != mc) throw new ConcurrentModificationException(); &#125; &#125; public boolean tryAdvance(Consumer&lt;? super K&gt; action) &#123; int hi; if (action == null) throw new NullPointerException(); Node&lt;K,V&gt;[] tab = map.table; if (tab != null &amp;&amp; tab.length &gt;= (hi = getFence()) &amp;&amp; index &gt;= 0) &#123; while (current != null || index &lt; hi) &#123; if (current == null) current = tab[index++]; else &#123; K k = current.key; current = current.next; action.accept(k); if (map.modCount != expectedModCount) throw new ConcurrentModificationException(); return true; &#125; &#125; &#125; return false; &#125; public int characteristics() &#123; return (fence &lt; 0 || est == map.size ? Spliterator.SIZED : 0) | Spliterator.DISTINCT; &#125; &#125; static final class ValueSpliterator&lt;K,V&gt; extends HashMapSpliterator&lt;K,V&gt; implements Spliterator&lt;V&gt; &#123; ValueSpliterator(HashMap&lt;K,V&gt; m, int origin, int fence, int est, int expectedModCount) &#123; super(m, origin, fence, est, expectedModCount); &#125; public ValueSpliterator&lt;K,V&gt; trySplit() &#123; int hi = getFence(), lo = index, mid = (lo + hi) &gt;&gt;&gt; 1; return (lo &gt;= mid || current != null) ? null : new ValueSpliterator&lt;&gt;(map, lo, index = mid, est &gt;&gt;&gt;= 1, expectedModCount); &#125; public void forEachRemaining(Consumer&lt;? super V&gt; action) &#123; int i, hi, mc; if (action == null) throw new NullPointerException(); HashMap&lt;K,V&gt; m = map; Node&lt;K,V&gt;[] tab = m.table; if ((hi = fence) &lt; 0) &#123; mc = expectedModCount = m.modCount; hi = fence = (tab == null) ? 0 : tab.length; &#125; else mc = expectedModCount; if (tab != null &amp;&amp; tab.length &gt;= hi &amp;&amp; (i = index) &gt;= 0 &amp;&amp; (i &lt; (index = hi) || current != null)) &#123; Node&lt;K,V&gt; p = current; current = null; do &#123; if (p == null) p = tab[i++]; else &#123; action.accept(p.value); p = p.next; &#125; &#125; while (p != null || i &lt; hi); if (m.modCount != mc) throw new ConcurrentModificationException(); &#125; &#125; public boolean tryAdvance(Consumer&lt;? super V&gt; action) &#123; int hi; if (action == null) throw new NullPointerException(); Node&lt;K,V&gt;[] tab = map.table; if (tab != null &amp;&amp; tab.length &gt;= (hi = getFence()) &amp;&amp; index &gt;= 0) &#123; while (current != null || index &lt; hi) &#123; if (current == null) current = tab[index++]; else &#123; V v = current.value; current = current.next; action.accept(v); if (map.modCount != expectedModCount) throw new ConcurrentModificationException(); return true; &#125; &#125; &#125; return false; &#125; public int characteristics() &#123; return (fence &lt; 0 || est == map.size ? Spliterator.SIZED : 0); &#125; &#125; static final class EntrySpliterator&lt;K,V&gt; extends HashMapSpliterator&lt;K,V&gt; implements Spliterator&lt;Map.Entry&lt;K,V&gt;&gt; &#123; EntrySpliterator(HashMap&lt;K,V&gt; m, int origin, int fence, int est, int expectedModCount) &#123; super(m, origin, fence, est, expectedModCount); &#125; public EntrySpliterator&lt;K,V&gt; trySplit() &#123; int hi = getFence(), lo = index, mid = (lo + hi) &gt;&gt;&gt; 1; return (lo &gt;= mid || current != null) ? null : new EntrySpliterator&lt;&gt;(map, lo, index = mid, est &gt;&gt;&gt;= 1, expectedModCount); &#125; public void forEachRemaining(Consumer&lt;? super Map.Entry&lt;K,V&gt;&gt; action) &#123; int i, hi, mc; if (action == null) throw new NullPointerException(); HashMap&lt;K,V&gt; m = map; Node&lt;K,V&gt;[] tab = m.table; if ((hi = fence) &lt; 0) &#123; mc = expectedModCount = m.modCount; hi = fence = (tab == null) ? 0 : tab.length; &#125; else mc = expectedModCount; if (tab != null &amp;&amp; tab.length &gt;= hi &amp;&amp; (i = index) &gt;= 0 &amp;&amp; (i &lt; (index = hi) || current != null)) &#123; Node&lt;K,V&gt; p = current; current = null; do &#123; if (p == null) p = tab[i++]; else &#123; action.accept(p); p = p.next; &#125; &#125; while (p != null || i &lt; hi); if (m.modCount != mc) throw new ConcurrentModificationException(); &#125; &#125; public boolean tryAdvance(Consumer&lt;? super Map.Entry&lt;K,V&gt;&gt; action) &#123; int hi; if (action == null) throw new NullPointerException(); Node&lt;K,V&gt;[] tab = map.table; if (tab != null &amp;&amp; tab.length &gt;= (hi = getFence()) &amp;&amp; index &gt;= 0) &#123; while (current != null || index &lt; hi) &#123; if (current == null) current = tab[index++]; else &#123; Node&lt;K,V&gt; e = current; current = current.next; action.accept(e); if (map.modCount != expectedModCount) throw new ConcurrentModificationException(); return true; &#125; &#125; &#125; return false; &#125; public int characteristics() &#123; return (fence &lt; 0 || est == map.size ? Spliterator.SIZED : 0) | Spliterator.DISTINCT; &#125; &#125; /* ------------------------------------------------------------ */ // LinkedHashMap support /* * The following package-protected methods are designed to be * overridden by LinkedHashMap, but not by any other subclass. * Nearly all other internal methods are also package-protected * but are declared final, so can be used by LinkedHashMap, view * classes, and HashSet. */ // Create a regular (non-tree) node Node&lt;K,V&gt; newNode(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; return new Node&lt;&gt;(hash, key, value, next); &#125; // For conversion from TreeNodes to plain nodes Node&lt;K,V&gt; replacementNode(Node&lt;K,V&gt; p, Node&lt;K,V&gt; next) &#123; return new Node&lt;&gt;(p.hash, p.key, p.value, next); &#125; // Create a tree bin node TreeNode&lt;K,V&gt; newTreeNode(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; return new TreeNode&lt;&gt;(hash, key, value, next); &#125; // For treeifyBin TreeNode&lt;K,V&gt; replacementTreeNode(Node&lt;K,V&gt; p, Node&lt;K,V&gt; next) &#123; return new TreeNode&lt;&gt;(p.hash, p.key, p.value, next); &#125; /** * Reset to initial default state. Called by clone and readObject. */ void reinitialize() &#123; table = null; entrySet = null; keySet = null; values = null; modCount = 0; threshold = 0; size = 0; &#125; // Callbacks to allow LinkedHashMap post-actions void afterNodeAccess(Node&lt;K,V&gt; p) &#123; &#125; void afterNodeInsertion(boolean evict) &#123; &#125; void afterNodeRemoval(Node&lt;K,V&gt; p) &#123; &#125; // Called only from writeObject, to ensure compatible ordering. void internalWriteEntries(java.io.ObjectOutputStream s) throws IOException &#123; Node&lt;K,V&gt;[] tab; if (size &gt; 0 &amp;&amp; (tab = table) != null) &#123; for (int i = 0; i &lt; tab.length; ++i) &#123; for (Node&lt;K,V&gt; e = tab[i]; e != null; e = e.next) &#123; s.writeObject(e.key); s.writeObject(e.value); &#125; &#125; &#125; &#125; /* ------------------------------------------------------------ */ // Tree bins /** * Entry for Tree bins. Extends LinkedHashMap.Entry (which in turn * extends Node) so can be used as extension of either regular or * linked node. */ static final class TreeNode&lt;K,V&gt; extends LinkedHashMap.Entry&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; parent; // red-black tree links TreeNode&lt;K,V&gt; left; TreeNode&lt;K,V&gt; right; TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red; TreeNode(int hash, K key, V val, Node&lt;K,V&gt; next) &#123; super(hash, key, val, next); &#125; /** * Returns root of tree containing this node. */ final TreeNode&lt;K,V&gt; root() &#123; for (TreeNode&lt;K,V&gt; r = this, p;;) &#123; if ((p = r.parent) == null) return r; r = p; &#125; &#125; /** * Ensures that the given root is the first node of its bin. */ static &lt;K,V&gt; void moveRootToFront(Node&lt;K,V&gt;[] tab, TreeNode&lt;K,V&gt; root) &#123; int n; if (root != null &amp;&amp; tab != null &amp;&amp; (n = tab.length) &gt; 0) &#123; int index = (n - 1) &amp; root.hash; TreeNode&lt;K,V&gt; first = (TreeNode&lt;K,V&gt;)tab[index]; if (root != first) &#123; Node&lt;K,V&gt; rn; tab[index] = root; TreeNode&lt;K,V&gt; rp = root.prev; if ((rn = root.next) != null) ((TreeNode&lt;K,V&gt;)rn).prev = rp; if (rp != null) rp.next = rn; if (first != null) first.prev = root; root.next = first; root.prev = null; &#125; assert checkInvariants(root); &#125; &#125; /** * Finds the node starting at root p with the given hash and key. * The kc argument caches comparableClassFor(key) upon first use * comparing keys. */ final TreeNode&lt;K,V&gt; find(int h, Object k, Class&lt;?&gt; kc) &#123; TreeNode&lt;K,V&gt; p = this; do &#123; int ph, dir; K pk; TreeNode&lt;K,V&gt; pl = p.left, pr = p.right, q; if ((ph = p.hash) &gt; h) p = pl; else if (ph &lt; h) p = pr; else if ((pk = p.key) == k || (k != null &amp;&amp; k.equals(pk))) return p; else if (pl == null) p = pr; else if (pr == null) p = pl; else if ((kc != null || (kc = comparableClassFor(k)) != null) &amp;&amp; (dir = compareComparables(kc, k, pk)) != 0) p = (dir &lt; 0) ? pl : pr; else if ((q = pr.find(h, k, kc)) != null) return q; else p = pl; &#125; while (p != null); return null; &#125; /** * Calls find for root node. */ final TreeNode&lt;K,V&gt; getTreeNode(int h, Object k) &#123; return ((parent != null) ? root() : this).find(h, k, null); &#125; /** * Tie-breaking utility for ordering insertions when equal * hashCodes and non-comparable. We don't require a total * order, just a consistent insertion rule to maintain * equivalence across rebalancings. Tie-breaking further than * necessary simplifies testing a bit. */ static int tieBreakOrder(Object a, Object b) &#123; int d; if (a == null || b == null || (d = a.getClass().getName(). compareTo(b.getClass().getName())) == 0) d = (System.identityHashCode(a) &lt;= System.identityHashCode(b) ? -1 : 1); return d; &#125; /** * Forms tree of the nodes linked from this node. * @return root of tree */ final void treeify(Node&lt;K,V&gt;[] tab) &#123; TreeNode&lt;K,V&gt; root = null; for (TreeNode&lt;K,V&gt; x = this, next; x != null; x = next) &#123; next = (TreeNode&lt;K,V&gt;)x.next; x.left = x.right = null; if (root == null) &#123; x.parent = null; x.red = false; root = x; &#125; else &#123; K k = x.key; int h = x.hash; Class&lt;?&gt; kc = null; for (TreeNode&lt;K,V&gt; p = root;;) &#123; int dir, ph; K pk = p.key; if ((ph = p.hash) &gt; h) dir = -1; else if (ph &lt; h) dir = 1; else if ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) dir = tieBreakOrder(k, pk); TreeNode&lt;K,V&gt; xp = p; if ((p = (dir &lt;= 0) ? p.left : p.right) == null) &#123; x.parent = xp; if (dir &lt;= 0) xp.left = x; else xp.right = x; root = balanceInsertion(root, x); break; &#125; &#125; &#125; &#125; moveRootToFront(tab, root); &#125; /** * Returns a list of non-TreeNodes replacing those linked from * this node. */ final Node&lt;K,V&gt; untreeify(HashMap&lt;K,V&gt; map) &#123; Node&lt;K,V&gt; hd = null, tl = null; for (Node&lt;K,V&gt; q = this; q != null; q = q.next) &#123; Node&lt;K,V&gt; p = map.replacementNode(q, null); if (tl == null) hd = p; else tl.next = p; tl = p; &#125; return hd; &#125; /** * Tree version of putVal. */ final TreeNode&lt;K,V&gt; putTreeVal(HashMap&lt;K,V&gt; map, Node&lt;K,V&gt;[] tab, int h, K k, V v) &#123; Class&lt;?&gt; kc = null; boolean searched = false; TreeNode&lt;K,V&gt; root = (parent != null) ? root() : this; for (TreeNode&lt;K,V&gt; p = root;;) &#123; int dir, ph; K pk; if ((ph = p.hash) &gt; h) dir = -1; else if (ph &lt; h) dir = 1; else if ((pk = p.key) == k || (k != null &amp;&amp; k.equals(pk))) return p; else if ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) &#123; if (!searched) &#123; TreeNode&lt;K,V&gt; q, ch; searched = true; if (((ch = p.left) != null &amp;&amp; (q = ch.find(h, k, kc)) != null) || ((ch = p.right) != null &amp;&amp; (q = ch.find(h, k, kc)) != null)) return q; &#125; dir = tieBreakOrder(k, pk); &#125; TreeNode&lt;K,V&gt; xp = p; if ((p = (dir &lt;= 0) ? p.left : p.right) == null) &#123; Node&lt;K,V&gt; xpn = xp.next; TreeNode&lt;K,V&gt; x = map.newTreeNode(h, k, v, xpn); if (dir &lt;= 0) xp.left = x; else xp.right = x; xp.next = x; x.parent = x.prev = xp; if (xpn != null) ((TreeNode&lt;K,V&gt;)xpn).prev = x; moveRootToFront(tab, balanceInsertion(root, x)); return null; &#125; &#125; &#125; /** * Removes the given node, that must be present before this call. * This is messier than typical red-black deletion code because we * cannot swap the contents of an interior node with a leaf * successor that is pinned by \"next\" pointers that are accessible * independently during traversal. So instead we swap the tree * linkages. If the current tree appears to have too few nodes, * the bin is converted back to a plain bin. (The test triggers * somewhere between 2 and 6 nodes, depending on tree structure). */ final void removeTreeNode(HashMap&lt;K,V&gt; map, Node&lt;K,V&gt;[] tab, boolean movable) &#123; int n; if (tab == null || (n = tab.length) == 0) return; int index = (n - 1) &amp; hash; TreeNode&lt;K,V&gt; first = (TreeNode&lt;K,V&gt;)tab[index], root = first, rl; TreeNode&lt;K,V&gt; succ = (TreeNode&lt;K,V&gt;)next, pred = prev; if (pred == null) tab[index] = first = succ; else pred.next = succ; if (succ != null) succ.prev = pred; if (first == null) return; if (root.parent != null) root = root.root(); if (root == null || root.right == null || (rl = root.left) == null || rl.left == null) &#123; tab[index] = first.untreeify(map); // too small return; &#125; TreeNode&lt;K,V&gt; p = this, pl = left, pr = right, replacement; if (pl != null &amp;&amp; pr != null) &#123; TreeNode&lt;K,V&gt; s = pr, sl; while ((sl = s.left) != null) // find successor s = sl; boolean c = s.red; s.red = p.red; p.red = c; // swap colors TreeNode&lt;K,V&gt; sr = s.right; TreeNode&lt;K,V&gt; pp = p.parent; if (s == pr) &#123; // p was s's direct parent p.parent = s; s.right = p; &#125; else &#123; TreeNode&lt;K,V&gt; sp = s.parent; if ((p.parent = sp) != null) &#123; if (s == sp.left) sp.left = p; else sp.right = p; &#125; if ((s.right = pr) != null) pr.parent = s; &#125; p.left = null; if ((p.right = sr) != null) sr.parent = p; if ((s.left = pl) != null) pl.parent = s; if ((s.parent = pp) == null) root = s; else if (p == pp.left) pp.left = s; else pp.right = s; if (sr != null) replacement = sr; else replacement = p; &#125; else if (pl != null) replacement = pl; else if (pr != null) replacement = pr; else replacement = p; if (replacement != p) &#123; TreeNode&lt;K,V&gt; pp = replacement.parent = p.parent; if (pp == null) root = replacement; else if (p == pp.left) pp.left = replacement; else pp.right = replacement; p.left = p.right = p.parent = null; &#125; TreeNode&lt;K,V&gt; r = p.red ? root : balanceDeletion(root, replacement); if (replacement == p) &#123; // detach TreeNode&lt;K,V&gt; pp = p.parent; p.parent = null; if (pp != null) &#123; if (p == pp.left) pp.left = null; else if (p == pp.right) pp.right = null; &#125; &#125; if (movable) moveRootToFront(tab, r); &#125; /** * Splits nodes in a tree bin into lower and upper tree bins, * or untreeifies if now too small. Called only from resize; * see above discussion about split bits and indices. * * @param map the map * @param tab the table for recording bin heads * @param index the index of the table being split * @param bit the bit of hash to split on */ final void split(HashMap&lt;K,V&gt; map, Node&lt;K,V&gt;[] tab, int index, int bit) &#123; TreeNode&lt;K,V&gt; b = this; // Relink into lo and hi lists, preserving order TreeNode&lt;K,V&gt; loHead = null, loTail = null; TreeNode&lt;K,V&gt; hiHead = null, hiTail = null; int lc = 0, hc = 0; for (TreeNode&lt;K,V&gt; e = b, next; e != null; e = next) &#123; next = (TreeNode&lt;K,V&gt;)e.next; e.next = null; if ((e.hash &amp; bit) == 0) &#123; if ((e.prev = loTail) == null) loHead = e; else loTail.next = e; loTail = e; ++lc; &#125; else &#123; if ((e.prev = hiTail) == null) hiHead = e; else hiTail.next = e; hiTail = e; ++hc; &#125; &#125; if (loHead != null) &#123; if (lc &lt;= UNTREEIFY_THRESHOLD) tab[index] = loHead.untreeify(map); else &#123; tab[index] = loHead; if (hiHead != null) // (else is already treeified) loHead.treeify(tab); &#125; &#125; if (hiHead != null) &#123; if (hc &lt;= UNTREEIFY_THRESHOLD) tab[index + bit] = hiHead.untreeify(map); else &#123; tab[index + bit] = hiHead; if (loHead != null) hiHead.treeify(tab); &#125; &#125; &#125; /* ------------------------------------------------------------ */ // Red-black tree methods, all adapted from CLR static &lt;K,V&gt; TreeNode&lt;K,V&gt; rotateLeft(TreeNode&lt;K,V&gt; root, TreeNode&lt;K,V&gt; p) &#123; TreeNode&lt;K,V&gt; r, pp, rl; if (p != null &amp;&amp; (r = p.right) != null) &#123; if ((rl = p.right = r.left) != null) rl.parent = p; if ((pp = r.parent = p.parent) == null) (root = r).red = false; else if (pp.left == p) pp.left = r; else pp.right = r; r.left = p; p.parent = r; &#125; return root; &#125; static &lt;K,V&gt; TreeNode&lt;K,V&gt; rotateRight(TreeNode&lt;K,V&gt; root, TreeNode&lt;K,V&gt; p) &#123; TreeNode&lt;K,V&gt; l, pp, lr; if (p != null &amp;&amp; (l = p.left) != null) &#123; if ((lr = p.left = l.right) != null) lr.parent = p; if ((pp = l.parent = p.parent) == null) (root = l).red = false; else if (pp.right == p) pp.right = l; else pp.left = l; l.right = p; p.parent = l; &#125; return root; &#125; static &lt;K,V&gt; TreeNode&lt;K,V&gt; balanceInsertion(TreeNode&lt;K,V&gt; root, TreeNode&lt;K,V&gt; x) &#123; x.red = true; for (TreeNode&lt;K,V&gt; xp, xpp, xppl, xppr;;) &#123; if ((xp = x.parent) == null) &#123; x.red = false; return x; &#125; else if (!xp.red || (xpp = xp.parent) == null) return root; if (xp == (xppl = xpp.left)) &#123; if ((xppr = xpp.right) != null &amp;&amp; xppr.red) &#123; xppr.red = false; xp.red = false; xpp.red = true; x = xpp; &#125; else &#123; if (x == xp.right) &#123; root = rotateLeft(root, x = xp); xpp = (xp = x.parent) == null ? null : xp.parent; &#125; if (xp != null) &#123; xp.red = false; if (xpp != null) &#123; xpp.red = true; root = rotateRight(root, xpp); &#125; &#125; &#125; &#125; else &#123; if (xppl != null &amp;&amp; xppl.red) &#123; xppl.red = false; xp.red = false; xpp.red = true; x = xpp; &#125; else &#123; if (x == xp.left) &#123; root = rotateRight(root, x = xp); xpp = (xp = x.parent) == null ? null : xp.parent; &#125; if (xp != null) &#123; xp.red = false; if (xpp != null) &#123; xpp.red = true; root = rotateLeft(root, xpp); &#125; &#125; &#125; &#125; &#125; &#125; static &lt;K,V&gt; TreeNode&lt;K,V&gt; balanceDeletion(TreeNode&lt;K,V&gt; root, TreeNode&lt;K,V&gt; x) &#123; for (TreeNode&lt;K,V&gt; xp, xpl, xpr;;) &#123; if (x == null || x == root) return root; else if ((xp = x.parent) == null) &#123; x.red = false; return x; &#125; else if (x.red) &#123; x.red = false; return root; &#125; else if ((xpl = xp.left) == x) &#123; if ((xpr = xp.right) != null &amp;&amp; xpr.red) &#123; xpr.red = false; xp.red = true; root = rotateLeft(root, xp); xpr = (xp = x.parent) == null ? null : xp.right; &#125; if (xpr == null) x = xp; else &#123; TreeNode&lt;K,V&gt; sl = xpr.left, sr = xpr.right; if ((sr == null || !sr.red) &amp;&amp; (sl == null || !sl.red)) &#123; xpr.red = true; x = xp; &#125; else &#123; if (sr == null || !sr.red) &#123; if (sl != null) sl.red = false; xpr.red = true; root = rotateRight(root, xpr); xpr = (xp = x.parent) == null ? null : xp.right; &#125; if (xpr != null) &#123; xpr.red = (xp == null) ? false : xp.red; if ((sr = xpr.right) != null) sr.red = false; &#125; if (xp != null) &#123; xp.red = false; root = rotateLeft(root, xp); &#125; x = root; &#125; &#125; &#125; else &#123; // symmetric if (xpl != null &amp;&amp; xpl.red) &#123; xpl.red = false; xp.red = true; root = rotateRight(root, xp); xpl = (xp = x.parent) == null ? null : xp.left; &#125; if (xpl == null) x = xp; else &#123; TreeNode&lt;K,V&gt; sl = xpl.left, sr = xpl.right; if ((sl == null || !sl.red) &amp;&amp; (sr == null || !sr.red)) &#123; xpl.red = true; x = xp; &#125; else &#123; if (sl == null || !sl.red) &#123; if (sr != null) sr.red = false; xpl.red = true; root = rotateLeft(root, xpl); xpl = (xp = x.parent) == null ? null : xp.left; &#125; if (xpl != null) &#123; xpl.red = (xp == null) ? false : xp.red; if ((sl = xpl.left) != null) sl.red = false; &#125; if (xp != null) &#123; xp.red = false; root = rotateRight(root, xp); &#125; x = root; &#125; &#125; &#125; &#125; &#125; /** * Recursive invariant check */ static &lt;K,V&gt; boolean checkInvariants(TreeNode&lt;K,V&gt; t) &#123; TreeNode&lt;K,V&gt; tp = t.parent, tl = t.left, tr = t.right, tb = t.prev, tn = (TreeNode&lt;K,V&gt;)t.next; if (tb != null &amp;&amp; tb.next != t) return false; if (tn != null &amp;&amp; tn.prev != t) return false; if (tp != null &amp;&amp; t != tp.left &amp;&amp; t != tp.right) return false; if (tl != null &amp;&amp; (tl.parent != t || tl.hash &gt; t.hash)) return false; if (tr != null &amp;&amp; (tr.parent != t || tr.hash &lt; t.hash)) return false; if (t.red &amp;&amp; tl != null &amp;&amp; tl.red &amp;&amp; tr != null &amp;&amp; tr.red) return false; if (tl != null &amp;&amp; !checkInvariants(tl)) return false; if (tr != null &amp;&amp; !checkInvariants(tr)) return false; return true; &#125; &#125;&#125;","raw":null,"content":null,"categories":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/tags/Java/"}]},{"title":"生产者消费者模式","slug":"生产者消费者模式","date":"2017-04-14T08:16:06.000Z","updated":"2017-04-14T08:16:06.000Z","comments":true,"path":"2017/04/14/生产者消费者模式/","link":"","permalink":"http://linbingdong.com/2017/04/14/生产者消费者模式/","excerpt":"在并发编程中使用生产者和消费者模式能够解决绝大多数并发问题。该模式通过平衡生产线程和消费线程的工作能力来提高程序的整体处理数据的速度。","text":"在并发编程中使用生产者和消费者模式能够解决绝大多数并发问题。该模式通过平衡生产线程和消费线程的工作能力来提高程序的整体处理数据的速度。 什么是生产者消费者模式生产者消费者模式通过一个容器（比如阻塞队列 BlockingQueue ）来解决生产者和消费者的紧耦合问题。生产者和消费者之间不直接通信，而是通过阻塞队列来通信。生产者生产完数据后不用等待消费者处理，而是直接将生产的数据放入阻塞队列；消费者不从生产者那里要数据，而是直接从阻塞队列里取。阻塞队列就相当于一个缓冲区，平衡了生产者和消费者的处理能力。 阻塞队列提供了可阻塞的 put 和 take 方法，以及支持定时的 offer 和 poll 方法。如果队列已经满了，那么 put 方法将阻塞直到有空间可用；如果队列为空，那么 take 方法将会阻塞直到阻塞队列里有元素可用。 为什么要用生产者消费者模式 生产者消费者模式可以使生产者类和消费者类解耦，这样能大大简化开发过程，因为它消除了生产者类和消费者类之间的代码依赖性。 生产者消费者模式可以解决生产消费能力不均衡的问题，从而提高整体处理数据的速度。假设不使用生产者消费者模式，如果生产者处理速度很快，而消费者处理速度很慢，那么生产者就必须等待消费者处理完，才能继续生产数据；有了生产者消费者模式，生产者就可以不用等待消费者了，这样提高了生产者生产数据的效率。 使用阻塞队列实现生产者消费者模式代码如下: /** * Created by lbd */import java.util.concurrent.BlockingQueue;import java.util.concurrent.LinkedBlockingDeque;public class ProducerConsumerPattern &#123; public static void main(String[] args) &#123; BlockingQueue&lt;String&gt; blockingQueue = new LinkedBlockingDeque&lt;&gt;(10); for (int i = 0; i &lt; 20; i++) &#123; new Thread(new Producer(blockingQueue), \"Producer\" + i).start(); &#125; for (int i = 0; i &lt; 2; i++) &#123; new Thread(new Consumer(blockingQueue), \"Consumer\" + i).start(); &#125; &#125;&#125;class Producer implements Runnable &#123; private BlockingQueue&lt;String&gt; blockingQueue; public Producer(BlockingQueue&lt;String&gt; blockingQueue) &#123; this.blockingQueue = blockingQueue; &#125; @Override public void run() &#123; try &#123; String product = \"Produced by \" + Thread.currentThread().getName(); blockingQueue.put(product); System.out.println(Thread.currentThread().getName() + \" produced a product\"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125;class Consumer implements Runnable &#123; private BlockingQueue&lt;String&gt; blockingQueue; public Consumer(BlockingQueue&lt;String&gt; blockingQueue) &#123; this.blockingQueue = blockingQueue; &#125; @Override public void run() &#123; while (true) &#123; try &#123; String product = blockingQueue.take(); System.out.println(Thread.currentThread().getName() + \" cunsumed product \"+ product); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 输出： Producer0 produced a productProducer2 produced a productProducer3 produced a productProducer1 produced a productProducer4 produced a productProducer5 produced a productProducer6 produced a productProducer7 produced a productProducer8 produced a productProducer9 produced a productConsumer0 cunsumed product Produced by Producer0Producer11 produced a productProducer12 produced a productProducer10 produced a productConsumer1 cunsumed product Produced by Producer1Consumer0 cunsumed product Produced by Producer2Producer13 produced a productConsumer1 cunsumed product Produced by Producer3Consumer1 cunsumed product Produced by Producer5Producer15 produced a productConsumer0 cunsumed product Produced by Producer4Producer16 produced a productConsumer1 cunsumed product Produced by Producer6Producer14 produced a productProducer18 produced a productConsumer1 cunsumed product Produced by Producer8Producer17 produced a productConsumer0 cunsumed product Produced by Producer7Producer19 produced a productConsumer1 cunsumed product Produced by Producer9Consumer0 cunsumed product Produced by Producer10Consumer1 cunsumed product Produced by Producer11Consumer0 cunsumed product Produced by Producer12Consumer1 cunsumed product Produced by Producer13Consumer0 cunsumed product Produced by Producer14Consumer1 cunsumed product Produced by Producer15Consumer0 cunsumed product Produced by Producer16Consumer1 cunsumed product Produced by Producer17Consumer0 cunsumed product Produced by Producer18Consumer1 cunsumed product Produced by Producer19","raw":null,"content":null,"categories":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/tags/Java/"}]},{"title":"程序员必读书单","slug":"程序员必读书单","date":"2017-04-14T08:15:36.000Z","updated":"2017-04-14T08:15:36.000Z","comments":true,"path":"2017/04/14/程序员必读书单/","link":"","permalink":"http://linbingdong.com/2017/04/14/程序员必读书单/","excerpt":"","text":"Lucida大神这篇文章写得非常详细，点此查看。","raw":null,"content":null,"categories":[{"name":"学习资料","slug":"学习资料","permalink":"http://linbingdong.com/categories/学习资料/"}],"tags":[{"name":"好文转载","slug":"好文转载","permalink":"http://linbingdong.com/tags/好文转载/"},{"name":"必读书单","slug":"必读书单","permalink":"http://linbingdong.com/tags/必读书单/"},{"name":"学习资料","slug":"学习资料","permalink":"http://linbingdong.com/tags/学习资料/"}]},{"title":"tmpfs","slug":"tmpfs","date":"2017-04-11T06:03:33.000Z","updated":"2017-04-11T06:03:33.000Z","comments":true,"path":"2017/04/11/tmpfs/","link":"","permalink":"http://linbingdong.com/2017/04/11/tmpfs/","excerpt":"tmpfs是一种基于内存的文件系统，它和虚拟磁盘ramdisk比较类似像，但不完全相同。和ramdisk一样，tmpfs可以使用内存，但它也可以使用swap分区来存储。而且传统的ramdisk是个块设备，要用mkfs格式化才能使用；而tmpfs是一个文件系统，并不是块设备，不需要格式化。tmpfs是最好的基于内存的文件系统。","text":"tmpfs是一种基于内存的文件系统，它和虚拟磁盘ramdisk比较类似像，但不完全相同。和ramdisk一样，tmpfs可以使用内存，但它也可以使用swap分区来存储。而且传统的ramdisk是个块设备，要用mkfs格式化才能使用；而tmpfs是一个文件系统，并不是块设备，不需要格式化。tmpfs是最好的基于内存的文件系统。 用一个简单的mount命令就可以创建tmpfs文件系统： mount tmpfs -t tmpfs /data/test -o size=10g 将tmpfs挂载到/data/mfs目录后，往/data/mfs写入的内容都会写到内存里。如果需要重新设置分配的内存大小，可以先umount，再重新挂载： umount /data/mfsmount tmpfs -t tmpfs /data/test -o size=20g 也可以在/etc/fstab里设置。 MooseFS是分布式文件系统，正常情况下文件是写到chunkserver节点的磁盘里。如果想让文件写入内存，可以将tmpfs挂到MooseFS的chunkserver节点的数据目录下： mount tmpfs -t tmpfs /data/mfs -o size=50g 这样以后往MooseFS里写数据就都写到内存里了。 注意： 挂载前应该先关闭MooseFS，挂载后再启动MooseFS。没有启动MooseFS的话执行df -h命令会无响应。 该方法只是用来尝鲜，如果想使用基于内存的分布式文件系统，应该使用类似Alluxio这样的组件。","raw":null,"content":null,"categories":[{"name":"Linux","slug":"Linux","permalink":"http://linbingdong.com/categories/Linux/"}],"tags":[{"name":"tmpfs","slug":"tmpfs","permalink":"http://linbingdong.com/tags/tmpfs/"}]},{"title":"分布式系统(Distributed System)资料大全","slug":"分布式系统(Distributed System)资料大全","date":"2017-04-09T15:17:43.000Z","updated":"2017-04-09T15:17:43.000Z","comments":true,"path":"2017/04/09/分布式系统(Distributed System)资料大全/","link":"","permalink":"http://linbingdong.com/2017/04/09/分布式系统(Distributed System)资料大全/","excerpt":"分布式系统资料大全，enjoy！","text":"分布式系统资料大全，enjoy！ 《Reconfigurable Distributed Storage for Dynamic Networks》 介绍:这是一篇介绍在动态网络里面实现分布式系统重构的paper.论文的作者(导师)是MIT读博的时候是做分布式系统的研究的,现在在NUS带学生,不仅仅是分布式系统,还有无线网络.如果感兴趣可以去他的主页了解. 《Distributed porgramming liboratory》 介绍:分布式编程实验室,他们发表的很多的paper,其中不仅仅是学术研究,还有一些工业界应用的论文. 《MIT Theory of Distributed Systems》 介绍:麻省理工的分布式系统理论主页,作者南希·林奇在2002年证明了CAP理论,并且著《分布式算法》一书. 《Notes on Distributed Systems for Young Bloods》 介绍:分布式系统搭建初期的一些建议 《Principles of Distributed Computing》 介绍:分布式计算原理课程 《Google’s Globally-Distributed Database》 介绍:Google全球分布式数据介绍,中文版 《The Architecture Of Algolia’s Distributed Search Network》 介绍:Algolia的分布式搜索网络的体系架构介绍 《Build up a High Availability Distributed Key-Value Store》 介绍:构建高可用分布式Key-Value存储系统 《Distributed Search Engine with Nanomsg and Bond》 介绍:Nanomsg和Bond的分布式搜索引擎 《Distributed Processing With MongoDB And Mongothon》 介绍:使用MongoDB和Mongothon进行分布式处理 《Salt: Combining ACID and BASE in a Distributed Database》 介绍:分布式数据库中把ACID与BASE结合使用. 《Makes it easy to understand Paxos for Distributed Systems》 介绍:理解的Paxos的分布式系统,参考阅读:关于Paxos的历史 《There is No Now Problems with simultaneity in distributed systems》 介绍:There is No Now Problems with simultaneity in distributed systems 《Distributed Systems》 介绍:伦敦大学学院分布式系统课程课件. 《Distributed systems for fun and profit》 介绍:分布式系统电子书籍. 《Distributed Systems Spring 2015》 介绍:卡内基梅隆大学春季分布式课程主页 《Distributed Systems: Concepts and Design (5th Edition)》 介绍: 电子书,分布式系统概念与设计(第五版) 《走向分布式》 介绍:这是一位台湾网友 ccshih 的文字，短短的篇幅介绍了分布式系统的若干要点。pdf 《Introduction to Distributed Systems Spring 2013》 介绍:清华大学分布式系统课程主页,里面的schedule栏目有很多宝贵的资源 《Distributed systems》 介绍:免费的在线分布式系统书籍 《Some good resources for learning about distributed computing》 介绍:Quora上面的一篇关于学习分布式计算的资源. 《Spanner: Google’s Globally-Distributed Database》 介绍:这个是第一个全球意义上的分布式数据库，也是Google的作品。其中介绍了很多一致性方面的设计考虑，为了简单的逻辑设计，还采用了原子钟，同样在分布式系统方面具有很强的借鉴意义. 《The Chubby lock service for loosely-coupled distributed systems》 介绍:Google的统面向松散耦合的分布式系统的锁服务,这篇论文详细介绍了Google的分布式锁实现机制Chubby。Chubby是一个基于文件实现的分布式锁，Google的Bigtable、Mapreduce和Spanner服务都是在这个基础上构建的，所以Chubby实际上是Google分布式事务的基础，具有非常高的参考价值。另外，著名的zookeeper就是基于Chubby的开源实现.推荐The google stack,Youtube:The Chubby lock service for loosely-coupled distributed systems 《Sinfonia: a new paradigm for building scalable distributed systems》 介绍:这篇论文是SOSP2007的Best Paper，阐述了一种构建分布式文件系统的范式方法，个人感觉非常有用。淘宝在构建TFS、OceanBase和Tair这些系统时都充分参考了这篇论文. 《Data-Intensive Text Processing with MapReduce》 介绍:Ebook:Data-Intensive Text Processing with MapReduce. 《Design and Implementation of a Query Processor for a Trusted Distributed Data Base Management System》 介绍:Design and Implementation of a Query Processor for a Trusted Distributed Data Base Management System. 《Distributed Query Processing》 介绍:分布式查询入门. 《Distributed Systems and the End of the API》 介绍:分布式系统和api总结. 《Distributed Query Reading》 介绍:分布式系统阅读论文，此外还推荐github上面的一个论文列表The Distributed Reader。 《Replication, atomicity and order in distributed systems》 介绍:Replication, atomicity and order in distributed systems 《MIT course:Distributed Systems》 介绍:2015年MIT分布式系统课程主页，这次用Golang作为授课语言。6.824 Distributed Systems课程主页 《Distributed systems for fun and profit》 介绍:免费分布式系统电子书。 《Ori：A Secure Distributed File System》 介绍:斯坦福开源的分布式文件系统。 《Availability in Globally Distributed Storage Systems》 介绍:Google论文：设计一个高可用的全球分布式存储系统。 《Calvin: Fast Distributed Transactions For Partitioned Database Systems》 介绍:对于分区数据库的分布式事务处理。 《Distributed Systems Building Block: Flake Ids》 介绍:Distributed Systems Building Block: Flake Ids. 《Introduction to Distributed System Design》 介绍:Google Code University课程，如何设计一个分布式系统。 《Sheepdog: Distributed Storage System for KVM》 介绍:KVM的分布式存储系统. 《Readings in Distributed Systems Systems》 介绍:分布式系统课程列表,包括数据库、算法等. 《Tera》 介绍:来自百度的分布式表格系统. 《Distributed systems: for fun and profit》 介绍:分布式系统的在线电子书. 《Distributed Systems Reading List》 介绍:分布式系统资料,此外还推荐Various articles about distributed systems. 《Designs, Lessons and Advice from Building Large Distributed Systems》 介绍:Designs, Lessons and Advice from Building Large Distributed Systems. 《Testing a Distributed System》 介绍:Testing a distributed system can be trying even under the best of circumstances. 《The Google File System》 介绍: 基于普通服务器构建超大规模文件系统的典型案例，主要面向大文件和批处理系统， 设计简单而实用。 GFS是google的重要基础设施， 大数据的基石， 也是Hadoop HDFS的参考对象。 主要技术特点包括： 假设硬件故障是常态（容错能力强）， 64MB大块， 单Master设计，Lease/链式复制， 支持追加写不支持随机写. 《Bigtable: A Distributed Storage System for Structured Data》 介绍:支持PB数据量级的多维非关系型大表， 在google内部应用广泛，大数据的奠基作品之一 ， Hbase就是参考BigTable设计。 Bigtable的主要技术特点包括： 基于GFS实现数据高可靠， 使用非原地更新技术（LSM树）实现数据修改， 通过range分区并实现自动伸缩等.中文版 《PacificA: Replication in Log-Based Distributed Storage Systems》 介绍:面向log-based存储的强一致的主从复制协议， 具有较强实用性。 这篇文章系统地讲述了主从复制系统应该考虑的问题， 能加深对主从强一致复制的理解程度。 技术特点： 支持强一致主从复制协议， 允许多种存储实现， 分布式的故障检测/Lease/集群成员管理方法. 《Object Storage on CRAQ, High-throughput chain replication for read-mostly workloads》 介绍:分布式存储论文:支持强一直的链式复制方法， 支持从多个副本读取数据,实现code. 《Finding a needle in Haystack: Facebook’s photo storage》 介绍:Facebook分布式Blob存储,主要用于存储图片. 主要技术特色:小文件合并成大文件,小文件元数据放在内存因此读写只需一次IO. 《Windows Azure Storage: A Highly Available Cloud Storage Service with Strong Consistency》 介绍: 微软的分布式存储平台, 除了支持类S3对象存储，还支持表格、队列等数据模型. 主要技术特点：采用Stream/Partition两层设计（类似BigTable）;写错（写满）就封存Extent,使得副本字节一致, 简化了选主和恢复操作; 将S3对象存储、表格、队列、块设备等融入到统一的底层存储架构中. 《Paxos Made Live – An Engineering Perspective》 介绍:从工程实现角度说明了Paxo在chubby系统的应用， 是理解Paxo协议及其应用场景的必备论文。 主要技术特点： paxo协议， replicated log， multi-paxo.参考阅读:关于Paxos的历史 《Dynamo: Amazon’s Highly Available Key-Value Store》 介绍:Amazon设计的高可用的kv系统,主要技术特点：综和运用一致性哈希,vector clock,最终一致性构建一个高可用的kv系统， 可应用于amazon购物车场景.新内容来自分布式存储必读论文 《Efficient Replica Maintenance for Distributed Storage Systems》 介绍:分布式存储系统中的副本存储问题. 《PADS: A Policy Architecture for Distributed Storage Systems》 介绍:分布式存储系统架构. 《The Chirp Distributed Filesystem》 介绍:开源分布式文件系统Chirp,对于想深入研究的开发者可以阅读文章的相关Papers. 《Time, Clocks, and the Ordering of Events in a Distributed System》 介绍:经典论文分布式时钟顺序的实现原理. 《Making reliable distributed systems in the presence of sodware errors》 介绍:面向软件错误构建可靠的分布式系统,中文笔记. 《MapReduce: Simplified Data Processing on Large Clusters》 介绍:MapReduce:超大集群的简单数据处理. 《Distributed Computer Systems Engineering》 介绍:麻省理工的分布式计算课程主页,里面的ppt和阅读列表很多干货. 《The Styx Architecture for Distributed Systems》 介绍:分布式系统Styx的架构剖析. 《What are some good resources for learning about distributed computing? Why?》 介绍:Quora上面的一个问答:有哪些关于分布式计算学习的好资源. 《RebornDB: The Next Generation Distributed Key-Value Store》 介绍:下一代分布式k-v存储数据库. 《Operating System Concepts Ninth Edition》 介绍:分布式系统归根结底还是需要操作系统的知识,这是耶鲁大学的操作系统概念书籍首页,里面有提供了第8版的在线电子版和最新的学习操作系统指南,学习分布式最好先学习操作系统. 《The Log: What every software engineer should know about real-time data’s unifying abstraction》 介绍:分布式系统Log剖析,非常的详细与精彩. 中文翻译 | 中文版笔记. 《Operating Systems Study Guide》 介绍:分布式系统基础之操作系统学习指南. 《分布式系统领域经典论文翻译集》 介绍:分布式系统领域经典论文翻译集. 《Maintaining performance in distributed systems》 介绍:分布式系统性能维护. 《Computer Science from the Bottom Up》 介绍:计算机科学，自底向上,小到机器码,大到操作系统内部体系架构,学习操作系统的另一个在线好材料. 《Operating Systems: Three Easy Pieces》 介绍:&lt;操作系统:三部曲&gt;在线电子书,虚拟、并发、持续. 《Database Systems: reading list》 介绍:数据库系统经典论文阅读列,此外推送github上面的db reading. 《Unix System Administration》 介绍:Unix System Administration ebook. 《The Amoeba Distributed Operating System》 介绍:分布式系统经典论文. 《Principles of Computer Systems》 介绍:计算机系统概念，以分布式为主.此外推荐Introduction to Operating Systems笔记 《Person page of EMİN GÜN SİRER》 介绍:推荐康奈尔大学的教授EMİN GÜN SİRER的主页,他的研究项目有分布式,数据存储。例如HyperDex数据库就是他的其中一个项目之一. 《Scalable, Secure, and Highly Available Distributed File Access》 介绍:来自卡内基梅隆如何构建可扩展的、安全、高可用性的分布式文件系统,其他papers. 《Distributed (Deep) Machine Learning Common》 介绍:分布式机器学习常用库. 《The Datacenter as a Computer》 介绍:介绍了如何构建仓储式数据中心,尤其是对于现在的云计算,分布式学习来说很有帮助.本书是Synthesis Lectures on Computer Architecture系列的书籍之一,这套丛书还有 《The Memory System》,《Automatic Parallelization》,《Computer Architecture Techniques for Power Efficiency》,《Performance Analysis and Tuning for General Purpose Graphics Processing Units》,《Introduction to Reconfigurable Supercomputing》,Memory Systems Cache, DRAM, Disk 等 《helsinki:Distributed Systems Course slider》 介绍:来自芬兰赫尔辛基的分布式系统课程课件:什么是分布式,复制,一致性,容错,同步,通信. 《TiDB is a distributed SQL database》 介绍:分布式数据库TiDB,Golang开发. 《S897: Large-Scale Systems》 介绍:课程资料:大规模系统. 《Large-scale L-BFGS using MapReduce》 介绍:使用MapReduce进行大规模分布式集群环境下并行L-BFGS. 《Twitter是如何构建高性能分布式日志的》 介绍:Twitter是如何构建高性能分布式日志的. 《Distributed Systems: When Limping Hardware Is Worse Than Dead Hardware》 介绍:在分布式系统中某个组件彻底死了影响很小，但半死不活（网络/磁盘），对整个系统却是毁灭性的. 《Tera - 高性能、可伸缩的结构化数据库》 介绍:来自百度的分布式数据库. 《SequoiaDB is a distributed document-oriented NoSQL Database》 介绍:SequoiaDB分布式文档数据库开源. 《Readings in distributed systems》 介绍:这个网址里收集了一堆各TOP大学分布式相关的课程. 《Paxos vs Raft》 介绍:这个网站是Raft算法的作者为教授Paxos和Raft算法做的，其中有两个视频链接，分别讲上述两个算法.参考阅读:关于Paxos的历史 《A Scalable Content-Addressable Network》 介绍:A Scalable Content-Addressable Network. 《500 Lines or Less》 介绍:这个项目其实是一本书（ The Architecture of Open Source Applications）的源代码附录，是一堆大牛合写的. 《MIT 6.824 Distributed System》 介绍:这只是一个课程主页，没有上课的视频，但是并不影响你跟着它上课：每一周读两篇课程指定的论文，读完之后看lecture-notes里对该论文内容的讨论，回答里面的问题来加深理解，最后在课程lab里把所看的论文实现。当你把这门课的作业刷完后，你会发现自己实现了一个分布式数据库. 《HDFS-alike in Go》 介绍:使用go开发的分布式文件系统. 《What are some good resources for learning about distributed computing? Why?》 介绍:Quora上关于学习分布式的资源问答. 《SeaweedFS is a simple and highly scalable distributed file system》 介绍:SeaweedFS是使用go开发的分布式文件系统项目,代码简单，逻辑清晰. 《Codis - yet another fast distributed solution for Redis》 介绍:Codis 是一个分布式 Redis 解决方案, 对于上层的应用来说, 连接到 Codis Proxy 和连接原生的 Redis Server 没有明显的区别 《Paper: Coordination Avoidance In Distributed Databases By Peter Bailis》 介绍:Coordination Avoidance In Distributed Databases. 《从零开始写分布式数据库》 介绍:本文以TiDB 源码为例. 《what we talk about when we talk about distributed systems》 介绍:分布式系统概念梳理,为分布式系统涉及的主要概念进行了梳理. 《Distributed locks with Redis》 介绍:使用Redis实现分布式锁. 《CS244b: Distributed Systems》 介绍: 斯坦福2014年秋季分布式课程. 《RAMP Made Easy》 介绍: 分布式的“读原子性”. 《Strategies and Principles of Distributed Machine Learning on Big Data》 介绍: 大数据分布式机器学习的策略与原理. 《Distributed Systems: What is the CAP theorem?》 介绍: 分布式CAP法则. 《How should I start to learn distributed storage system as a beginner?》 介绍: 新手如何步入分布式存储系统. 《Cassandra - A Decentralized Structured Storage System》 介绍: 分布式存储系统Cassandra剖析,推荐白皮书Introduction to Apache Cassandra. 《What is the best resource to learn about distributed systems?》 介绍: 分布式系统学习资源. 《What are some high performance TCP hacks?》 介绍: 一些高性能TCP黑客技巧. 《Maintaining performance in distributed systems》 介绍:分布式系统性能提升. 《A simple totally ordered broadcast protocol》 介绍:Benjamin Reed 和 Flavio P.Junqueira 所著论文,对Zab算法进行了介绍,zab算法是Zookeeper保持数据一致性的核心,在国内有很多公司都使用zookeeper做为分布式的解决方案.推荐与此相关的一篇文章ZooKeeper’s atomic broadcast protocol: Theory and practice. 《zFS - A Scalable Distributed File System Using Object Disk》 介绍:可扩展的分布式文件系统ZFS,The Zettabyte File System,End-to-end Data Integrity for File Systems: A ZFS Case Study. 《A Distributed Haskell for the Modern Web》 介绍:分布式Haskell在当前web中的应用. 《Reasoning about Consistency Choices in Distributed Systems》 介绍:POPL2016的论文,关于分布式系统一致性选择的论述,POPL所接受的论文,github上已经有人整理. 《Paxos Made Simple》 介绍:Paxos让分布式更简单.译文.参考阅读:关于Paxos的历史,understanding Paxos part1,Understanding Paxos – Part 2.Quora: What is a simple explanation of the Paxos algorithm?,Tutorial Summary: Paxos Explained from Scratch,Paxos algorithm explained, part 1: The essentials,Paxos algorithm explained, part 2: Insights 《Consensus Protocols: Paxos》 介绍:分布式系统一致性协议:Paxos.参考阅读:关于Paxos的历史 《Consensus on Transaction Commit》 介绍：事务提交的一致性探讨. 《The Part-Time Parliaments》 介绍:在《The Part-Time Parliament》中描述了基本协议的交互过程。在基本协议的基础上完善各种问题得到了最终的议会协议。 为了让人更容易理解《The Part-Time Parliament》中描述的Paxos算法，Lamport在2001发表了《Paxos Made Simple》，以更平直的口头语言描述了Paxos，而没有包含正式的证明和数学术语。《Paxos Made Simple》中，将算法的参与者更细致的划分成了几个角色：Proposer、Acceptor、Learner。另外还有Leader和Client.参考阅读:关于Paxos的历史 《Paxos Made Practical》 介绍:看这篇论文时可以先看看理解Paxos Made Practical. 《PaxosLease: Diskless Paxos for Leases》 介绍：PaxosLease：实现租约的无盘Paxos算法,译文. 《Paxos Made Moderately Complex》 介绍：Paxos算法实现,译文,同时推荐42 Paxos Made Moderately Complex. 《Hadoop Reading List》 介绍：Hadoop学习清单. 《Hadoop Reading List》 介绍：Hadoop学习清单. 《2010 NoSQL Summer Reading List》 介绍：NoSQL知识清单,里面不仅仅包含了数据库阅读清单还包含了分布式系统资料. 《Raft: Understandable Distributed Consensus》 介绍：Raft可视化图帮助理解分布式一致性 《Etcd:Distributed reliable key-value store for the most critical data of a distributed system》 介绍：Etcd分布式Key-Value存储引擎 《Understanding Availability》 介绍：理解peer-to-peer系统中的可用性究竟是指什么.同时推荐基于 Peer-to-Peer 的分布式存储系统的设计 《Process structuring, synchronization, and recovery using atomic actions》 介绍：经典论文 《Programming Languages for Parallel Processing》 介绍：并行处理的编程语音 《Analysis of Six Distributed File Systems》 介绍：此篇论文对HDFS,MooseFS,iRODS,Ceph,GlusterFS,Lustre六个存储系统做了详细分析.如果是自己研发对应的存储系统推荐先阅读此篇论文 《A Survey of Distributed File Systems》 介绍：分布式文件系统综述 《Concepts of Concurrent Programming》 介绍：并行编程的概念,同时推荐卡内基梅隆FTP 《Concurrency Control Performance Modeling:Alternatives and Implications》 介绍：并发控制性能建模：选择与意义 《Distributed Systems - Concepts and Design 5th Edition》 介绍：ebook分布式系统概念与设计 《分布式系统设计的形式方法》 介绍：分布式系统设计的形式方法 《互斥和选举算法》 介绍：互斥和选举算法 《Actors：A model Of Concurrent Cornputation In Distributed Systems》 介绍：经典论文 《Security Engineering: A Guide to Building Dependable Distributed Systems》 介绍：如何构建一个安全可靠的分布式系统,About the Author,Bibliography:文献资料,章节访问把链接最后的01换成01-27即可 《15-712 Advanced and Distributed Operating Systems》 介绍：卡内基梅隆大学的分布式系统博士生课程主页,有很丰富的资料 《Dapper, Google’s Large-Scale Distributed Systems Tracing Infrastructure》 介绍：Dapper，大规模分布式系统的跟踪系统,译文,译文对照 《CS262a: Advanced Topics in Computer Systems》 介绍：伯克利大学计算机系统进阶课程,内容有深度,涵盖分布式,数据库等内容 《Egnyte Architecture: Lessons Learned In Building And Scaling A Multi Petabyte Distributed System》 介绍：PB级分布式系统构建/扩展经验 《CS162: Operating Systems and Systems Programming》 介绍：伯克利大学计算机系统课程:操作系统与系统编程 《MDCC: Multi-Data Center Consistency》 介绍：MDCC主要解决跨数据中心的一致性问题中间件,一种新的协议 《Research at Google:Distributed Systems and Parallel Computing》 介绍：google公开对外发表的分布式系统与并行计算论文 《HDFS Architecture Guide》 介绍：分布式文件系统HDFS架构 《ActorDB distributed SQL database》 介绍：分布式 Key/Value数据库 《An efficient data location protocol for self-organizing storage clusters》 介绍：是著名的Ceph的负载平衡策略，文中提出的几种策略都值得尝试，比较赞的一点是可以对照代码体会和实践,如果你还需要了解可以看看Ceph:一个 Linux PB 级分布式文件系统,除此以外,论文的引用部分也挺值得阅读的,同时推荐Ceph: A Scalable, High-Performance Distributed File System 《A Self-Organizing Storage Cluster for Parallel Data-Intensive Applications》 介绍：Surrento的冷热平衡策略就采用了延迟写技术 《HBA: Distributed Metadata Management for Large Cluster-Based Storage Systems》 介绍：对于分布式存储系统的元数据管理. 《Server-Side I/O Coordination for Parallel File Systems》 介绍：服务器端的I/O协调并行文件系统处理,网络,文件存储等都会涉及到IO操作.不过里面涉及到很多技巧性的思路在实践时需要斟酌 《Distributed File Systems: Concepts and Examples》 介绍：分布式文件系统概念与应用 《CSE 221: Graduate Operating Systems》 介绍：加利福尼亚大学的研究生操作系统课程主页，论文很值得阅读 《S4: Distributed Stream Computing Platform》 介绍：Yahoo出品的流式计算系统，目前最流行的两大流式计算系统之一（另一个是storm），Yahoo的主要广告计算平台 《Pregel: a system for large-scale graph processing》 介绍：Google的大规模图计算系统，相当长一段时间是Google PageRank的主要计算系统，对开源的影响也很大（包括GraphLab和GraphChi） 《GraphLab: A New Framework for Parallel Machine Learning》 介绍：CMU基于图计算的分布式机器学习框架，目前已经成立了专门的商业公司，在分布式机器学习上很有两把刷子，其单机版的GraphChi在百万维度的矩阵分解都只需要2~3分钟； 《F1: A Distributed SQL Database That Scales》 介绍：这篇论文是Google 2013年发表的，介绍了F1的架构思路，13年时就开始支撑Google的AdWords业务，另外两篇介绍文章F1 - The Fault-Tolerant Distributed RDBMS Supporting Google’s Ad Business .Google NewSQL之F1 《Cockroach DB:A Scalable, Survivable, Strongly-Consistent SQL Database》 介绍：CockroachDB ：一个可伸缩的、跨地域复制的，且支持事务的数据存储,InfoQ介绍,Design and Architecture of CockroachDb 《Multi-Paxos: An Implementation and Evaluation》 介绍：Multi-Paxos实现与总结，此外推荐Paxos/Multi-paxos Algorithm,Multi-Paxos Example，地址:ftp://ftp.cs.washington.edu/tr/2009/09/UW-CSE-09-09-02.PDF 《Zab: High-performance broadcast for primary-backup systems》 介绍：一致性协议zab分析 《A Distributed Hash Table》 介绍：分布式哈希算法论文,扩展阅读Introduction to Distributed Hash Tables,Distributed Hash Tables 《Comparing the performance of distributed hash tables under churn》 介绍：分布式hash表性能的Churn问题 《Brewer’s Conjecture and the Feasibility of Consistent, Available, Partition-Tolerant Web》 介绍：分布式系统的CAP问题,推荐Perspectives on the CAP Theorem.对CAP理论的解析文章,PODC ppt,A plain english introduction to CAP Theorem,IEEE Computer issue on the CAP Theorem 《F2FS: A New File System for Flash Storage》 介绍：闪存存储文件系统F2FS 《Better I/O Through Byte-Addressable, Persistent Memory》 介绍：微软发表的关于i/o访问优化论文 《tmpfs: A Virtual Memory File System》 介绍：虚拟内存文件系统tmpfs 《BTRFS: The Linux B-tree Filesystem》 介绍：Linux B-tree文件系统. 《Akamai technical publication》 介绍：Akamai是全球最大的云计算机平台之一，承载了全球15-30%网络流量,如果你是做CDN或者是云服务,这个里面的论文会给你很有帮助.例如这几天看facebook开源的osquery。找到通过db的方式运维,找到Keeping Track of 70,000+ Servers: The Akamai Query System这篇论文，先看论文领会思想，然后再使用工具osquery实践 《BASE: An Acid Alternative》 介绍：来自eBay 的解决方案,译文Base: 一种Acid的替代方案,应用案例参考保证分布式系统数据一致性的6种方案 《A Note on Distributed Computing》 介绍：Jim Waldo和Sam Kendall等人共同撰写了一篇非常有名的论文“分布式计算备忘录”，这篇论文在Reddit上被人推荐为“每个程序员都应当至少读上两篇”的论文。在这篇论文中，作者表示“忽略本地计算与分布式计算之间的区别是一种危险的思想”，特别指出了Emerald、Argus、DCOM以及CORBA的设计问题。作者将这些设计问题归纳为“三个错误的原则”： “对于某个应用来说，无论它的部署环境如何，总有一种单一的、自然的面向对象设计可以符合其需求。” “故障与性能问题与某个应用的组件实现直接相关，在最初的设计中无需考虑这些问题。” “对象的接口与使用对象的上下文无关”. 《Distributed Systems Papers》 介绍：分布式系统领域经典论文列表. 《Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web》 介绍：Consistent Hashing算法描述. 《SIGMOD 2016: Accepted Research Papers》 介绍：SIGMOD是世界上最有名的数据库会议之一,最具有权威性,收录论文审核非常严格.2016年的SIGMOD 会议照常进行,上面收录了今年SIGMOD收录的论文,把题目输入google中加上pdf就能找到,很多论文值得阅读,SIGMOD 2015 《Notes on CPSC 465/565: Theory of Distributed Systems》 介绍:耶鲁大学的分布式系统理论课程笔记 《Distributed Operating System Doc PDF》 介绍:分布式系统文档资源（可下载） 《Anatomy of a database system》 介绍:数据库系统剖析，这本书是由伯克利大学的Joseph M. Hellerstein和M. Stonebraker合著的一篇论文.对数据库剖析很有深度.除此以外还有一篇文章Architecture of a Database System。数据库系统架构,厦门大学的数据库实验室教授林子雨组织过翻译-ALL.pdf) 《A Relational Model of Data for Large Shared Data Banks》 介绍:数据库关系模型论文 《RUC Innovative data systems reaserch lab recommand papers》 介绍:中国人民大学数据研究实验室推荐的数据库领域论文 《A Scalable Distributed Information Management System》 介绍:构建可扩展的分布式信息管理系统 《Distributed Systems in Haskell》 介绍:Haskell中的分布式系统开发 《Large-scale cluster management at Google with Borg》 介绍:Google使用Borg进行大规模集群的管理,伯克利大学ppt介绍,中文版 《Lock Free Programming Practice》 介绍:并发编程（Concurrency Programming）资料,主要涵盖lock free数据结构实现、内存回收方法、memory model等备份链接 密码: xc5j 《Distributed Algorithms Lecture Notes for 6.852》 介绍:Nancy Lynch’s的分布式算法研究生课程讲义 《Distributed Algorithms for Topic Models》 介绍:分布式算法主题模型. 《RecSys - ACM Recommender Systems》 介绍:世界上非常有名的推荐系统会议，我比较推荐接收的PAPER 《All Things Distributed》 介绍:推荐一个博客,博主是Amazon CTO Werner Vogels,这是一个关注分布式领域的博客.大部分博文是关于在工业界应用. 《programming, database, distributed system resource list》 介绍:这个Git是由阿里(alibaba)的技术专家何登成维护,主要是分布式数据库. 《Making reliable distributed systems in the presence of sodware errors》 介绍:Erlang的作者Joe Armstrong撰写的论文，面对软件错误构建可靠的分布式系统.中文译版 《CS 525: Advanced Distributed Systems[Spring 2016]》 介绍:伊利诺伊大学的Advanced Distributed Systems 里把各个方向重要papers（updated Spring 2015）列举出来，可以参考一下 《Distributed Algorithms》 介绍:这是一本分布式算法电子书,作者是Jukka Suomela.讲述了多个计算模型,一致性,唯一标示,并发等. 《TinyLFU: A Highly Efficient Cache Admission Policy》 介绍:当时是在阅读如何设计一个缓存系统时看到的，然后通过Google找到了这一篇关于缓存策略的论文，它是LFU的改良版,中文介绍.如果有兴趣可以看看Golang实现版。结合起来可能会帮助你理解 《6.S897: Large-Scale Systems》 介绍:斯坦福大学给研究生开的分布式系统课程。教师是 spark 作者 matei. 能把这些内容真正理解透，分布式系统的功力就很强了。 《学习分布式系统需要怎样的知识？》 介绍:[怎么学系列]学习分布式系统需要怎样的知识？ 《Distributed systems theory for the distributed systems engineer》 介绍:分布式系统工程师的分布式系统理论 《A Distributed Systems Reading List》 介绍:分布式系统论文阅读列表,此外推荐威斯康星大学麦迪逊分校计算机系分布式系统学习推荐阅读列表 《Distributed Systems Reading Group》 介绍:麻省理工大学分布式系统小组，他们会把平时阅读到的优秀论文分享出来。虽然有些论文本页已经收录，但是里面的安排表schedule还是挺赞的 《Scalable Software Architecture》 介绍:分布式系统、可扩展性与系统设计相关报告、论文与网络资源汇总. 《MapReduce&amp;Hadoop resource》 介绍:MapReduce&amp;Hadoop相关论文，涉及分布式系统设计，性能分析，实践，优化等多个方面 《Distributed Systems: Principles and Paradigms(second edtion)》_-_Tannenbaum-distributed_systems_principles_and_paradigms_2nd_edition.pdf) 介绍:分布式系统原理与范型第二版,课后解答 《Distributed Systems Seminar’s reading list for Spring 2017》 介绍:分布式系统研讨会论文阅读列表 《A Critique of the CAP Theorem》 介绍:这是一篇评论CAP定理的论文，学习CAP很有帮助,推荐阅读评论文章“A Critique of the CAP Theorem” 《Evolving Distributed Systems》 介绍:推荐文章《不断演进的分布式系统》. 《Ask HN: Recommendations for a book on Distributed Systems?》 介绍:HN上面关于分布式系统相关领域学习的书籍推荐. 《SeaweedFS:A simple and highly scalable distributed file system》 介绍:Golang开源项目,分布式文件存储系统SeaweedFS 《The Design and Implementation of a Log-Structured File System》 介绍:论文推荐:设计并实现一个日志结构的文件系统. 原文链接：https://raw.githubusercontent.com/ty4z2008/Qix/master/ds.md","raw":null,"content":null,"categories":[{"name":"分布式系统","slug":"分布式系统","permalink":"http://linbingdong.com/categories/分布式系统/"}],"tags":[{"name":"分布式系统","slug":"分布式系统","permalink":"http://linbingdong.com/tags/分布式系统/"},{"name":"资料大全","slug":"资料大全","permalink":"http://linbingdong.com/tags/资料大全/"}]},{"title":"JVM之Java内存区域与内存溢出异常","slug":"JVM之Java内存区域与内存溢出异常","date":"2017-04-08T13:57:17.000Z","updated":"2017-04-08T13:57:17.000Z","comments":true,"path":"2017/04/08/JVM之Java内存区域与内存溢出异常/","link":"","permalink":"http://linbingdong.com/2017/04/08/JVM之Java内存区域与内存溢出异常/","excerpt":"Java的JVM可以自动管理内存，包括内存动态分配和垃圾收集等。","text":"Java的JVM可以自动管理内存，包括内存动态分配和垃圾收集等。 简介JVM在执行Java程序的过程中会把它所管理的内存划分为若干个不同的数据区域。这些区域都有各自的用途，以及创建和销毁的时间，有的区域随着JVM进程的启动而存在，有些区域则依赖用户线程的启动和结束而建立和销毁。 先看看JVM运行时数据区包括哪几个部分： 可以看出JVM运行时数据区包括：堆、虚拟机栈、本地方法栈、方法区、程序计数器和运行时常量池。其中，运行时常量池在方法区里。 接下来对这几个区域一一介绍。 Java堆Java堆（Java Heap）是JVM所管理的内存中最大的一块。 堆的作用是用来存放对象实例，所有的对象实例和数组都在堆上分配内存。 堆也是垃圾收集器管理的主要区域，因此也被称为“GC堆”。因为垃圾收集器主要用来收集对象，对象在堆上分配，所以自然堆是垃圾收集器管理的主要区域。 堆被所有的线程共享，在虚拟机启动时创建，物理上可在不连续的内存空间中，跟磁盘空间一样。 Java堆溢出什么情况下会堆溢出当创建新对象，堆上内存不够时就会产生堆溢出。 只要不断创建对象，并且保证GC Roots到对象之间有可达路径来避免垃圾回收机制清除这些对象，那么当堆上的内存不够创建新对象时就会产生内存溢出异常。 制造堆溢出/** * -Xms20m -Xmx20m -XX:+HeapDumpOnOutOfMemoryError */public class HeapOOM &#123; static class OOMObject&#123; &#125; public static void main(String[] args) &#123; List&lt;OOMObject&gt; list = new ArrayList&lt;&gt;(); long length=0; while (true)&#123; try &#123; list.add(new OOMObject()); length += 1; &#125; catch (Throwable e)&#123; System.out.println(\"number of obj: \"+length); throw e; &#125; &#125; &#125;&#125; 输出： java.lang.OutOfMemoryError: Java heap spaceDumping heap to java_pid39193.hprof ...Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: Java heap spaceHeap dump file created [27917334 bytes in 0.227 secs] at java.util.Arrays.copyOf(Arrays.java:3210)number of obj: 810325 at java.util.Arrays.copyOf(Arrays.java:3181) at java.util.ArrayList.grow(ArrayList.java:261) at java.util.ArrayList.ensureExplicitCapacity(ArrayList.java:235) at java.util.ArrayList.ensureCapacityInternal(ArrayList.java:227) at java.util.ArrayList.add(ArrayList.java:458) at com.lbd.jvm.HeapOOM.main(HeapOOM.java:20) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)Process finished with exit code 1 解决堆溢出重点确认内存中的对象是否是必要的，也就是确认到底是出现了内存泄露（Memory Leak）还是内存溢出（Memory Overflow）。 方法：通过内存映像分析工具（如Eclipse Memory Analyzer）对Dump出来的堆转储快照进行分析。 如果是内存泄露： 进一步通过工具查看泄露对象到GC Roots的引用链，这样就能找到泄露对象是通过怎样的路径与GC Roots相关联并导致垃圾收集器无法自动回收它们的，这样就可以比较准确地定位出泄露代码的位置 如果不是内存泄露： 也就是内存中的对象确实还必须存活这，那就应该检查虚拟机的堆参数（-Xms和-Xmx），看看是否可以调大一些。 方法区方法区与Java堆一样，是各个线程共享的内存区域，用于存储已被虚拟机加载的类信息（类的版本、字段、方法、接口等描述信息）、常量、静态变量、即使编译器编译后的代码等数据。 方法区的内存回收主要针对常量池的回收和堆类型的卸载。垃圾收集行为在该区域比较少出现。 在HotSpot虚拟机中，方法区又被称为“永久代”（Permanent Generation），这样HotSpot的垃圾收集器可以像管理Java堆一样管理这部分内存，就不用专门为方法区编写内存管理代码了。 方法区溢出如果产生大量的类或者大量的字符串常量（运行时常量池溢出）可能导致方法区溢出。 Java SE API可以动态产生类，如反射时的GeneratedConstructorAccessor和动态代理等。 运行时常量池运行时常量池是方法区的一部分，用于存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后进入方法区的运行时常量池中存放。 运行时常量池溢出什么时候会运行时常量池溢出产生大量的字符串常量。 制造运行时常量池溢出注：以下代码只在JDK1.6及之前的版本才会产生运行时常量池溢出异常，因为在这些版本中常量池分配在永久代内，可以通过-XX:PermSize=1M -XX:MaxPermSize=1M来限制方法区的大小，从而间接限制其中常量池的容量。 /** * -XX:PermSize=1M -XX:MaxPermSize=1M */public class RuntimeConstantPoolOOM &#123; public static void main(String[] args) &#123; List&lt;String&gt; list = new ArrayList&lt;&gt;(); int i = 0; while (true)&#123; list.add(String.valueOf(i++).intern()); &#125; &#125;&#125; 虚拟机栈虚拟机栈描述的是Java方法执行的内存模型：每个方法在执行的时候都会创建一个栈帧（Stack Frame）用于存储局部变量表、操作数栈、动态链接、方法出口等信息。每一个方法从调用直至执行完成的过程，就对应者一个栈帧在虚拟机栈中入栈到出栈的过程。 虚拟机栈是线程私有的。为Java方法服务。 虚拟机栈中最重要的是局部变量表。局部变量表存放了编译期可知的各种基本数据类型和对象引用类型。是在编译期确定的。 虚拟机栈溢出什么情况下会Java虚拟机栈溢出 如果线程请求的栈深度大于虚拟机所允许的最大深度（一般是递归），将抛出StackOverflowError异常。 如果虚拟机在扩展栈时无法申请到足够的内存空间，则抛出OutOfMemoryError异常。 制造虚拟机栈溢出（StackOverflowError）递归。因为递归需要用到栈。设置栈容量为256k（-Xss256k）。 /** * -Xss256k */public class JavaVMStackSOF &#123; private int stackLength = 1; public void stackLeak()&#123; stackLength++; stackLeak(); &#125; public static void main(String[] args) &#123; JavaVMStackSOF oom = new JavaVMStackSOF(); try &#123; oom.stackLeak(); &#125; catch (Throwable e)&#123; System.out.println(\"stack length:\" + oom.stackLength); throw e; &#125; &#125;&#125; 输出： Exception in thread &quot;main&quot; java.lang.StackOverflowErrorstack length:2789 at com.lbd.jvm.JavaVMStackSOF.stackLeak(JavaVMStackSOF.java:10) at com.lbd.jvm.JavaVMStackSOF.stackLeak(JavaVMStackSOF.java:10) at com.lbd.jvm.JavaVMStackSOF.stackLeak(JavaVMStackSOF.java:10) at com.lbd.jvm.JavaVMStackSOF.stackLeak(JavaVMStackSOF.java:10) at com.lbd.jvm.JavaVMStackSOF.stackLeak(JavaVMStackSOF.java:10) 当调用了2789次后出现了栈溢出StackOverflowError。 制造虚拟机栈溢出（OutOfMemoryError）创建足够多的线程，当扩展栈时无法申请到足够的内存空间，就会抛出OutOfMemoryError异常。 /** * -Xss2M * dangerous,don't run this program! */public class JavaVMStackOOM &#123; private void dontStop() &#123; while (true); &#125; public void stackLeakByThread () &#123; while (true) &#123; Thread thread = new Thread(new Runnable() &#123; @Override public void run() &#123; dontStop(); &#125; &#125;); thread.start(); &#125; &#125; public static void main(String[] args) &#123; JavaVMStackOOM oom = new JavaVMStackOOM(); oom.stackLeakByThread(); &#125;&#125; 本地方法栈本地方法栈与虚拟机栈所发挥的作用很相似。区别在于：虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈为虚拟机使用到的Native方法服务。 程序计数器程序计数器用来记录正在执行的虚拟机字节码指令的地址。程序计数器是线程私有的。是唯一一个没有规定任何OutOfMemoryError情况的区域。 因为Java虚拟机的多线程是通过线程轮流切换并分配CPU执行时间的方式来实现的。为了线程切换后能恢复到正确的执行位置，每个线程需要有一个独立的程序计数器，各线程之间计数器互不影响，独立存储。 直接内存直接内存并不是虚拟机运行时数据区的一部分，也不是Java虚拟机规范中定义的内存区域。 JDK1.4中新加入了NIO(New Input/Ouput)类，引入了一种基于通道（Channel）和缓冲区（Buffer）的I/O方式，它可以使用Native函数库直接分配堆外内存，然后通过一个存储在Java堆中的DirectByteBufer对象作为这块内存的引用进行操作。","raw":null,"content":null,"categories":[{"name":"JVM","slug":"JVM","permalink":"http://linbingdong.com/categories/JVM/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/tags/Java/"},{"name":"JVM","slug":"JVM","permalink":"http://linbingdong.com/tags/JVM/"}]},{"title":"CountDownLatch && CyclicBarrier example","slug":"CountDownLatch && CyclicBarrier example","date":"2017-04-04T16:00:00.000Z","updated":"2017-09-03T02:16:14.000Z","comments":true,"path":"2017/04/05/CountDownLatch && CyclicBarrier example/","link":"","permalink":"http://linbingdong.com/2017/04/05/CountDownLatch && CyclicBarrier example/","excerpt":"CountDownLatch &amp;&amp; CyclicBarrier example","text":"CountDownLatch &amp;&amp; CyclicBarrier example CountDownLatchpackage com.lbd.concurrent;import java.util.Random;import java.util.concurrent.*;/** * Created by lbd. */public class CountDownLatchDemo &#123; public static void main(String[] args) &#123; ExecutorService executor = Executors.newCachedThreadPool(); CountDownLatch latch = new CountDownLatch(3); Worker w1 = new Worker(\"worker1\", latch); Worker w2 = new Worker(\"worker2\", latch); Worker w3 = new Worker(\"worker3\", latch); Boss boss = new Boss(latch); executor.execute(w1); executor.execute(w2); executor.execute(w3); executor.execute(boss); executor.shutdown(); &#125;&#125;class Worker implements Runnable &#123; private CountDownLatch downLatch; private String name; public Worker(String name, CountDownLatch downLatch) &#123; this.name = name; this.downLatch = downLatch; &#125; public void run() &#123; System.out.println(this.name + \" is working...\"); try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(this.name + \" is done!\"); downLatch.countDown(); &#125;&#125;class Boss implements Runnable &#123; private CountDownLatch downLatch; public Boss(CountDownLatch downLatch) &#123; this.downLatch = downLatch; &#125; public void run() &#123; System.out.println(\"boss is waiting for all workers...\"); try &#123; downLatch.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(\"all work is done! Boss gonna check\"); &#125;&#125; output: worker1 is working...worker2 is working...worker3 is working...boss is waiting for all workers...worker2 is done!worker3 is done!worker1 is done!all work is done! Boss gonna check CyclicBarrierpackage com.lbd.concurrent;import java.util.Random;import java.util.concurrent.*;/** * Created by lbd. */public class CyclicBarrierDemo &#123; public static void main(String[] args) &#123; ExecutorService executor = Executors.newCachedThreadPool(); CyclicBarrier barrier = new CyclicBarrier(3); Worker1 w1 = new Worker1(\"worker1\", barrier); Worker1 w2 = new Worker1(\"worker2\", barrier); Worker1 w3 = new Worker1(\"worker3\", barrier); executor.execute(w1); executor.execute(w2); executor.execute(w3); executor.shutdown(); &#125;&#125;class Worker1 implements Runnable &#123; private CyclicBarrier barrier; private String name; public Worker1(String name, CyclicBarrier barrier) &#123; this.barrier = barrier; this.name = name; &#125; @Override public void run() &#123; try &#123; Thread.sleep(new Random().nextInt(10000)); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(barrier.getNumberWaiting() + \" worker arrived\"); System.out.println(this.name + \" arrived\"); try &#123; barrier.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (BrokenBarrierException e) &#123; e.printStackTrace(); &#125; System.out.println(\"all arrived,star working!\"); &#125;&#125; output: 0 worker arrivedworker2 arrived1 worker arrivedworker1 arrived2 worker arrivedworker3 arrivedall arrived,star working!all arrived,star working!all arrived,star working!","raw":null,"content":null,"categories":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/tags/Java/"}]},{"title":"MooseFS安装配置","slug":"MooseFS安装配置","date":"2017-03-30T09:32:18.000Z","updated":"2017-03-30T09:32:18.000Z","comments":true,"path":"2017/03/30/MooseFS安装配置/","link":"","permalink":"http://linbingdong.com/2017/03/30/MooseFS安装配置/","excerpt":"记录 MooseFS 的部署过程。","text":"记录 MooseFS 的部署过程。 简介MooseFS 是一个分布式文件系统，支持挂载的形式。 主要角色 master 节点：元数据节点，复制调度和管理元数据。 metalogger 节点：用于备份 master 的元数据和日志。 chunkserver 节点：数据节点，数据实际存放的节点。 client ：客户端。通过 FUSE 将 mfs 文件系统挂载到客户端后，客户端可以像使用一个普通的磁盘分区一样来使用 mfs 。 集群规划 mfsmaster mfsmetalogger mfschunkserver mfsclient 192.168.20.96 192.168.20.97 192.168.20.98 192.168.20.99 192.168.20.96 192.168.20.97 192.168.20.98 192.168.20.99 准备工作安装fuse模块确保需要安装 mfsclient 的节点上已经安装了 Linux 内核模块 FUSE 。 若未安装，可通过 yum 或者编译安装的方式进行安装。 创建用户和用户组每个节点都要创建 mfs 用户和用户组。 groupadd mfsuseradd -g mfs mfs 修改/etc/hosts在每个节点上为 mfsmaster 所在节点（192.168.20.96）增加一个别名 mfsmaster 。 修改 /etc/hosts ： 192.168.20.96 mfsmaster 解压rpm包将 mfs.tar.gz 解压到每个节点的 /opt 目录下，解压后会生成 /opt/mfs 目录。 安装配置mfsmaster+cgi节点 192.168.20.96 cd /opt/mfsrpm -ivh moosefs-2.0.77-1.x86_64.rpm moosefs-master-2.0.77-1.x86_64.rpm moosefs-cgi-2.0.77-1.x86_64.rpm moosefs-cgiserv-2.0.77-1.x86_64.rpm mfsmetalogger节点 192.168.20.97 安装： cd /opt/mfsrpm -ivh moosefs-2.0.77-1.x86_64.rpm moosefs-metalogger-2.0.77-1.x86_64.rpm mfschunkserver节点 192.168.20.98 、 192.168.20.99 创建 /mnt/mfs 目录： mkdir -p /mnt/mfschown -R mfs:mfs /mnt/mfs 修改 /etc/mfs/mfshdd.cfg ，在任意位置增加一行: /mnt/mfs 安装： cd /opt/mfsrpm -ivh moosefs-2.0.77-1.x86_64.rpm moosefs-chunkserver-2.0.77-1.x86_64.rpm mfsclient节点 192.168.20.96 、192.168.20.97 、192.168.20.98 、192.168.20.99 创建 /mnt/mfs-cli 目录： mkdir /mnt/mfs-clichown -R mfs:mfs /mnt/mfs-cli/ 安装： rpm -ivh moosefs-client-2.0.77-1.x86_64.rpm 挂载： mfsmount /mnt/mfs-cli/ -H mfsmaster 启动 mfsmaster systemctl start moosefs-master mfscgiserv systemctl start moosefs-cgiserv 启动 mfsmaster 和 mfscgiserv 后，在浏览器中输入 http://192.168.20.96:9425 查看Web 页面 ： mfsmetalogger systemctl start moosefs-metalogger mfschunkserver systemctl start moosefs-chunkserver 使用所有客户端节点 /mnt/mfs-cli 目录下的内容都是相同的。只需把文件放入任意客户端节点的 /mnt/mfs-cli 目录下即可，该目录对所有客户端节点可见。","raw":null,"content":null,"categories":[{"name":"MooseFS","slug":"MooseFS","permalink":"http://linbingdong.com/categories/MooseFS/"}],"tags":[{"name":"分布式系统","slug":"分布式系统","permalink":"http://linbingdong.com/tags/分布式系统/"},{"name":"分布式文件系统","slug":"分布式文件系统","permalink":"http://linbingdong.com/tags/分布式文件系统/"},{"name":"MooseFS","slug":"MooseFS","permalink":"http://linbingdong.com/tags/MooseFS/"}]},{"title":"PostgreSQL安装PostGIS插件并使用","slug":"PostgreSQL安装PostGISC插件并使用","date":"2017-03-25T12:15:11.000Z","updated":"2017-03-25T12:15:11.000Z","comments":true,"path":"2017/03/25/PostgreSQL安装PostGISC插件并使用/","link":"","permalink":"http://linbingdong.com/2017/03/25/PostgreSQL安装PostGISC插件并使用/","excerpt":"PostGIS是对象关系型数据库PostgreSQL的一个插件，PostGIS提供如下空间信息服务功能：空间对象、空间索引、空间操作函数和空间操作符。同时，PostGIS遵循OpenGIS的规范。","text":"PostGIS是对象关系型数据库PostgreSQL的一个插件，PostGIS提供如下空间信息服务功能：空间对象、空间索引、空间操作函数和空间操作符。同时，PostGIS遵循OpenGIS的规范。 1. 简介PostGIS是对象关系型数据库PostgreSQL的一个插件，PostGIS提供如下空间信息服务功能：空间对象、空间索引、空间操作函数和空间操作符。同时，PostGIS遵循OpenGIS的规范。 PostGIS支持所有的空间数据类型，这些类型包括：点（POINT）、线（LINESTRING）、多边形（POLYGON）、多点 （MULTIPOINT）、多线（MULTILINESTRING）、多多边形（MULTIPOLYGON）和集合对象集 （GEOMETRYCOLLECTION）等。PostGIS支持所有的对象表达方法，比如WKT和WKB。 PostGIS支持所有的数据存取和构造方法，如GeomFromText()、AsBinary()，以及GeometryN()等。 PostGIS提供简单的空间分析函数（如Area和Length）同时也提供其他一些具有复杂分析功能的函数，比如Distance。 PostGIS提供了对于元数据的支持，如GEOMETRY_COLUMNS和SPATIAL_REF_SYS，同时，PostGIS也提供了相应的支持函数，如AddGeometryColumn和DropGeometryColumn。 PostGIS提供了一系列的二元谓词（如Contains、Within、Overlaps和Touches）用于检测空间对象之间的空间关系，同时返回布尔值来表征对象之间符合这个关系。 PostGIS提供了空间操作符（如Union和Difference）用于空间数据操作。比如，Union操作符融合多边形之间的边界。两个交迭的多边形通过Union运算就会形成一个新的多边形，这个新的多边形的边界为两个多边形中最大边界。 PostGIS还提供以下功能： 数据库坐标变换 数据库中的几何类型可以通过Transform函数从一种投影系变换到另一种投影系中。在OpenGIS中的几何类型都将SRID作为自身结构的一部分，但不知什么原因，在OpenGIS的SFSQL规范中，并没有引入Transform。 球体长度运算 存储在普通地理坐标系中的集合类型如果不进行坐标变换是无法进行程度运算的，OpenGIS所提供的坐标变换使得积累类型的程度计算变成可能。 三维的几何类型 SFSQL规范只是针对二维集合类型。OpenGIS提供了对三维集合类型的支持，具体是利用输入的集合类型维数来决定输出的表现方式。例如，即便 所有几何对象内部都以三维形式存储，纯粹的二维交叉点通常还是以二维的形式返回。此外，还提供几何对象在不同维度间转换的功能。 空间聚集函数 在数据库中，聚集函数是一个执行某一属性列所有数据操作的函数。比如Sum和Average，Sum是求某一关系属性列的数据总和，Average 则是求取某一关系属性列的数据平均值。与此对应，空间聚集函数也是执行相同的操作，不过操作的对象是空间数据。例如聚集函数Extent返回一系列要素中 的最大的包裹矩形框，如“SELECT EXTENT(GEOM) FROM ROADS”这条SQL语句的执行结果是返回ROADS这个数据表中所有的包裹矩形框。 栅格数据类型 PostGIS通过一种新的数据类型片，提供对于大的栅格数据对象的存储。片由以下几个部分组成：包裹矩形框、SRID、类型和一个字节序列。通过 将片的大小控制在数据库页值（32×32）以下，使得快速的随即访问变成可能。一般大的图片也是通过将其切成32×32像素的片然后再存储在数据库中的。 2. 部署2.1 安装PostGIS yum install postgis2_94 # 因为安装的PostgreSQL版本为9.4，所以是postgis2_94 注： 需要PostgreSQL9.1以上版本才支持PostGIS. 2.2 使PostGIS可用想要在PostgreSQL中使用PostGIS插件，安装只是第一步。每个数据库想要使用PostGIS必须先在该数据库中使PostGIS可用。假设我们想在gisdb这个数据库中使用PostGIS,先进入gisdb数据库，执行以下步骤： gisdb=# CREATE EXTENSION postgis;gisdb=# CREATE EXTENSION postgis_topology; 2.3 查看是否安装成功 在gisdb数据库中输入\\du，查看已安装的插件 gisdb=# \\dx 已安装扩展列表 名称 | 版本 | 架构模式 | 描述------------------+-------+------------+--------------------------------------------------------------------- plpgsql | 1.0 | pg_catalog | PL/pgSQL procedural language postgis | 2.1.8 | public | PostGIS geometry, geography, and raster spatial types and functions postgis_topology | 2.1.8 | topology | PostGIS topology spatial types and functions(3 行记录) 可以看到已经安装了postgis和postgis_topology。 3. 使用3.1 创建空间数据表首先建立一个常规的表格存储有关城市（cities）的信息。这个表格有两栏，一个是 ID 编号，一个是城市名： gisdb=# CREATE TABLE cities (id int4, name varchar(50)); 现在添加一个空间列用于存储城市的位置。习惯上这个列叫做 the_geom。它记录了数据为什么类型（点、线、面）、有几维（这里是二维）以及空间坐标系统。此处使用 EPSG:4326 坐标系统： gisdb=# SELECT AddGeometryColumn (&apos;cities&apos;, &apos;the_geom&apos;, 4326, &apos;POINT&apos;, 2); 完成后，查询 cities 表单应当显示这个新栏目。同时页面将显示当前表达没有记录（0 rows）。 gisdb=# select * from cities; id | name | the_geom----+-----------------+----------------------------------------------------（0行记录） 为添加记录，需要使用 SQL 命令。对于空间列，使用 PostGIS 的 ST_GeomFromText可以将文本转化为坐标与参考系号的记录： gisdb=# INSERT INTO cities (id, the_geom, name) VALUES (1,ST_GeomFromText(&apos;POINT(-0.1257 51.508)&apos;,4326),&apos;London, England&apos;);gisdb=# INSERT INTO cities (id, the_geom, name) VALUES (2,ST_GeomFromText(&apos;POINT(-81.233 42.983)&apos;,4326),&apos;London, Ontario&apos;);gisdb=# INSERT INTO cities (id, the_geom, name) VALUES (3,ST_GeomFromText(&apos;POINT(27.91162491 -33.01529)&apos;,4326),&apos;East London,SA&apos;); 当然，这样的输入方式难以操作。其它方式可以更快的输入数据。就目前来说，表格内已经有了一些城市数据，可以先进行查询等操作。 3.2 简单查询标准的 SQL 操作都可以用于 PostGIS 表： gisdb=# SELECT * FROM cities; id | name | the_geom----+-----------------+---------------------------------------------------- 1 | London, England | 0101000020E6100000BBB88D06F016C0BF1B2FDD2406C14940 2 | London, Ontario | 0101000020E6100000F4FDD478E94E54C0E7FBA9F1D27D4540 3 | East London,SA | 0101000020E610000040AB064060E93B4059FAD005F58140C0(3 行记录) 这里的坐标是无法阅读的 16 进制格式。要以 WKT 文本显示，使用 ST_AsText(the_geom) 或ST_AsEwkt(the_geom) 函数。也可以使用 ST_X(the_geom) 和 ST_Y(the_geom) 显示一个维度的坐标： gisdb=# SELECT id, ST_AsText(the_geom), ST_AsEwkt(the_geom), ST_X(the_geom), ST_Y(the_geom) FROM cities; id | st_astext | st_asewkt | st_x | st_y----+------------------------------+----------------------------------------+-------------+----------- 1 | POINT(-0.1257 51.508) | SRID=4326;POINT(-0.1257 51.508) | -0.1257 | 51.508 2 | POINT(-81.233 42.983) | SRID=4326;POINT(-81.233 42.983) | -81.233 | 42.983 3 | POINT(27.91162491 -33.01529) | SRID=4326;POINT(27.91162491 -33.01529) | 27.91162491 | -33.01529(3 行记录) 3.3 空间查询PostGIS 为 PostgreSQL 扩展了许多空间操作功能。以上已经涉及了转换空间坐标格式的 ST_GeomFromText 。多数空间操作以 ST（spatial type）开头，在 PostGIS 文档相应章节有罗列。这里回答一个具体的问题：上面三个城市相互的距离是多少？查询语句怎么写？ gisdb=# SELECT p1.name,p2.name,ST_Distance_Sphere(p1.the_geom,p2.the_geom) FROM cities AS p1, cities AS p2 WHERE p1.id &gt; p2.id; name | name | st_distance_sphere-----------------+-----------------+-------------------- London, Ontario | London, England | 5875787.03777356 East London,SA | London, England | 9789680.59961472 East London,SA | London, Ontario | 13892208.6782928(3 行记录) 输出显示了距离数据。注意 ‘WHERE’ 部分防止了输出城市到自身的距离（0）或者两个城市不同排列的距离数据（London, England 到 London, Ontario 和 London, Ontario 到 London, England 的距离是一样的）。","raw":null,"content":null,"categories":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"http://linbingdong.com/categories/PostgreSQL/"}],"tags":[{"name":"关系数据库","slug":"关系数据库","permalink":"http://linbingdong.com/tags/关系数据库/"},{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"http://linbingdong.com/tags/PostgreSQL/"}]},{"title":"ZooKeeper原理及其在Hadoop和HBase中的应用","slug":"ZooKeeper原理及其在Hadoop和HBase中的应用","date":"2017-03-23T00:47:28.000Z","updated":"2017-03-23T00:47:28.000Z","comments":true,"path":"2017/03/23/ZooKeeper原理及其在Hadoop和HBase中的应用/","link":"","permalink":"http://linbingdong.com/2017/03/23/ZooKeeper原理及其在Hadoop和HBase中的应用/","excerpt":"ZooKeeper是一个开源的分布式协调服务，由雅虎创建，是Google Chubby的开源实现。分布式应用程序可以基于ZooKeeper实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master选举、分布式锁和分布式队列等功能。","text":"ZooKeeper是一个开源的分布式协调服务，由雅虎创建，是Google Chubby的开源实现。分布式应用程序可以基于ZooKeeper实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master选举、分布式锁和分布式队列等功能。 简介ZooKeeper是一个开源的分布式协调服务，由雅虎创建，是Google Chubby的开源实现。分布式应用程序可以基于ZooKeeper实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master选举、分布式锁和分布式队列等功能。 基本概念本节将介绍ZooKeeper的几个核心概念。这些概念贯穿于之后对ZooKeeper更深入的讲解，因此有必要预先了解这些概念。 集群角色在ZooKeeper中，有三种角色： Leader Follower Observer 一个ZooKeeper集群同一时刻只会有一个Leader，其他都是Follower或Observer。 ZooKeeper配置很简单，每个节点的配置文件(zoo.cfg)都是一样的，只有myid文件不一样。myid的值必须是zoo.cfg中server.{数值}的{数值}部分。 zoo.cfg文件内容示例： maxClientCnxns=0# The number of milliseconds of each ticktickTime=2000# The number of ticks that the initial# synchronization phase can takeinitLimit=10# The number of ticks that can pass between# sending a request and getting an acknowledgementsyncLimit=5# the directory where the snapshot is stored.dataDir=/var/lib/zookeeper/data# the port at which the clients will connectclientPort=2181# the directory where the transaction logs are stored.dataLogDir=/var/lib/zookeeper/logsserver.1=192.168.20.101:2888:3888server.2=192.168.20.102:2888:3888server.3=192.168.20.103:2888:3888server.4=192.168.20.104:2888:3888server.5=192.168.20.105:2888:3888minSessionTimeout=4000maxSessionTimeout=100000 在装有ZooKeeper的机器的终端执行 zookeeper-server status 可以看当前节点的ZooKeeper是什么角色（Leader or Follower）。 [root@node-20-103 ~]# zookeeper-server statusJMX enabled by defaultUsing config: /etc/zookeeper/conf/zoo.cfgMode: follower [root@node-20-104 ~]# zookeeper-server statusJMX enabled by defaultUsing config: /etc/zookeeper/conf/zoo.cfgMode: leader 如上，node-20-104是Leader，node-20-103是follower。 ZooKeeper默认只有Leader和Follower两种角色，没有Observer角色。 为了使用Observer模式，在任何想变成Observer的节点的配置文件中加入：peerType=observer并在所有server的配置文件中，配置成observer模式的server的那行配置追加:observer，例如：server.1:localhost:2888:3888:observer ZooKeeper集群的所有机器通过一个Leader选举过程来选定一台被称为『Leader』的机器，Leader服务器为客户端提供读和写服务。 Follower和Observer都能提供读服务，不能提供写服务。两者唯一的区别在于，Observer机器不参与Leader选举过程，也不参与写操作的『过半写成功』策略，因此Observer可以在不影响写性能的情况下提升集群的读性能。 会话（Session）Session是指客户端会话，在讲解客户端会话之前，我们先来了解下客户端连接。在ZooKeeper中，一个客户端连接是指客户端和ZooKeeper服务器之间的TCP长连接。ZooKeeper对外的服务端口默认是2181，客户端启动时，首先会与服务器建立一个TCP连接，从第一次连接建立开始，客户端会话的生命周期也开始了，通过这个连接，客户端能够通过心跳检测和服务器保持有效的会话，也能够向ZooKeeper服务器发送请求并接受响应，同时还能通过该连接接收来自服务器的Watch事件通知。Session的SessionTimeout值用来设置一个客户端会话的超时时间。当由于服务器压力太大、网络故障或是客户端主动断开连接等各种原因导致客户端连接断开时，只要在SessionTimeout规定的时间内能够重新连接上集群中任意一台服务器，那么之前创建的会话仍然有效。 数据节点（ZNode）在谈到分布式的时候，一般『节点』指的是组成集群的每一台机器。而ZooKeeper中的数据节点是指数据模型中的数据单元，称为ZNode。ZooKeeper将所有数据存储在内存中，数据模型是一棵树（ZNode Tree），由斜杠（/）进行分割的路径，就是一个ZNode，如/hbase/master,其中hbase和master都是ZNode。每个ZNode上都会保存自己的数据内容，同时会保存一系列属性信息。 注：这里的ZNode可以理解成既是Unix里的文件，又是Unix里的目录。因为每个ZNode不仅本身可以写数据（相当于Unix里的文件），还可以有下一级文件或目录（相当于Unix里的目录）。 在ZooKeeper中，ZNode可以分为持久节点和临时节点两类。 持久节点 所谓持久节点是指一旦这个ZNode被创建了，除非主动进行ZNode的移除操作，否则这个ZNode将一直保存在ZooKeeper上。 临时节点 临时节点的生命周期跟客户端会话绑定，一旦客户端会话失效，那么这个客户端创建的所有临时节点都会被移除。 另外，ZooKeeper还允许用户为每个节点添加一个特殊的属性：SEQUENTIAL。一旦节点被标记上这个属性，那么在这个节点被创建的时候，ZooKeeper就会自动在其节点后面追加上一个整型数字，这个整型数字是一个由父节点维护的自增数字。 版本ZooKeeper的每个ZNode上都会存储数据，对应于每个ZNode，ZooKeeper都会为其维护一个叫作Stat的数据结构，Stat中记录了这个ZNode的三个数据版本，分别是version（当前ZNode的版本）、cversion（当前ZNode子节点的版本）和aversion（当前ZNode的ACL版本）。 状态信息每个ZNode除了存储数据内容之外，还存储了ZNode本身的一些状态信息。用 get 命令可以同时获得某个ZNode的内容和状态信息。如下： [zk: localhost:2181(CONNECTED) 23] get /yarn-leader-election/appcluster-yarn/ActiveBreadCrumbappcluster-yarnrm1cZxid = 0x1b00133dc0 //Created ZXID,表示该ZNode被创建时的事务IDctime = Tue Jan 03 15:44:42 CST 2017 //Created Time,表示该ZNode被创建的时间mZxid = 0x1d00000063 //Modified ZXID，表示该ZNode最后一次被更新时的事务IDmtime = Fri Jan 06 08:44:25 CST 2017 //Modified Time，表示该节点最后一次被更新的时间pZxid = 0x1b00133dc0 //表示该节点的子节点列表最后一次被修改时的事务ID。注意，只有子节点列表变更了才会变更pZxid，子节点内容变更不会影响pZxid。cversion = 0 //子节点的版本号dataVersion = 11 //数据节点的版本号aclVersion = 0 //ACL版本号ephemeralOwner = 0x0 //创建该节点的会话的seddionID。如果该节点是持久节点，那么这个属性值为0。dataLength = 22 //数据内容的长度numChildren = 0 //子节点的个数 在ZooKeeper中，version属性是用来实现乐观锁机制中的『写入校验』的（保证分布式数据原子性操作）。 事务操作在ZooKeeper中，能改变ZooKeeper服务器状态的操作称为事务操作。一般包括数据节点创建与删除、数据内容更新和客户端会话创建与失效等操作。对应每一个事务请求，ZooKeeper都会为其分配一个全局唯一的事务ID，用ZXID表示，通常是一个64位的数字。每一个ZXID对应一次更新操作，从这些ZXID中可以间接地识别出ZooKeeper处理这些事务操作请求的全局顺序。 WatcherWatcher（事件监听器），是ZooKeeper中一个很重要的特性。ZooKeeper允许用户在指定节点上注册一些Watcher，并且在一些特定事件触发的时候，ZooKeeper服务端会将事件通知到感兴趣的客户端上去。该机制是ZooKeeper实现分布式协调服务的重要特性。 ACLZooKeeper采用ACL（Access Control Lists）策略来进行权限控制。ZooKeeper定义了如下5种权限。 CREATE: 创建子节点的权限。 READ: 获取节点数据和子节点列表的权限。 WRITE：更新节点数据的权限。 DELETE: 删除子节点的权限。 ADMIN: 设置节点ACL的权限。 注意：CREATE 和 DELETE 都是针对子节点的权限控制。 ZooKeeper典型应用场景ZooKeeper是一个高可用的分布式数据管理与协调框架。基于对ZAB算法的实现，该框架能够很好地保证分布式环境中数据的一致性。也是基于这样的特性，使得ZooKeeper成为了解决分布式一致性问题的利器。 数据发布与订阅（配置中心）数据发布与订阅，即所谓的配置中心，顾名思义就是发布者将数据发布到ZooKeeper节点上，供订阅者进行数据订阅，进而达到动态获取数据的目的，实现配置信息的集中式管理和动态更新。 在我们平常的应用系统开发中，经常会碰到这样的需求：系统中需要使用一些通用的配置信息，例如机器列表信息、数据库配置信息等。这些全局配置信息通常具备以下3个特性。 数据量通常比较小。 数据内容在运行时动态变化。 集群中各机器共享，配置一致。 对于这样的全局配置信息就可以发布到ZooKeeper上，让客户端（集群的机器）去订阅该消息。 发布/订阅系统一般有两种设计模式，分别是推（Push）和拉（Pull）模式。 推：服务端主动将数据更新发送给所有订阅的客户端。 拉：客户端主动发起请求来获取最新数据，通常客户端都采用定时轮询拉取的方式。 ZooKeeper采用的是推拉相结合的方式。如下： 客户端想服务端注册自己需要关注的节点，一旦该节点的数据发生变更，那么服务端就会向相应的客户端发送Watcher事件通知，客户端接收到这个消息通知后，需要主动到服务端获取最新的数据（推拉结合）。 命名服务(Naming Service)命名服务也是分布式系统中比较常见的一类场景。在分布式系统中，通过使用命名服务，客户端应用能够根据指定名字来获取资源或服务的地址，提供者等信息。被命名的实体通常可以是集群中的机器，提供的服务，远程对象等等——这些我们都可以统称他们为名字（Name）。其中较为常见的就是一些分布式服务框架（如RPC、RMI）中的服务地址列表。通过在ZooKeepr里创建顺序节点，能够很容易创建一个全局唯一的路径，这个路径就可以作为一个名字。 ZooKeeper的命名服务即生成全局唯一的ID。 分布式协调/通知ZooKeeper中特有Watcher注册与异步通知机制，能够很好的实现分布式环境下不同机器，甚至不同系统之间的通知与协调，从而实现对数据变更的实时处理。使用方法通常是不同的客户端都对ZK上同一个ZNode进行注册，监听ZNode的变化（包括ZNode本身内容及子节点的），如果ZNode发生了变化，那么所有订阅的客户端都能够接收到相应的Watcher通知，并做出相应的处理。 ZK的分布式协调/通知，是一种通用的分布式系统机器间的通信方式。 心跳检测机器间的心跳检测机制是指在分布式环境中，不同机器（或进程）之间需要检测到彼此是否在正常运行，例如A机器需要知道B机器是否正常运行。在传统的开发中，我们通常是通过主机直接是否可以相互PING通来判断，更复杂一点的话，则会通过在机器之间建立长连接，通过TCP连接固有的心跳检测机制来实现上层机器的心跳检测，这些都是非常常见的心跳检测方法。 下面来看看如何使用ZK来实现分布式机器（进程）间的心跳检测。 基于ZK的临时节点的特性，可以让不同的进程都在ZK的一个指定节点下创建临时子节点，不同的进程直接可以根据这个临时子节点来判断对应的进程是否存活。通过这种方式，检测和被检测系统直接并不需要直接相关联，而是通过ZK上的某个节点进行关联，大大减少了系统耦合。 工作进度汇报在一个常见的任务分发系统中，通常任务被分发到不同的机器上执行后，需要实时地将自己的任务执行进度汇报给分发系统。这个时候就可以通过ZK来实现。在ZK上选择一个节点，每个任务客户端都在这个节点下面创建临时子节点，这样便可以实现两个功能： 通过判断临时节点是否存在来确定任务机器是否存活。 各个任务机器会实时地将自己的任务执行进度写到这个临时节点上去，以便中心系统能够实时地获取到任务的执行进度。 Master选举Master选举可以说是ZooKeeper最典型的应用场景了。比如HDFS中Active NameNode的选举、YARN中Active ResourceManager的选举和HBase中Active HMaster的选举等。 针对Master选举的需求，通常情况下，我们可以选择常见的关系型数据库中的主键特性来实现：希望成为Master的机器都向数据库中插入一条相同主键ID的记录，数据库会帮我们进行主键冲突检查，也就是说，只有一台机器能插入成功——那么，我们就认为向数据库中成功插入数据的客户端机器成为Master。 依靠关系型数据库的主键特性确实能够很好地保证在集群中选举出唯一的一个Master。但是，如果当前选举出的Master挂了，那么该如何处理？谁来告诉我Master挂了呢？显然，关系型数据库无法通知我们这个事件。但是，ZooKeeper可以做到！ 利用ZooKeepr的强一致性，能够很好地保证在分布式高并发情况下节点的创建一定能够保证全局唯一性，即ZooKeeper将会保证客户端无法创建一个已经存在的ZNode。也就是说，如果同时有多个客户端请求创建同一个临时节点，那么最终一定只有一个客户端请求能够创建成功。利用这个特性，就能很容易地在分布式环境中进行Master选举了。 成功创建该节点的客户端所在的机器就成为了Master。同时，其他没有成功创建该节点的客户端，都会在该节点上注册一个子节点变更的Watcher，用于监控当前Master机器是否存活，一旦发现当前的Master挂了，那么其他客户端将会重新进行Master选举。 这样就实现了Master的动态选举。 分布式锁分布式锁是控制分布式系统之间同步访问共享资源的一种方式。 分布式锁又分为排他锁和共享锁两种。 排他锁排他锁（Exclusive Locks，简称X锁），又称为写锁或独占锁。 如果事务T1对数据对象O1加上了排他锁，那么在整个加锁期间，只允许事务T1对O1进行读取和更新操作，其他任何事务都不能在对这个数据对象进行任何类型的操作（不能再对该对象加锁），直到T1释放了排他锁。 可以看出，排他锁的核心是如何保证当前只有一个事务获得锁，并且锁被释放后，所有正在等待获取锁的事务都能够被通知到。 如何利用ZooKeeper实现排他锁？ 定义锁 ZooKeeper上的一个ZNode可以表示一个锁。例如/exclusive_lock/lock节点就可以被定义为一个锁。 获得锁 如上所说，把ZooKeeper上的一个ZNode看作是一个锁，获得锁就通过创建ZNode的方式来实现。所有客户端都去/exclusive_lock节点下创建临时子节点/exclusive_lock/lock。ZooKeeper会保证在所有客户端中，最终只有一个客户端能够创建成功，那么就可以认为该客户端获得了锁。同时，所有没有获取到锁的客户端就需要到/exclusive_lock节点上注册一个子节点变更的Watcher监听，以便实时监听到lock节点的变更情况。 释放锁 因为/exclusive_lock/lock是一个临时节点，因此在以下两种情况下，都有可能释放锁。 当前获得锁的客户端机器发生宕机或重启，那么该临时节点就会被删除，释放锁。 正常执行完业务逻辑后，客户端就会主动将自己创建的临时节点删除，释放锁。 无论在什么情况下移除了lock节点，ZooKeeper都会通知所有在/exclusive_lock节点上注册了节点变更Watcher监听的客户端。这些客户端在接收到通知后，再次重新发起分布式锁获取，即重复『获取锁』过程。 共享锁 共享锁（Shared Locks，简称S锁），又称为读锁。如果事务T1对数据对象O1加上了共享锁，那么T1只能对O1进行读操作，其他事务也能同时对O1加共享锁（不能是排他锁），直到O1上的所有共享锁都释放后O1才能被加排他锁。 总结：可以多个事务同时获得一个对象的共享锁（同时读），有共享锁就不能再加排他锁（因为排他锁是写锁） ZooKeeper在大型分布式系统中的应用前面已经介绍了ZooKeeper的典型应用场景。本节将以常见的大数据产品Hadoop和HBase为例来介绍ZooKeeper在其中的应用，帮助大家更好地理解ZooKeeper的分布式应用场景。 ZooKeeper在Hadoop中的应用在Hadoop中，ZooKeeper主要用于实现HA(High Availability），包括HDFS的NamaNode和YARN的ResourceManager的HA。同时，在YARN中，ZooKeepr还用来存储应用的运行状态。HDFS的NamaNode和YARN的ResourceManager利用ZooKeepr实现HA的原理是一样的，所以本节以YARN为例来介绍。 从上图可以看出，YARN主要由ResourceManager（RM）、NodeManager（NM）、ApplicationMaster（AM）和Container四部分组成。其中最核心的就是ResourceManager。 ResourceManager负责集群中所有资源的统一管理和分配，同时接收来自各个节点（NodeManager）的资源汇报信息，并把这些信息按照一定的策略分配给各个应用程序（Application Manager），其内部维护了各个应用程序的ApplicationMaster信息、NodeManager信息以及资源使用信息等。 为了实现HA，必须有多个ResourceManager并存（一般就两个），并且只有一个ResourceManager处于Active状态，其他的则处于Standby状态，当Active节点无法正常工作（如机器宕机或重启）时，处于Standby的就会通过竞争选举产生新的Active节点。 主备切换下面我们就来看看YARN是如何实现多个ResourceManager之间的主备切换的。 创建锁节点在ZooKeeper上会有一个/yarn-leader-election/appcluster-yarn的锁节点，所有的ResourceManager在启动的时候，都会去竞争写一个Lock子节点：/yarn-leader-election/appcluster-yarn/ActiveBreadCrumb，该节点是临时节点。ZooKeepr能够为我们保证最终只有一个ResourceManager能够创建成功。创建成功的那个ResourceManager就切换为Active状态，没有成功的那些ResourceManager则切换为Standby状态。 [zk: localhost:2181(CONNECTED) 16] get /yarn-leader-election/appcluster-yarn/ActiveBreadCrumbappcluster-yarnrm2cZxid = 0x1b00133dc0ctime = Tue Jan 03 15:44:42 CST 2017mZxid = 0x1f00000540mtime = Sat Jan 07 00:50:20 CST 2017pZxid = 0x1b00133dc0cversion = 0dataVersion = 28aclVersion = 0ephemeralOwner = 0x0dataLength = 22numChildren = 0 可以看到此时集群中ResourceManager2为Active。 注册Watcher监听所有Standby状态的ResourceManager都会向/yarn-leader-election/appcluster-yarn/ActiveBreadCrumb节点注册一个节点变更的Watcher监听，利用临时节点的特性，能够快速感知到Active状态的ResourceManager的运行情况。 主备切换当Active状态的ResourceManager出现诸如宕机或重启的异常情况时，其在ZooKeeper上连接的客户端会话就会失效，因此/yarn-leader-election/appcluster-yarn/ActiveBreadCrumb节点就会被删除。此时其余各个Standby状态的ResourceManager就都会接收到来自ZooKeeper服务端的Watcher事件通知，然后会重复进行步骤1的操作。 以上就是利用ZooKeeper来实现ResourceManager的主备切换的过程，实现了ResourceManager的HA。 HDFS中NameNode的HA的实现原理跟YARN中ResourceManager的HA的实现原理相同。其锁节点为/hadoop-ha/mycluster/ActiveBreadCrumb。 ResourceManager状态存储在 ResourceManager 中，RMStateStore 能够存储一些 RM 的内部状态信息，包括 Application 以及它们的 Attempts 信息、Delegation Token 及 Version Information 等。需要注意的是，RMStateStore 中的绝大多数状态信息都是不需要持久化存储的，因为很容易从上下文信息中将其重构出来，如资源的使用情况。在存储的设计方案中，提供了三种可能的实现，分别如下。 基于内存实现，一般是用于日常开发测试。 基于文件系统的实现，如HDFS。 基于ZooKeeper实现。 由于这些状态信息的数据量都不是很大，因此Hadoop官方建议基于ZooKeeper来实现状态信息的存储。在ZooKeepr上，ResourceManager 的状态信息都被存储在/rmstore这个根节点下面。 [zk: localhost:2181(CONNECTED) 28] ls /rmstore/ZKRMStateRoot[RMAppRoot, AMRMTokenSecretManagerRoot, EpochNode, RMDTSecretManagerRoot, RMVersionNode] RMAppRoot 节点下存储的是与各个 Application 相关的信息，RMDTSecretManagerRoot 存储的是与安全相关的 Token 等信息。每个 Active 状态的 ResourceManager 在初始化阶段都会从 ZooKeeper 上读取到这些状态信息，并根据这些状态信息继续进行相应的处理。 小结： ZooKeepr在Hadoop中的应用主要有： HDFS中NameNode的HA和YARN中ResourceManager的HA。 存储RMStateStore状态信息 ZooKeeper在HBase中的应用HBase主要用ZooKeeper来实现HMaster选举与主备切换、系统容错、RootRegion管理、Region状态管理和分布式SplitWAL任务管理等。 HMaster选举与主备切换HMaster选举与主备切换的原理和HDFS中NameNode及YARN中ResourceManager的HA原理相同。 系统容错当HBase启动时，每个RegionServer都会到ZooKeeper的/hbase/rs节点下创建一个信息节点（下文中，我们称该节点为”rs状态节点”），例如/hbase/rs/[Hostname]，同时，HMaster会对这个节点注册监听。当某个 RegionServer 挂掉的时候，ZooKeeper会因为在一段时间内无法接受其心跳（即 Session 失效），而删除掉该 RegionServer 服务器对应的 rs 状态节点。与此同时，HMaster 则会接收到 ZooKeeper 的 NodeDelete 通知，从而感知到某个节点断开，并立即开始容错工作。 HBase为什么不直接让HMaster来负责RegionServer的监控呢？如果HMaster直接通过心跳机制等来管理RegionServer的状态，随着集群越来越大，HMaster的管理负担会越来越重，另外它自身也有挂掉的可能，因此数据还需要持久化。在这种情况下，ZooKeeper就成了理想的选择。 RootRegion管理对应HBase集群来说，数据存储的位置信息是记录在元数据region，也就是RootRegion上的。每次客户端发起新的请求，需要知道数据的位置，就会去查询RootRegion，而RootRegion自身位置则是记录在ZooKeeper上的（默认情况下，是记录在ZooKeeper的/hbase/meta-region-server节点中）。当RootRegion发生变化，比如Region的手工移动、重新负载均衡或RootRegion所在服务器发生了故障等是，就能够通过ZooKeeper来感知到这一变化并做出一系列相应的容灾措施，从而保证客户端总是能够拿到正确的RootRegion信息。 Region管理HBase里的Region会经常发生变更，这些变更的原因来自于系统故障、负载均衡、配置修改、Region分裂与合并等。一旦Region发生移动，它就会经历下线（offline）和重新上线（online）的过程。 在下线期间数据是不能被访问的，并且Region的这个状态变化必须让全局知晓，否则可能会出现事务性的异常。对于大的HBase集群来说，Region的数量可能会多达十万级别，甚至更多，这样规模的Region状态管理交给ZooKeeper来做也是一个很好的选择。 分布式SplitWAL任务管理当某台RegionServer服务器挂掉时，由于总有一部分新写入的数据还没有持久化到HFile中，因此在迁移该RegionServer的服务时，一个重要的工作就是从WAL中恢复这部分还在内存中的数据，而这部分工作最关键的一步就是SplitWAL，即HMaster需要遍历该RegionServer服务器的WAL，并按Region切分成小块移动到新的地址下，并进行日志的回放（replay）。 由于单个RegionServer的日志量相对庞大（可能有上千个Region，上GB的日志），而用户又往往希望系统能够快速完成日志的恢复工作。因此一个可行的方案是将这个处理WAL的任务分给多台RegionServer服务器来共同处理，而这就又需要一个持久化组件来辅助HMaster完成任务的分配。当前的做法是，HMaster会在ZooKeeper上创建一个SplitWAL节点（默认情况下，是/hbase/SplitWAL节点），将“哪个RegionServer处理哪个Region”这样的信息以列表的形式存放到该节点上，然后由各个RegionServer服务器自行到该节点上去领取任务并在任务执行成功或失败后再更新该节点的信息，以通知HMaster继续进行后面的步骤。ZooKeeper在这里担负起了分布式集群中相互通知和信息持久化的角色。 小结： 以上就是一些HBase中依赖ZooKeeper完成分布式协调功能的典型场景。但事实上，HBase对ZooKeepr的依赖还不止这些，比如HMaster还依赖ZooKeeper来完成Table的enable/disable状态记录，以及HBase中几乎所有的元数据存储都是放在ZooKeeper上的。 由于ZooKeeper出色的分布式协调能力及良好的通知机制，HBase在各版本的演进过程中越来越多地增加了ZooKeeper的应用场景，从趋势上来看两者的交集越来越多。HBase中所有对ZooKeeper的操作都封装在了org.apache.hadoop.hbase.zookeeper这个包中，感兴趣的同学可以自行研究。 参考 《从Paxos到Zookeeper》","raw":null,"content":null,"categories":[{"name":"ZooKeeper","slug":"ZooKeeper","permalink":"http://linbingdong.com/categories/ZooKeeper/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://linbingdong.com/tags/大数据/"},{"name":"ZooKeeper","slug":"ZooKeeper","permalink":"http://linbingdong.com/tags/ZooKeeper/"}]},{"title":"Python函数式编程","slug":"Python函数式编程","date":"2017-03-15T08:24:20.000Z","updated":"2017-03-15T08:24:20.000Z","comments":true,"path":"2017/03/15/Python函数式编程/","link":"","permalink":"http://linbingdong.com/2017/03/15/Python函数式编程/","excerpt":"虽然 Python 不是函数式编程语言（是命令式编程语言），但是支持许多有价值的函数式编程工具。Python 提供 3 种内建函数和 lambda 表达式等来支持函数式编程。","text":"虽然 Python 不是函数式编程语言（是命令式编程语言），但是支持许多有价值的函数式编程工具。Python 提供 3 种内建函数和 lambda 表达式等来支持函数式编程。 匿名函数Python 允许用 lambda 关键字创造匿名函数。匿名顾名思义就是没有名字，即不需要以标准的方式来声明，比如说，使用 def 加函数名来声明。一个完整的 lambda “语句”代表了一个表达式，这个表达式的定义体必须和声明放在同一行。语法如下： lambda [arg1[, arg2, ... argN]]: expression 参数是可选的，如果使用参数的话，参数通常也会出现在表达式中。 注意：lambda 表达式返回可调用的函数对象。其实 lambda 表达式本身就是一个函数，这个函数定义了输入（冒号左边）和输出（冒号右边），只不过这个函数没有名字，但是我们可以把它赋给一个变量。 比如简单的加法函数。一般我们是这么写的： def add(x, y): return x+y lambda 表达式这么写： lambda x, y : x + y 我们可以把 lambda x, y : x + y 赋值给 f ，然后给 f 传参数： &gt;&gt;&gt; f = lambda x, y : x + y&gt;&gt;&gt; f&lt;function &lt;lambda&gt; at 0x10377f320&gt;&gt;&gt;&gt; f(-10,8)-2&gt;&gt;&gt; f(12, 100)112&gt;&gt;&gt; f(-33, -22)-55 可以看到，f 确实是个函数，可以接收两个参数，并返回这两个参数的和，等价于上面的 add 函数。 高阶函数高阶函数英文叫 Higher-order function 。一般函数的输入参数和返回值都只能是变量或常量，如果某个函数可以接收函数作为其输入参数，或者其返回值中包含函数，那么该函数就是高阶函数。 Python 中有三个内建的用来支持函数式编程的高阶函数，分别是 filter()，map() 和 reduce()。 filter()filter(function, sequence) 返回一个 sequence (序列)，返回的序列中包括了输入序列中所有调用 function(item) 后返回值为 true 的元素。 filter() 工作流程如下图： 举个栗子： &gt;&gt;&gt; def f(x): return x % 3 == 0 or x % 5 == 0...&gt;&gt;&gt; filter(f, range(2, 25))[3, 5, 6, 9, 10, 12, 15, 18, 20, 21, 24] 因为 filter() 的输入参数中包含函数 f() ，所以 filter() 是高阶函数。上面的例子中返回 2~24 中能被 3 或 5 整除的数组成的列表。 当然，也可以使用匿名函数 lambda 表达式实现： &gt;&gt;&gt; filter(lambda x : x % 3 == 0 or x % 5 == 0, range(2, 25))[3, 5, 6, 9, 10, 12, 15, 18, 20, 21, 24] 或者使用列表生成式： &gt;&gt;&gt; [x for x in range(2, 25) if x % 3 == 0 or x % 5 == 0][3, 5, 6, 9, 10, 12, 15, 18, 20, 21, 24] map()map() 与 filter() 相似，因为它也能通过函数来处理序列。map()将函数调用“映射”到序列的每个元素上，并返回一个含有所有返回值的列表。 map() 工作流程如下图： 举个栗子： &gt;&gt;&gt; def cube(x): return x**3...&gt;&gt;&gt; map(cube, range(1,11))[1, 8, 27, 64, 125, 216, 343, 512, 729, 1000] 上面的例子中，将 1~10 里的每个数分别调用 cube() ，并将返回值（x 的 3 次方）放入列表中。 lambda 表达式： &gt;&gt;&gt; map(lambda x : x**3, range(1, 11))[1, 8, 27, 64, 125, 216, 343, 512, 729, 1000] 列表生成式： &gt;&gt;&gt; [x**3 for x in range(1, 11)][1, 8, 27, 64, 125, 216, 343, 512, 729, 1000] 注意：map() 也可以处理多个序列。 &gt;&gt;&gt; map(lambda x, y: x + y, [1, 3, 5], [2, 4, 6])[3, 7, 11]&gt;&gt;&gt; map(lambda x, y: (x+y, x-y), [1, 3, 5], [2, 4, 6])[(3, -1), (7, -1), (11, -1)]&gt;&gt;&gt; map(None, [1, 3, 5], [2, 4, 6])[(1, 2), (3, 4), (5, 6)] 工作流程如下： reduce()reduce(function, sequence) 返回一个单值，它是这样构造的：首先以序列的前两个元素调用函数 function，再以返回值和第三个参数调用，依次执行下去。 reduce() 工作流程如下： 例如，以下程序计算 0 到 5 的整数之和: &gt;&gt;&gt; def add(x, y): return x+y...&gt;&gt;&gt; reduce(add, range(0, 5))10 实际上 reduce() 执行了如下的运算： ((((0+1)+2)+3)+4) ==&gt; 10 lambda 表达式： reduce(lambda x, y : x + y, range(0, 5)) 偏函数偏函数解决这样的问题：如果我们有函数是多个参数的，我们希望能固定其中某几个参数的值（类似于默认值）。 举个栗子： int() 函数可以把字符串转换为整数，当仅传入字符串时，int() 函数默认按十进制转换： &gt;&gt;&gt; int('11111')11111 但 int() 函数还提供额外的 base 参数（默认值为10） 。如果传入 base 参数，就可以做 N 进制的转换： &gt;&gt;&gt; int('11111',8)4681&gt;&gt;&gt; int('11111',base=16)69905 假设要转换大量的二进制字符串，每次都传入 int(x, base=2) 非常麻烦，于是，我们想到，可以定义一个 int2() 的函数，默认把 base=2 传进去： def int2(x, base=2): return int(x, base) 这样，我们就可以方便地转换二进制了： &gt;&gt;&gt; int2('1000000')64&gt;&gt;&gt; int2('1010101')85 functools.partial 就是帮助我们创建一个偏函数的，不需要我们自己定义 int2() ，可以直接使用下面的代码创建一个新的函数 int2 ： &gt;&gt;&gt; import functools&gt;&gt;&gt; int2 = functools.partial(int, base=2)&gt;&gt;&gt; int2('11111')31&gt;&gt;&gt; int2('10000')16 总结一下，functools.partial 的作用就是，把一个函数的某些参数给固定住（也就是设置默认值），返回一个新的函数，调用这个新函数会更简单。 需要注意的是，上面的新的 int2 函数，仅仅是把 base 参数重新设定默认值为 2 ，但也可以在函数调用时传入其他值： &gt;&gt;&gt; int2('11111',base=10)11111&gt;&gt;&gt; int2('11111',base=8)4681 参考 《Python核心编程》 Python官方文档 廖雪峰的Python教程","raw":null,"content":null,"categories":[{"name":"Python","slug":"Python","permalink":"http://linbingdong.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://linbingdong.com/tags/Python/"}]},{"title":"Hadoop YARN介绍","slug":"Hadoop YARN介绍","date":"2017-03-11T12:33:42.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2017/03/11/Hadoop YARN介绍/","link":"","permalink":"http://linbingdong.com/2017/03/11/Hadoop YARN介绍/","excerpt":"YARN是Hadoop集群的资源管理系统。Hadoop2.0对MapReduce框架做了彻底的设计重构。YARN的基本设计思想是将MRv1中的JobTracker拆分成了两个独立的服务：一个全局的资源管理器ResourceManager和每个应用程序特有的ApplicationMaster。其中ResourceManager负责整个系统的资源管理和分配，而ApplicationMaster负责单个应用程序的管理。","text":"YARN是Hadoop集群的资源管理系统。Hadoop2.0对MapReduce框架做了彻底的设计重构。YARN的基本设计思想是将MRv1中的JobTracker拆分成了两个独立的服务：一个全局的资源管理器ResourceManager和每个应用程序特有的ApplicationMaster。其中ResourceManager负责整个系统的资源管理和分配，而ApplicationMaster负责单个应用程序的管理。 YARN产生背景MRv1的局限YARN是在MRv1基础上演化而来的，它克服了MRv1中的各种局限性。在正式介绍YARN之前，先了解下MRv1的一些局限性，主要有以下几个方面： 扩展性差。在MRv1中，JobTracker同时兼备了资源管理和作业控制两个功能，这成为系统的一个最大瓶颈，严重制约了Hadoop集群扩展性。 可靠性差。MRv1采用了master/slave结构，其中，master存在单点故障问题，一旦它出现故障将导致整个集群不可用。 资源利用率低。MRv1采用了基于槽位的资源分配模型，槽位是一种粗粒度的资源划分单位，通常一个任务不会用完槽位对应的资源，且其他任务也无法使用这些空闲资源。此外，Hadoop将槽位分为Map Slot和Reduce Slot两种，且不允许它们之间共享，常常会导致一种槽位资源紧张而另外一种闲置（比如一个作业刚刚提交时，只会运行Map Task，此时Reduce Slot闲置）。 无法支持多种计算框架。随着互联网高速发展，MapReduce这种基于磁盘的离线计算框架已经不能满足应用要求，从而出现了一些新的计算框架，包括内存计算框架、流式计算框架和迭代式计算框架等，而MRv1不能支持多种计算框架并存。 为了克服以上几个缺点，Apache开始尝试对Hadoop进行升级改造，进而诞生了更加先进的下一代MapReduce计算框架MRv2。正是由于MRv2将资源管理功能抽象成了一个独立的通用系统YARN，直接导致下一代MapReduce的核心从单一的计算框架MapReduce转移为通用的资源管理系统YARN。 集群资源统一管理随着互联网的高速发展，新的计算框架不断出现，从支持离线处理的MapReduce，到支持在线处理的Storm，从迭代式计算框架Spark到流式处理框架S4，各种框架各有所长，各自解决了某一类应用问题。这时候就需要一个组件对同一个集群上的不同计算框架进行资源的统一管理。 相比于“一种计算框架一个集群”的模式，共享集群的模式存在多种好处： 资源利用率高。如果每个框架一个集群，可能在某段时间内，有些计算框架的集群资源紧张，而另外一些集群资源空闲。共享集群模式则通过多种框架共享资源，使得集群中的资源得到更加充分的利用。 运维成本低。如果采用“一个框架一个集群”的模式，则可能需要多个管理员管理这些集群，进而增加运维成本，而共享模式通常需要少数管理员即可完成多个框架的统一管理。 数据共享。随着数据量的暴增，跨集群间的数据移动不仅需花费更长的时间，且硬件成本也会大大增加，而共享集群模式可让多种框架共享数据和硬件资源，将大大减小数据移动带来的成本。 YARN基本设计思想MRv1主要由编程模型、数据处理引擎（由Map Task和Reduce Task组成）和运行时环境三部分组成。为了保证编程模型的向后兼容性，MRv2重用了MRv1中的编程模型和数据处理引擎，但运行时环境被完全重写。 MRv1的运行时环境主要由两类服务组成，分别是JobTracker和TaskTracker。其中，JobTracker负责资源管理和作业控制。TaskTracker负责单个节点的资源管理和任务执行。 MRv1将资源管理和应用程序管理两部分混杂在一起，使得它在扩展性、容错性和多框架支持等方面存在明显缺陷。 而MRv2则通过将资源管理和应用程序管理两部分剥离开，分别由ResourceManager和ApplicationMaster负责，其中ResourceManager专管资源管理和调度，而ApplicationMaster则负责与具体应用程序相关的任务切分、任务调度和容错等，具体如下图所示。 YARN基本架构YARN是Hadoop 2.0中的资源管理系统，它的基本设计思想是将MRv1中的JobTracker拆分成了两个独立的服务：一个全局的资源管理器ResourceManager和每个应用程序特有的ApplicationMaster。其中ResourceManager负责整个系统的资源管理和分配，而ApplicationMaster负责单个应用程序的管理。 YARN总体上仍然是Master/Slave结构，在整个资源管理框架中，ResourceManager为Master，NodeManager为Slave，ResourceManager负责对各个NodeManager上的资源进行统一管理和调度。当用户提交一个应用程序时，需要提供一个用以跟踪和管理这个程序的ApplicationMaster，它负责向ResourceManager申请资源，并要求NodeManger启动可以占用一定资源的任务。由于不同的ApplicationMaster被分布到不同的节点上，因此它们之间不会相互影响。 下图描述了YARN的基本组成结构，YARN主要由ResourceManager、NodeManager、ApplicationMaster（图中给出了MapReduce和MPI两种计算框架的ApplicationMaster，分别为MR AppMstr和MPI AppMstr）和Container等几个组件构成。 接下来对YARN里几个重要的组件一一介绍。 1. ResourceManager(RM)RM是一个全局的资源管理器,负责整个系统的资源管理和分配。它主要由两个组件构成：调度器（Scheduler）和应用程序管理器（Applications Manager，ASM）。 （1）调度器（分配Container） 调度器根据容量、队列等限制条件（如每个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。需要注意的是，该调度器是一个“纯调度器”，它不再从事任何与具体应用程序相关的工作，比如不负责监控或者跟踪应用的执行状态等，也不负责重新启动因应用执行失败或者硬件故障而产生的失败任务，这些均交由应用程序相关的ApplicationMaster完成。调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位用一个抽象概念“资源容器”（Resource Container，简称Container）表示，Container是一个动态资源分配单位，它将内存、CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。此外，该调度器是一个可插拔的组件，用户可根据自己的需要设计新的调度器，YARN提供了多种直接可用的调度器，比如Fair Scheduler和Capacity Scheduler等。 （2）应用程序管理器 应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状态并在失败时重新启动它等。 2. ApplicationMaster（AM）用户提交的每个应用程序均包含一个AM，主要功能包括： 与RM调度器协商以获取资源（以Container表示） 将得到的任务进一步分配给内部的任务 与NM通信以启动/停止任务 监控所有任务运行状态，并在任务失败时重新为任务申请资源以重启任务 3. NodeManager（NM）NM是每个节点上的资源和任务管理器。一方面，它定时地向RM汇报本节点的资源使用情况和Container运行状态；另一方面，它接受并处理来自AM的Container启动/停止等各种请求。 4. ContainerContainer是YARN中的资源抽象，它封装了某个节点上的多维资源，如CPU、内存、磁盘、网络等。当AM向RM申请资源时，RM向AM返回的资源便是用Container表示的。YARN会为每个任务分配一个Container，且该任务只能使用该Container中描述的资源。Container是一个动态资源划分单位，是根据应用程序的需求自动生成的。目前，YARN仅支持CPU和内存两种资源。 YARN工作流程运行在YARN上的应用程序主要分为两类：短应用程序和长应用程序。其中，短应用程序是指一定时间内可运行完成并正常退出的应用程序，如MapReduce作业、Spark DAG作业等。长应用程序是指不出意外，永不终止运行的应用程序，通常是一些服务，比如Storm Service（包括Nimbus和Supervisor两类服务），HBase Service（包括HMaster和RegionServer两类服务）等，而它们本身作为一种框架提供编程接口供用户使用。尽管这两类应用程序作业不同，一类直接运行数据处理程序，一类用于部署服务（服务之上再运行数据处理程序），但运行在YARN上的流程是相同的。 当用户向YARN中提交一个应用程序后，YARN将分两个阶段运行该应用程序：第一阶段是启动ApplicationMaster。第二阶段是由ApplicationMaster创建应用程序，为它申请资源，并监控它的整个运行过程，直到运行完成。具体如下： 用户向YARN中提交应用程序，其中包括ApplicationMaster程序、启动ApplicationMaster的命令、用户程序等。 ResourceManager为该应用程序分配第一个Container，并与对应的NodeManager通信，要求它在这个Container中启动应用程序的ApplicationMaster。 ApplicationMaster首先向ResourceManager注册，这样用户就可以直接通过ResourceManager查看应用程序的运行状态，然后它将为各个任务申请资源，并监控它的运行状态，直到运行结束，即重复步骤4~7。 ApplicationMaster采用轮询的方式通过RPC协议向ResourceManager申请和领取资源。 一旦ApplicationMaster申请到资源后，便与对应的NodeManager通信，要求它启动任务。 NodeManager为任务设置好运行环境（包括环境变量、JAR包、二进制程序等）后，将任务启动命令写到一个脚本中，并通过运行该脚本启动任务。 各个任务通过某个RPC协议向ApplicationMaster汇报自己的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。 应用程序运行完成后，ApplicationMaster向ResourceManager注销并关闭自己。","raw":null,"content":null,"categories":[{"name":"YARN","slug":"YARN","permalink":"http://linbingdong.com/categories/YARN/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://linbingdong.com/tags/大数据/"},{"name":"YARN","slug":"YARN","permalink":"http://linbingdong.com/tags/YARN/"},{"name":"Hadoop","slug":"Hadoop","permalink":"http://linbingdong.com/tags/Hadoop/"},{"name":"资源管理","slug":"资源管理","permalink":"http://linbingdong.com/tags/资源管理/"}]},{"title":"Java文件管理操作","slug":"Java文件管理操作","date":"2017-03-11T12:33:42.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2017/03/11/Java文件管理操作/","link":"","permalink":"http://linbingdong.com/2017/03/11/Java文件管理操作/","excerpt":"linux里有mkdir、rm、cp、mv、touch等对文件或目录进行操作的命令，Java中也有对应的方法。\nPath和Files类封装了处理文件系统所需的所有功能。比如，可以用Files类删除或重命名文件，或者查询文件最后被修改的时间等。换句话说，流类关心的是文件的内容，而Files类关心的是对文件和目录的操作。","text":"linux里有mkdir、rm、cp、mv、touch等对文件或目录进行操作的命令，Java中也有对应的方法。 Path和Files类封装了处理文件系统所需的所有功能。比如，可以用Files类删除或重命名文件，或者查询文件最后被修改的时间等。换句话说，流类关心的是文件的内容，而Files类关心的是对文件和目录的操作。 Path和Files是在Java SE 7中新添加进来的类，比自JDK 1.0以来就一直使用的File类要方便得多。 java.nio.file.Paths 7Path表示的是文件或目录的路径。是一个字符串。 java.nio.file.Paths static Path get(String first, String…more) Path dir = Paths.get(\"/Users/linbingdong\",\"Downloads\",\"xxx.txt\"); Paths.get方法通过给定的字符串创建一个路径。该方法可以接受一个或多个字符串，并自动将它们用默认文件系统的路径分隔符连接起来。 要操作文件或目录之前，都需要先用Paths.get方法生成相应的路径。 java.nio.file.Files 7Files类使得普通文件操作变得快捷。 java.nio.file.Files static byte[] readAllBytes(Path path) static Path write(Path path, byte[] contents, OpenOption…options) static Path copy(Path from, Path to, CopyOption…options) static Path move (Path from, Path to, CopyOption…options) static void delete(Path path) static Path createFile(Path path, FileAttribute&lt;?&gt;… attrs) static Path createDirectory(Path path, FileAttribute&lt;?&gt;…attrs) static Path createDirectories(Path path, FileAttribute&lt;?&gt;…attrs) static boolean exists(Path path) static boolean isReadable(Path path) static boolean isDirectory(Path path) 要读入文件内容是，首先将文件内容以字节的方式读入字节数组，再将字节数组作为参数传入新建的String对象。 要将String写入文件，需要调用String.getBytes()。 具体各方法的用法见示例 示例： import java.io.IOException;import java.nio.charset.Charset;import java.nio.file.*;/** * Created by lbd on 2017/1/5. */public class TestFile &#123; public static void main(String[] args) throws IOException &#123; Path from = Paths.get(\"/Users/linbingdong/hehe\"); Path to = Paths.get(\"/Users/linbingdong/hehe2\"); Path newDir = Paths.get(\"/Users/linbingdong/test\",\"hi\",\"hei\",\"oo\"); //Files.copy(from,to); Files.createDirectories(newDir); //Files.copy(from,newDir, StandardCopyOption.COPY_ATTRIBUTES,StandardCopyOption.REPLACE_EXISTING); //Files.move(from,to); System.out.println(newDir.getParent()); System.out.println(newDir.getRoot()); System.out.println(newDir.getFileName()); //获取路径中最后一个字段,不管是目录还是文件 System.out.println(newDir.getFileSystem()); System.out.println(newDir.getNameCount()); //路径层级的个数 byte[] bytes = Files.readAllBytes(to); //获取文本内容存入byte数组 String contents = new String(bytes, Charset.defaultCharset()); //将byte数组转化成String对象 System.out.println(contents); Files.write(to,contents.getBytes()); //覆盖写 获取String对象对应的byte数组 //Files.write(to,contents.getBytes(), StandardOpenOption.APPEND); //追加写 //Files.delete(newDir); Files.createDirectories(newDir); //创建路径中的所有中间目录 相当于mkdir -p Path newFile = Paths.get(\"/Users/linbingdong/test\",\"hi\",\"hei\",\"oo\",\"hhhh\"); //Files.createFile(newFile); //创建一个新文件 最后是文件名 System.out.println(Files.exists(newFile)); //判断是否存在 System.out.println(Files.size(newFile)); //返回文件的字节数 System.out.println(Files.isDirectory(newFile)); //判断是否是文件夹 Path basePath = Paths.get(\"/Users/linbingdong/test\"); try(DirectoryStream&lt;Path&gt; entries = Files.newDirectoryStream(basePath)) &#123; //迭代目录中的文件 for (Path entry: entries) &#123; System.out.println(entry.getFileName()); &#125; &#125; &#125;&#125; 注：以上的方法适用于处理中等长度的文本文件，如果要处理的文件较大，或者是二进制文件，那么还是应该使用所熟知的流或者读入器、写出器： InputStream in = Files.newInputStream(Path);OutputStream out = Files.newOutputStream(Path);Reader in = Files.newBufferedReader(Path,charset);Writer out = Files.newBufferedWriter(path,charset);","raw":null,"content":null,"categories":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/tags/Java/"}]},{"title":"Java对象流与序列化","slug":"Java对象流与序列化","date":"2017-03-11T12:33:42.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2017/03/11/Java对象流与序列化/","link":"","permalink":"http://linbingdong.com/2017/03/11/Java对象流与序列化/","excerpt":"Java语言支持一种称为对象序列化（object serialization）的非常通用的机制，它可以将任何对象写入到流中，并在之后将其读回。被序列化的每个对象都有一个序列号，唯一标识该对象，而不是通过该对象的内存地址来表示对象。这样就能将一个对象集合从一台集群传送到另一台机器，与对象在内存中的地址无关。","text":"Java语言支持一种称为对象序列化（object serialization）的非常通用的机制，它可以将任何对象写入到流中，并在之后将其读回。被序列化的每个对象都有一个序列号，唯一标识该对象，而不是通过该对象的内存地址来表示对象。这样就能将一个对象集合从一台集群传送到另一台机器，与对象在内存中的地址无关。 示例： ObjectStreamTest.java import java.io.*;/** * Created by lbd on 2017/1/5. */public class ObjectStreamTest &#123; public static void main(String[] args) throws IOException, ClassNotFoundException &#123; Employee harry = new Employee(\"Harry Hacker\", 50000, 1989, 10, 1); Manager carl = new Manager(\"Carl Cracker\", 80000, 1987, 12, 15); carl.setSecretary(harry); Manager tony = new Manager(\"Tony Tester\", 40000, 1990, 3, 15); tony.setSecretary(harry); Employee[] staff = new Employee[3]; staff[0] = carl; staff[1] = harry; staff[2] = tony; try(ObjectOutputStream out = new ObjectOutputStream(new FileOutputStream(\"employee1.dat\")))&#123; out.writeObject(staff); &#125; try(ObjectInputStream in = new ObjectInputStream(new FileInputStream(\"employee1.dat\")))&#123; Employee[] newStaff = (Employee[]) in.readObject(); for (Employee e : newStaff)&#123; System.out.println(e); &#125; &#125; &#125;&#125;class Employee implements Serializable&#123; String name; double salary; int year; int month; int day; public Employee(String name,double salary,int year,int month,int day)&#123; this.name = name; this.salary = salary; this.year = year; this.month = month; this.day = day; &#125; public String getName() &#123; return name; &#125; public double getSalary() &#123; return salary; &#125; public int getYear() &#123; return year; &#125; public int getMonth() &#123; return month; &#125; public int getDay() &#123; return day; &#125; @Override public String toString() &#123; return this.name + \" \" + this.salary + \" \" + this.year + \" \" + this.month + \" \" + this.day; &#125;&#125;class Manager extends Employee implements Serializable&#123; Employee secretary; public Manager(String name, double salary, int year, int month, int day) &#123; super(name, salary, year, month, day); &#125; public void setSecretary(Employee e)&#123; this.secretary = e; &#125;&#125; 注意： Employee和Manager都必须实现Serializable接口才能被序列化!(Serializable接口没有任何方法) 对象流输出中包含所有对象的类型和数据域 每个对象都被赋予一个序列号 相同对象的重复出现将被存储为对这个对象的序列号的引用","raw":null,"content":null,"categories":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/tags/Java/"}]},{"title":"Hive安装配置文档（含Hive Metastore三种配置方式详解）","slug":"Hive安装配置文档","date":"2017-03-11T12:33:42.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2017/03/11/Hive安装配置文档/","link":"","permalink":"http://linbingdong.com/2017/03/11/Hive安装配置文档/","excerpt":"本文介绍Hive安装配置的整个过程，包括MySQL、Hive及Metastore的安装配置，并分析了Metastore三种配置方式的区别。","text":"本文介绍Hive安装配置的整个过程，包括MySQL、Hive及Metastore的安装配置，并分析了Metastore三种配置方式的区别。 网上有很多介绍Hive Metastore三种配置方式的文章，但是理解都不对，给读者造成了很多误导。本人详细阅读Apache和CDH官方文档中关于Hive Metastore的部分，并经过实践，终于填好各种坑，安装配置成功，遂记录下本文，供大家参考。 1. 相关概念Hive Metastore有三种配置方式，分别是： Embedded Metastore Database (Derby) 内嵌模式 Local Metastore Server 本地元存储 Remote Metastore Server 远程元存储 Metadata、Metastore作用 metadata即元数据。元数据包含用Hive创建的database、tabel等的元信息。元数据存储在关系型数据库中。如Derby、MySQL等。 Metastore的作用是：客户端连接metastore服务，metastore再去连接MySQL数据库来存取元数据。有了metastore服务，就可以有多个客户端同时连接，而且这些客户端不需要知道MySQL数据库的用户名和密码，只需要连接metastore 服务即可。 三种配置方式区别 内嵌模式使用的是内嵌的Derby数据库来存储元数据，也不需要额外起Metastore服务。这个是默认的，配置简单，但是一次只能一个客户端连接，适用于用来实验，不适用于生产环境。 本地元存储和远程元存储都采用外部数据库来存储元数据，目前支持的数据库有：MySQL、Postgres、Oracle、MS SQL Server.在这里我们使用MySQL。 本地元存储和远程元存储的区别是：本地元存储不需要单独起metastore服务，用的是跟hive在同一个进程里的metastore服务。远程元存储需要单独起metastore服务，然后每个客户端都在配置文件里配置连接到该metastore服务。远程元存储的metastore服务和hive运行在不同的进程里。 在生产环境中，建议用远程元存储来配置Hive Metastore。 集群规划本教程Hadoop相关软件全部基于CDH5.5.1，用yum安装，系统环境如下： 操作系统：CentOS 7.2 Hadoop 2.6.0 Hive1.1.0 Spark1.5.0 MySQL 5.6 JDK 1.8 Maven 3.3.3 Scala 2.10 各节点规划如下： 192.168.117.51 Goblin01 nn1 jn1 rm1 worker master hive metastore mysql192.168.117.52 Goblin02 zk2 nn2 jn2 rm2 worker hive metastore192.168.117.53 Goblin03 zk3 dn1 jn3 worker hive192.168.117.54 Goblin04 zk4 dn2 worker hive 说明：Goblin01~04是每台机器的hostname，zk代表zookeeper，nn代表hadoop的namenode，dn代表datanode，jn代表journalnode，rm代表resourcemanager，worker代表Spark的slaves，master代表Spark的master 如果不需要Hive on Spark，只需要Hive on MR，则不需要安装Spark、Maven和Scala。 我们把metastore服务和MySQL都装在51上（装在哪一台都可以），51-54都安装Hive，这样多个客户端可以同时执行Hive命令。 在执行以下步骤之前，请确保已经安装了Hadoop集群 安装MySQL 下载mysql的repo源 $ wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm 安装mysql-community-release-el7-5.noarch.rpm包 $ sudo rpm -ivh mysql-community-release-el7-5.noarch.rpm 安装这个包后，会获得两个mysql的yum repo源：/etc/yum.repos.d/mysql-community.repo，/etc/yum.repos.d/mysql-community-source.repo。 安装mysql $ sudo yum install mysql-server 配置MySQL和metastoreStep 1: Install and start MySQL if you have notalready done so $ sudo yum install mysql-server$ sudo service mysqld start Step 2: Configure the MySQL Service and Connector 因为使用MySQL作为存储元数据的数据库，所以需要把连接MySQL的jar包放入或链接到$HIVE_HOME/lib目录下。 $ sudo yum install mysql-connector-java$ ln -s /usr/share/java/mysql-connector-java.jar /usr/lib/hive/lib/mysql-connector-java.jar To set the MySQL root password: $ sudo /usr/bin/mysql_secure_installation[...]Enter current password for root (enter for none):OK, successfully used password, moving on...[...]Set root password? [Y/n] yNew password:Re-enter new password:Remove anonymous users? [Y/n] Y[...]Disallow root login remotely? [Y/n] N[...]Remove test database and access to it [Y/n] Y[...]Reload privilege tables now? [Y/n] YAll done! To make sure the MySQL server starts at boot: $ sudo /sbin/chkconfig mysqld on$ sudo /sbin/chkconfig --list mysqldmysqld 0:off 1:off 2:on 3:on 4:on 5:on 6:off Step 3. Create the Database and User $ mysql -u root -pEnter password:mysql&gt; CREATE DATABASE metastore;mysql&gt; USE metastore;mysql&gt; SOURCE /usr/lib/hive/scripts/metastore/upgrade/mysql/hive-schema-0.12.0.mysql.sql;mysql&gt; CREATE USER &apos;hive&apos;@&apos;metastorehost&apos; IDENTIFIED BY &apos;mypassword&apos;;...mysql&gt; REVOKE ALL PRIVILEGES, GRANT OPTION FROM &apos;hive&apos;@&apos;metastorehost&apos;;mysql&gt; GRANT ALL ON metastore.* TO &apos;hive&apos;@&apos;metastorehost&apos; IDENTIFIED BY &apos;hive&apos;;mysql&gt; GRANT ALL ON metastore.* TO &apos;hive&apos;@&apos;%&apos; IDENTIFIED BY &apos;hive&apos;;mysql&gt; FLUSH PRIVILEGES;mysql&gt; ALTER DATABASE metastore CHARACTER SET latin1;mysql&gt; quit; Step 4. Format the Database $ cd /usr/lib/hive/bin$ ./schematool --dbType mysql --initSchema Hive配置HDFS存储位置配置Hive配置文件里要用到HDFS的一些路径，需要先手动创建。 hdfs dfs -mkdir -p /usr/hive/warehousehdfs dfs -mkdir -p /usr/hive/tmphdfs dfs -mkdir -p /usr/hive/loghdfs dfs -chmod g+w /usr/hive/warehousehdfs dfs -chmod g+w /usr/hive/tmphdfs dfs -chmod g+w /usr/hive/log 上述语句涉及hive-site.xml hive.metastore.warehouse.dir等，表示数据在hdfs中的存储位置 hive-env.sh (所有节点）export HADOOP_HOME=/usr/lib/hadoopexport HIVE_CONF_DIR=/usr/lib/hive/conf hive-log4j.properties（所有节点）首先创建log存放的文件夹 mkdir /usr/lib/hive/logs 然后配置hive-log4j.properties hive.log.dir=/usr/lib/hive/logs 服务端hive-site.xml服务端指的是Metastore服务所在的机器，即安装metastore的机器，这里是51和52。 &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://Goblin01:3306/metastore?createDatabaseIfNotExist=true&lt;/value&gt; &lt;description&gt;the URL of the MySQL database&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/usr/hive/warehouse&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hive.exec.scratchdir&lt;/name&gt;&lt;value&gt;/usr/hive/tmp&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hive.querylog.location&lt;/name&gt;&lt;value&gt;/usr/hive/log&lt;/value&gt;&lt;/property&gt; 客户端hive-site.xml这里指的是53和54。 &lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://Goblin01:9083,Goblin02:9083&lt;/value&gt; &lt;description&gt;IP address (or fully-qualified domain name) and port of the metastore host&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/usr/hive/warehouse&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.exec.scratchdir&lt;/name&gt; &lt;value&gt;/usr/hive/tmp&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.querylog.location&lt;/name&gt; &lt;value&gt;/usr/hive/log&lt;/value&gt;&lt;/property&gt; 启动Hive 启动MySQL $ service mysqld start 启动metastore服务 $ service hive-metastore start 启动Hive CLI 因为在4台机器上都安装了hive，并且作了相关的配置，所有四台机器均可以启动Hive CLI（Hive交互式shell） $ hive 参考资料 https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin#AdminManualMetastoreAdmin-RemoteMetastoreDatabase http://www.cloudera.com/documentation/archive/cdh/4-x/4-2-0/CDH4-Installation-Guide/cdh4ig_topic_18_4.html \u0003\u0001","raw":null,"content":null,"categories":[{"name":"Hive","slug":"Hive","permalink":"http://linbingdong.com/categories/Hive/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://linbingdong.com/tags/大数据/"},{"name":"NoSQL","slug":"NoSQL","permalink":"http://linbingdong.com/tags/NoSQL/"},{"name":"Hive","slug":"Hive","permalink":"http://linbingdong.com/tags/Hive/"}]},{"title":"Phoenix安装配置","slug":"Phoenix安装配置","date":"2017-03-11T12:33:42.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2017/03/11/Phoenix安装配置/","link":"","permalink":"http://linbingdong.com/2017/03/11/Phoenix安装配置/","excerpt":"介绍Phoenix安装配置的过程，记录在安装配置过程中遇到的问题及解决方案。","text":"介绍Phoenix安装配置的过程，记录在安装配置过程中遇到的问题及解决方案。 1.简介Phoenix最早是saleforce的一个开源项目，后来成为Apache基金的顶级项目 Phoenix是构建在HBase上的一个SQL层，能让我们用标准的JDBC APIs而不是HBase客户端APIs来创建表、插入数据和对HBase数据进行查询 因此Phoenix跟HBase是离不开的，Phoenix的安装也是基于HBase的。在安装Phoenix之前，请确保集群上已经安装了Hadoop集群跟HBase集群 本集群上已部署CHD5.5.1版本的Hadooop（2.6.0）和HBase（1.0.0） 各节点规划如下： 192.168.20.100 node-20-100 hdfs-master1 nn1 jn1 hmaster1192.168.20.101 node-20-101 hdfs-master2 nn2 zk1 jn2 hmaster2 Phoenix192.168.20.102 node-20-102 hdfs-slave1 dn1 zk2 jn3 hregionserver1192.168.20.103 node-20-103 hdfs-slave2 dn2 zk3 jn4 hregionserver2192.168.20.104 node-20-104 hdfs-slave3 dn3 zk4 jn5 hregionserver3192.168.20.105 node-20-105 hdfs-slave4 dn4 zk5 hregionserver4 注意：Cloudera官方并不支持Phoenix，也就是说从Phoenix官网下的预编译的包在CDH安装的HBase上根本不能运行，Phoenix官网对此也没有任何说明！！！刚开始按照Phoenix官网给出的步骤安装完Phoenix后运行报错，幸好有StackOverflow，才找到了原因，并且找到解决方案。链接如下：http://stackoverflow.com/questions/31849454/using-phoenix-with-cloudera-hbase-installed-from-repo 解决方案：需要自己从Phoenix官网下载跟HBase版本对应的Phoenix版本（Phoenix 4.x版本均支持HBase 1.0.0）的源码来编译，得到编译后的两个相关jar包。具体安装步骤如下 2.安装配置Phoenix最新版本是4.8，4.x均支持HBase 1.0.0，这里选用Phoenix 4.6.0版本 2.1下载并解压从Phoenix官网下载预编译的phoenix-4.6.0-HBase-1.0-bin.tar.gz Phoenix客户端所在的节点最好安装有ZooKeeper，方便后续操作 这里选择192.168.20.101节点作为Phoenix客户端 切换到/opt目录下：$ cd /opt wget：$ wget &#39;http://archive.apache.org/dist/phoenix/phoenix-4.6.0-HBase-1.0/bin/phoenix-4.6.0-HBase-1.0-bin.tar.gz&#39; 解压：$ tar -xzvf phoenix-4.6.0-HBase-1.0-bin.tar.gz 建立软链接：$ ln -s phoenix-4.6.0-HBase-1.0-bin phoenix 2.2编译源码如果从Phoenix官网下载4.6.0的源码来编译，需要自己修改pom.xml文件，比较麻烦。Github上已经有人把修改过pom.xml文件的源码上传了，链接如下：https://github.com/chiastic-security/phoenix-for-cloudera/tree/4.6-HBase-1.0-cdh5.5 下载该链接的源码，用maven进行编译 进入phoenix-for-cloudera-4.6-HBase-1.0-cdh5.5目录 $ mvn package -DskipTests 编译成功后显示如下： 2.3替换将编译后`phoenix-for-cloudera-4.6-HBase-1.0-cdh5.5/phoenix-assembly/target目录下的phoenix-4.6.0-cdh5.5.1-client.jar和phoenix-4.6.0-cdh5.5.1-server.jar分别替换/opt/phoenix目录下的phoenix-4.6.0-HBase-1.0-client.jar和phoenix-4.6.0-HBase-1.0-server.jar 2.4将相关jar拷贝到HBase的lib目录下拷贝phoenix-4.6.0-cdh5.5.1-server.jar到集群上每个HBase的lib目录下 $ cp phoenix-4.6.0-cdh5.5.1-server.jar /usr/lib/hbase/lib$ scp phoenix-4.6.0-cdh5.5.1-server.jar node-20-100:/usr/lib/hbase/lib/phoenix-4.6.0-cdh5.5.1-server.jar$ scp phoenix-4.6.0-cdh5.5.1-server.jar node-20-102:/usr/lib/hbase/lib/phoenix-4.6.0-cdh5.5.1-server.jar$ scp phoenix-4.6.0-cdh5.5.1-server.jar node-20-103:/usr/lib/hbase/lib/phoenix-4.6.0-cdh5.5.1-server.jar$ scp phoenix-4.6.0-cdh5.5.1-server.jar node-20-104:/usr/lib/hbase/lib/phoenix-4.6.0-cdh5.5.1-server.jar$ scp phoenix-4.6.0-cdh5.5.1-server.jar node-20-105:/usr/lib/hbase/lib/phoenix-4.6.0-cdh5.5.1-server.jar 2.5配置Phoenix客户端的CLASSPATH将phoenix-4.6.0-cdh5.5.1-client.jar添加到Phoenix客户端的CLASSPATH中，这里是192.168.20.101节点 在/etc/profile.d目录下新建phoenix.shexport CLASSPATH=.:/opt/phoenix/phoenix-4.6.0-cdh5.5.1-client.jar $ source phoenix.sh 2.6配置hbase-site.xml Master的hbase-site.xml &lt;property&gt; &lt;name&gt;hbase.regionserver.wal.codec&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.master.loadbalancer.class&lt;/name&gt;&lt;value&gt;org.apache.phoenix.hbase.index.balancer.IndexLoadBalancer&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.coprocessor.master.classes&lt;/name&gt;&lt;value&gt;org.apache.phoenix.hbase.index.master.IndexMasterObserver&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.rpc.timeout&lt;/name&gt; &lt;value&gt;300000&lt;/value&gt;&lt;/property&gt; Region Server的hbase-site.xml &lt;property&gt; &lt;name&gt;hbase.regionserver.wal.codec&lt;/name&gt;&lt;value&gt;org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hbase.region.server.rpc.scheduler.factory.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hbase.ipc.PhoenixRpcSchedulerFactory&lt;/value&gt;&lt;description&gt;Factory to create the Phoenix RPC Scheduler that usesseparate queues for index and metadata updates&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.rpc.controllerfactory.class&lt;/name&gt;&lt;value&gt;org.apache.hadoop.hbase.ipc.controller.ServerRpcControllerFactory&lt;/value&gt;&lt;description&gt;Factory to create the Phoenix RPCScheduler that uses separate queues for index and metadataupdates&lt;/description&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hbase.coprocessor.regionserver.classes&lt;/name&gt;&lt;value&gt;org.apache.hadoop.hbase.regionserver.LocalIndexMerger&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.rpc.timeout&lt;/name&gt; &lt;value&gt;300000&lt;/value&gt;&lt;/property&gt; 至此，Phoenix安装配置完毕 3.验证是否可用3.1进入CLI切换到/opt/phoenix/bin目录下 $ chmod 777 sqlline.py$ chmod 777 psql.py 运行 $ ./sqlline.py localhost 结果如下： 说明Phoenix安装成功 3.2在终端执行SQL脚本运行$ ./sqlline.py localhost ../examples/STOCK_SYMBOL.sql 报错： 解决： 将examples/STOCK_SYMBOL.sql里第一行-- creates stock table with single row删掉 重新运行： 3.3加载数据运行： $ psql.py localhost ../examples/web_stat.sql ../examples/web_stat.csv ../examples/web_stat_queries.sql 结果：","raw":null,"content":null,"categories":[{"name":"Phoenix","slug":"Phoenix","permalink":"http://linbingdong.com/categories/Phoenix/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://linbingdong.com/tags/大数据/"},{"name":"NoSQL","slug":"NoSQL","permalink":"http://linbingdong.com/tags/NoSQL/"},{"name":"Phoenix","slug":"Phoenix","permalink":"http://linbingdong.com/tags/Phoenix/"}]},{"title":"Phoenix综述（SQL on HBase）","slug":"Phoenix文档","date":"2017-03-11T12:33:42.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2017/03/11/Phoenix文档/","link":"","permalink":"http://linbingdong.com/2017/03/11/Phoenix文档/","excerpt":"1. Phoenix概述Phoenix最早是saleforce的一个开源项目，后来成为Apache基金的顶级项目。","text":"1. Phoenix概述Phoenix最早是saleforce的一个开源项目，后来成为Apache基金的顶级项目。 Phoenix是构建在HBase上的一个SQL层，能让我们用标准的JDBC APIs而不是HBase客户端APIs来创建表，插入数据和对HBase数据进行查询。 put the SQL back in NoSQL Phoenix完全使用Java编写，作为HBase内嵌的JDBC驱动。Phoenix查询引擎会将SQL查询转换为一个或多个HBase扫描，并编排执行以生成标准的JDBC结果集。直接使用HBase API、协同处理器与自定义过滤器，对于简单查询来说，其性能量级是毫秒，对于百万级别的行数来说，其性能量级是秒。 HBase的查询工具有很多，如：Hive、Tez、Impala、Spark SQL、Phoenix等。 Phoenix通过以下方式使我们可以少写代码，并且性能比我们自己写代码更好： 将SQL编译成原生的HBase scans。 确定scan关键字的最佳开始和结束 让scan并行执行 … 使用Phoenix的公司 2. 历史演进 3.0/4.0 release ARRAY Type. 支持标准的JDBC数组类型 Sequences. 支持 CREATE/DROP SEQUENCE, NEXT VALUE FOR, CURRENT VALUE FOR也实现了 Multi-tenancy. 同一张HBase物理表上，不同的租户可以创建相互独立的视图 Views. 同一张HBase物理表上可以创建不同的视图 3.1/4.1 release Apache Pig Loader . 通过pig来处理数据时支持pig加载器来利用Phoenix的性能 Derived Tables. 允许在一个FROM子句中使用SELECT子句来定义一张衍生表 Local Indexing. 后面介绍 Tracing. 后面介绍 3.2/4.2 release Subqueries 支持在WHERE和FROM子句中的独立子查询和相关子查询 Semi/anti joins. 通过标准的[NOT] IN 和 [NOT] EXISTS关键字来支持半/反连接 Optimize foreign key joins. 通过利用跳跃扫描过滤器来优化外键连接 Statistics Collection. 通过收集表的统计信息来提高并行查询能力 3.3/4.3 release Many-to-many joins. 支持两边都太大以至于无法放进内存的连接 Map-reduce Integration. 支持Map-reduce集成 Functional Indexes. 后面介绍 4.4 release User Defined Functions. 后面介绍 4.5 release Asynchronous Index Population. 通过一个Map-reduce job，索引可以被异步创建 4.6 release Time series Optimization. 优化针对时间序列数据的查询 4.7 release Transaction Support. 后面介绍 4.8 release DISTINCT Query Optimization. 使用搜索逻辑来大幅提高 SELECT DISTINCT 和 COUNT DISTINCT的查询性能 Local Index Improvements. Reworked 后面介绍 Hive Integration. 能够在Phoenix内使用Hive来支持大表和大表之间的连接 Namespace Mapping. 将Phoenix schema映射到HBase的命名空间来增强不同schema之间的隔离性 3. 特性3.1 Transactions (beta) 事务该特性还处于beta版，并非正式版。通过集成Tephra,Phoenix可以支持ACID特性。Tephra也是Apache的一个项目,是事务管理器，它在像HBase这样的分布式数据存储上提供全局一致事务。HBase本身在行层次和区层次上支持强一致性，Tephra额外提供交叉区、交叉表的一致性来支持可扩展性。 要想让Phoenix支持事务特性，需要以下步骤： 配置客户端hbase-site.xml &lt;property&gt; &lt;name&gt;phoenix.transactions.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 配置服务端hbase-site.xml &lt;property&gt; &lt;name&gt;data.tx.snapshot.dir&lt;/name&gt; &lt;value&gt;/tmp/tephra/snapshots&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;data.tx.timeout&lt;/name&gt; &lt;value&gt;60&lt;/value&gt; &lt;description&gt; set the transaction timeout (time after which open transactions become invalid) to a reasonable value.&lt;/description&gt;&lt;/property&gt; 配置$HBASE_HOME并启动Tephra ./bin/tephra 通过以上配置，Phoenix已经支持了事务特性，但创建表的时候默认还是不支持的。如果想创建一个表支持事务特性，需要显示声明，如下： CREATE TABLE my_table (k BIGINT PRIMARY KEY, v VARCHAR) TRANSACTIONAL=true; 就是在建表语句末尾增加 TRANSACTIONAL=true。 原本存在的表也可以更改成支持事务的，需要注意的是，事务表无法改回非事务的，因此更改的时候要小心。一旦改成事务的，就改不回去了。 ALTER TABLE my_other_table SET TRANSACTIONAL=true; 3.2 User-defined functions(UDFs) 用户定义函数3.2.1 概述Phoenix从4.4.0版本开始支持用户自定义函数。 用户可以创建临时或永久的用户自定义函数。这些用户自定义函数可以像内置的create、upsert、delete一样被调用。临时函数是针对特定的会话或连接，对其他会话或连接不可见。永久函数的元信息会被存储在一张叫做SYSTEM.FUNCTION的系统表中，对任何会话或连接均可见。 3.2.2 配置 hive-site.xml &lt;property&gt; &lt;name&gt;phoenix.functions.allowUserDefinedFunctions&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;fs.hdfs.impl&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.DistributedFileSystem&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;$&#123;hbase.tmp.dir&#125;/hbase&lt;/value&gt; &lt;description&gt;The directory shared by region servers and into which HBase persists. The URL should be &apos;fully-qualified&apos; to include the filesystem scheme. For example, to specify the HDFS directory &apos;/hbase&apos; where the HDFS instance&apos;s namenode is running at namenode.example.org on port 9000, set this value to: hdfs://namenode.example.org:9000/hbase. By default, we write to whatever $&#123;hbase.tmp.dir&#125; is set too -- usually /tmp -- so change this configuration or else all data will be lost on machine restart.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.dynamic.jars.dir&lt;/name&gt; &lt;value&gt;$&#123;hbase.rootdir&#125;/lib&lt;/value&gt; &lt;description&gt; The directory from which the custom udf jars can be loaded dynamically by the phoenix client/region server without the need to restart. However, an already loaded udf class would not be un-loaded. See HBASE-1936 for more details. &lt;/description&gt;&lt;/property&gt; 后两个配置需要跟hbse服务端的配置一致。 以上配置完后，在JDBC连接时还需要执行以下语句： Properties props = new Properties();props.setProperty(&quot;phoenix.functions.allowUserDefinedFunctions&quot;, &quot;true&quot;);Connection conn = DriverManager.getConnection(&quot;jdbc:phoenix:localhost&quot;, props); 以下是可选的配置，用于动态类加载的时候把jar包从hdfs拷贝到本地文件系统 &lt;property&gt; &lt;name&gt;hbase.local.dir&lt;/name&gt; &lt;value&gt;$&#123;hbase.tmp.dir&#125;/local/&lt;/value&gt; &lt;description&gt;Directory on the local filesystem to be used as a local storage.&lt;/description&gt;&lt;/property&gt; 3.3 Secondary Indexing 二级索引在HBase中，只有一个单一的按照字典序排序的rowKey索引，当使用rowKey来进行数据查询的时候速度较快，但是如果不使用rowKey来查询的话就会使用filter来对全表进行扫描，很大程度上降低了检索性能。而Phoenix提供了二级索引技术来应对这种使用rowKey之外的条件进行检索的场景。 Covered Indexes 只需要通过索引就能返回所要查询的数据，所以索引的列必须包含所需查询的列(SELECT的列和WHRER的列) Functional Indexes 从Phoeinx4.3以上就支持函数索引，其索引不局限于列，可以合适任意的表达式来创建索引，当在查询时用到了这些表达式时就直接返回表达式结果 Global Indexes Global indexing适用于多读少写的业务场景。使用Global indexing的话在写数据的时候会消耗大量开销，因为所有对数据表的更新操作（DELETE, UPSERT VALUES and UPSERT SELECT）,会引起索引表的更新，而索引表是分布在不同的数据节点上的，跨节点的数据传输带来了较大的性能消耗。在读数据的时候Phoenix会选择索引表来降低查询消耗的时间。在默认情况下如果想查询的字段不是索引字段的话索引表不会被使用，也就是说不会带来查询速度的提升。 Local Indexes Local indexing适用于写操作频繁的场景。与Global indexing一样，Phoenix会自动判定在进行查询的时候是否使用索引。使用Local indexing时，索引数据和数据表的数据是存放在相同的服务器中的避免了在写操作的时候往不同服务器的索引表中写索引带来的额外开销。使用Local indexing的时候即使查询的字段不是索引字段索引表也会被使用，这会带来查询速度的提升，这点跟Global indexing不同。一个数据表的所有索引数据都存储在一个单一的独立的可共享的表中。 3.4 Statistics Collection 统计信息收集 UPDATE STATISTICS可以更新某张表的统计信息，以提高查询性能 3.5 Row timestamp 时间戳 从4.6版本开始，Phoenix提供了一种将HBase原生的row timestamp映射到Phoenix列的方法。这样有利于充分利用HBase提供的针对存储文件的时间范围的各种优化，以及Phoenix内置的各种查询优化。 3.6 Paged Queries 分页查询 Phoenix支持分页查询： Row Value Constructors (RVC) OFFSET with limit 3.7 Salted Tables 散步表 如果row key是自动增长的，那么HBase的顺序写会导致region server产生数据热点的问题，Phoenix的Salted Tables技术可以解决region server的热点问题 3.8 Skip Scan 跳跃扫描 可以在范围扫描的时候提高性能 3.9 Views 视图 标准的SQL视图语法现在在Phoenix上也支持了。这使得能在同一张底层HBase物理表上创建多个虚拟表。 3.10 Multi tenancy 多租户 通过指定不同的租户连接实现数据访问的隔离 3.11 Dynamic Columns 动态列 Phoenix 1.2, specifying columns dynamically is now supported by allowing column definitions to included in parenthesis after the table in the FROM clause on a SELECT statement. Although this is not standard SQL, it is useful to surface this type of functionality to leverage the late binding ability of HBase. 3.12 Bulk CSV Data Loading 大量CSV数据加载 加载CSV数据到Phoenix表有两种方式：1. 通过psql命令以单线程的方式加载，数据量少的情况下适用。 2. 基于MapReduce的bulk load工具，适用于数据量大的情况 3.13 Query Server 查询服务器 Phoenix4.4引入的一个单独的服务器来提供thin客户端的连接 3.14 Tracing 追踪 从4.1版本开始Phoenix增加这个特性来追踪每条查询的踪迹，这使用户能够看到每一条查询或插入操作背后从客户端到HBase端执行的每一步。 3.15 Metrics 指标 Phoenix提供各种各样的指标使我们能够知道Phoenix客户端在执行不同SQL语句的时候其内部发生了什么。这些指标在客户端JVM中通过两种方式来收集： Request level metrics - collected at an individual SQL statementlevel Global metrics - collected at the client JVM level 4. 架构和组成 Phoenix架构 Phoenix在Hadoop生态系统中的位置 5. 数据存储 Phoenix将HBase的数据模型映射到关系型世界 6. 对QL的支持支持的命令如下： SELECT Example:SELECT * FROM TEST LIMIT 1000;SELECT * FROM TEST LIMIT 1000 OFFSET 100;SELECT full_name FROM SALES_PERSON WHERE ranking &gt;= 5.0 UNION ALL SELECT reviewer_name FROM CUSTOMER_REVIEW WHERE score &gt;= 8.0 UPSERT VALUES Example:UPSERT INTO TEST VALUES(&apos;foo&apos;,&apos;bar&apos;,3);UPSERT INTO TEST(NAME,ID) VALUES(&apos;foo&apos;,123); UPSERT SELECT Example:UPSERT INTO test.targetTable(col1, col2) SELECT col3, col4 FROM test.sourceTable WHERE col5 &lt; 100UPSERT INTO foo SELECT * FROM bar; DELETE Example:DELETE FROM TEST;DELETE FROM TEST WHERE ID=123;DELETE FROM TEST WHERE NAME LIKE &apos;foo%&apos;; CREATE TABLE CREATE TABLE my_schema.my_table ( id BIGINT not null primary key, date)CREATE TABLE my_table ( id INTEGER not null primary key desc, date DATE not null,m.db_utilization DECIMAL, i.db_utilization) m.DATA_BLOCK_ENCODING=&apos;DIFF&apos;CREATE TABLE stats.prod_metrics ( host char(50) not null, created_date date not null,txn_count bigint CONSTRAINT pk PRIMARY KEY (host, created_date) )CREATE TABLE IF NOT EXISTS &quot;my_case_sensitive_table&quot; ( &quot;id&quot; char(10) not null primary key, &quot;value&quot; integer) DATA_BLOCK_ENCODING=&apos;NONE&apos;,VERSIONS=5,MAX_FILESIZE=2000000 split on (?, ?, ?)CREATE TABLE IF NOT EXISTS my_schema.my_table (org_id CHAR(15), entity_id CHAR(15), payload binary(1000),CONSTRAINT pk PRIMARY KEY (org_id, entity_id) )TTL=86400 DROP TABLE Example:DROP TABLE my_schema.my_table;DROP TABLE IF EXISTS my_table;DROP TABLE my_schema.my_table CASCADE; CREATE FUNCTION Example:CREATE FUNCTION my_reverse(varchar) returns varchar as &apos;com.mypackage.MyReverseFunction&apos; using jar &apos;hdfs:/localhost:8080/hbase/lib/myjar.jar&apos;CREATE FUNCTION my_reverse(varchar) returns varchar as &apos;com.mypackage.MyReverseFunction&apos;CREATE FUNCTION my_increment(integer, integer constant defaultvalue=&apos;10&apos;) returns integer as &apos;com.mypackage.MyIncrementFunction&apos; using jar &apos;/hbase/lib/myincrement.jar&apos;CREATE TEMPORARY FUNCTION my_reverse(varchar) returns varchar as &apos;com.mypackage.MyReverseFunction&apos; using jar &apos;hdfs:/localhost:8080/hbase/lib/myjar.jar&apos; DROP FUNCTION Example:DROP FUNCTION IF EXISTS my_reverseDROP FUNCTION my_reverse CREATE VIEW Example:CREATE VIEW &quot;my_hbase_table&quot;( k VARCHAR primary key, &quot;v&quot; UNSIGNED_LONG) default_column_family=&apos;a&apos;;CREATE VIEW my_view ( new_col SMALLINT ) AS SELECT * FROM my_table WHERE k = 100;CREATE VIEW my_view_on_view AS SELECT * FROM my_view WHERE new_col &gt; 70; DROP VIEW Example:DROP VIEW my_viewDROP VIEW IF EXISTS my_schema.my_viewDROP VIEW IF EXISTS my_schema.my_view CASCADE CREATE SEQUENCE Example:CREATE SEQUENCE my_sequence;CREATE SEQUENCE my_sequence START WITH -1000CREATE SEQUENCE my_sequence INCREMENT BY 10CREATE SEQUENCE my_schema.my_sequence START 0 CACHE 10 DROP SEQUENCE Example:DROP SEQUENCE my_sequenceDROP SEQUENCE IF EXISTS my_schema.my_sequence ALTER Example:ALTER TABLE my_schema.my_table ADD d.dept_id char(10) VERSIONS=10ALTER TABLE my_table ADD dept_name char(50), parent_id char(15) null primary keyALTER TABLE my_table DROP COLUMN d.dept_id, parent_id;ALTER VIEW my_view DROP COLUMN new_col;ALTER TABLE my_table SET IMMUTABLE_ROWS=true,DISABLE_WAL=true; CREATE INDEX Example:CREATE INDEX my_idx ON sales.opportunity(last_updated_date DESC)CREATE INDEX my_idx ON log.event(created_date DESC) INCLUDE (name, payload) SALT_BUCKETS=10CREATE INDEX IF NOT EXISTS my_comp_idx ON server_metrics ( gc_time DESC, created_date DESC ) DATA_BLOCK_ENCODING=&apos;NONE&apos;,VERSIONS=?,MAX_FILESIZE=2000000 split on (?, ?, ?)CREATE INDEX my_idx ON sales.opportunity(UPPER(contact_name)) DROP INDEX Example:DROP INDEX my_idx ON sales.opportunityDROP INDEX IF EXISTS my_idx ON server_metrics ALTER INDEX Example:ALTER INDEX my_idx ON sales.opportunity DISABLEALTER INDEX IF EXISTS my_idx ON server_metrics REBUILD EXPLAIN Example:EXPLAIN SELECT NAME, COUNT(*) FROM TEST GROUP BY NAME HAVING COUNT(*) &gt; 2;EXPLAIN SELECT entity_id FROM CORE.CUSTOM_ENTITY_DATA WHERE organization_id=&apos;00D300000000XHP&apos; AND SUBSTR(entity_id,1,3) = &apos;002&apos; AND created_date &lt; CURRENT_DATE()-1; UPDATE STATISTICS Example:UPDATE STATISTICS my_tableUPDATE STATISTICS my_schema.my_table INDEXUPDATE STATISTICS my_indexUPDATE STATISTICS my_table COLUMNSUPDATE STATISTICS my_table SET phoenix.stats.guidepost.width=50000000 CREATE SCHEMA Example:CREATE SCHEMA IF NOT EXISTS my_schemaCREATE SCHEMA my_schema USE Example:USE my_schemaUSE DEFAULT DROP SCHEMA Example:DROP SCHEMA IF EXISTS my_schemaDROP SCHEMA my_schema 7. 安装部署7.1 安装预编译的Phoenix 下载并解压最新版的phoenix-[version]-bin.tar包 将phoenix-[version]-server.jar放入服务端和master节点的HBase的lib目录下 重启HBase 将phoenix-[version]-client.jar添加到所有Phoenix客户端的classpath 7.2 使用Phoenix7.2.1 命令行若要在命令行执行交互式SQL语句： 1.切换到bin目录 2.执行以下语句 $ sqlline.py localhost 若要在命令行执行SQL脚本 $ sqlline.py localhost ../examples/stock_symbol.sql 7.2.2 客户端 SQuirrel是用来连接Phoenix的客户端。 SQuirrel安装步骤如下： 1. Remove prior phoenix-[*oldversion*]-client.jar from the lib directory of SQuirrel, copy phoenix-[*newversion*]-client.jar to the lib directory (*newversion* should be compatible with the version of the phoenix server jar used with your HBase installation)2. Start SQuirrel and add new driver to SQuirrel (Drivers -&gt; New Driver)3. In Add Driver dialog box, set Name to Phoenix, and set the Example URL to jdbc:phoenix:localhost.4. Type “org.apache.phoenix.jdbc.PhoenixDriver” into the Class Name textbox and click OK to close this dialog.5. Switch to Alias tab and create the new Alias (Aliases -&gt; New Aliases)6. In the dialog box, Name: *any name*, Driver: Phoenix, User Name: *anything*, Password: *anything*7. Construct URL as follows: jdbc:phoenix: *zookeeper quorum server*. For example, to connect to a local HBase use: jdbc:phoenix:localhost8. Press Test (which should succeed if everything is setup correctly) and press OK to close.9. Now double click on your newly created Phoenix alias and click Connect. Now you are ready to run SQL queries against Phoenix. 8. 测试8.1 PherfPherf是可以通过Phoenix来进行性能和功能测试的工具。Pherf可以用来生成高度定制的数据集，并且测试SQL在这些数据集上的性能。 8.1.1 构建PherfPherf是在用maven构建Phoenix的过程中同时构建的。可以用两种不同的配置来构建： 集群（默认） This profile builds Pherf such that it can run along side an existing cluster. The dependencies are pulled from the HBase classpath. 独立 This profile builds all of Pherf’s dependencies into a single standalone jar. The deps will be pulled from the versions specified in Phoenix’s pom. 构建全部的Phoenix。包含Pherf的默认配置。 mvn clean package -DskipTests 用Pherf的独立配置来构建Phoenix。 mvn clean package -P standalone -DskipTests 8.1.2 安装用以上的Maven命令构建完Pherf后，会在该模块的目标目录下生成一个zip文件。 将该zip文件解压到合适的目录 配置env.sh文件 ./pherf.sh -h 想要在一个真正的集群上测试，运行如下命令: ./pherf.sh -drop all -l -q -z localhost -schemaFile .*user_defined_schema.sql -scenarioFile .*user_defined_scenario.xml 8.1.3 命令示例 列出所有可运行的场景文件 $./pherf.sh -listFiles 删掉全部场景文件中存在的特定的表、加载和查询数据 $./pherf.sh -drop all -l -q -z localhost 8.1.4 参数 -h Help-l Apply schema and load data-q Executes Multi-threaded query sets and write results-z [quorum] Zookeeper quorum-m Enable monitor for statistics-monitorFrequency [frequency in Ms] _Frequency at which the monitor will snopshot stats to log file.-drop [pattern] Regex drop all tables with schema name as PHERF. Example drop Event tables: -drop .(EVENT). Drop all: -drop . or -drop all-scenarioFile Regex or file name of a specific scenario file to run.-schemaFile Regex or file name of a specific schema file to run.-export Exports query results to CSV files in CSV_EXPORT directory-diff Compares results with previously exported results-hint Executes all queries with specified hint. Example SMALL-rowCountOverride-rowCountOverride [number of rows] Specify number of rows to be upserted rather than using row count specified in schema 8.1.5 为数据生成增加规则8.1.6 定义场景8.1.7 结果结果实时写入结果目录中。可以打开.jpg格式文件来实时可视化。 8.1.8 测试 Run unit tests: mvn test -DZK_QUORUM=localhostRun a specific method: mvn -Dtest=ClassName#methodName testMore to come… 8.2 性能Phoenix通过以下方法来奉行把计算带到离数据近的地方的哲学： 协处理器在服务端执行操作来最小化服务端和客户端的数据传输 定制的过滤器为了删减数据使之尽可能地靠近源数据并最小化启动代价，Phoenix使用原生的HBase APIs而不是使用Map/Reduce框架 8.2.1 Phoenix对比相近产品8.2.1.1 Phoenix vs Hive (running over HDFS and HBase) Query: select count(1) from table over 10M and 100M rows. Data is 5 narrow columns. Number of Region Servers: 4 (HBase heap: 10GB, Processor: 6 cores @ 3.3GHz Xeon) 8.2.1.2 Phoenix vs Impala (running over HBase) Query: select count(1) from table over 1M and 5M rows. Data is 3 narrow columns. Number of Region Server: 1 (Virtual Machine, HBase heap: 2GB, Processor: 2 cores @ 3.3GHz Xeon) 8.2.2 Latest Automated Performance RunLatest Automated Performance Run | Automated Performance Runs History 8.2.3 Phoenix1.2性能提升 Essential Column Family Skip Scan Salting Top-N 9. 参考资料 http://phoenix.apache.org http://phoenix.apache.org/Phoenix-in-15-minutes-or-less.html http://hadooptutorial.info/apache-phoenix-hbase-an-sql-layer-on-hbase/ http://www.phoenixframework.org/docs/resources https://en.wikipedia.org/wiki/Apache_Phoenix","raw":null,"content":null,"categories":[{"name":"Phoenix","slug":"Phoenix","permalink":"http://linbingdong.com/categories/Phoenix/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://linbingdong.com/tags/大数据/"},{"name":"NoSQL","slug":"NoSQL","permalink":"http://linbingdong.com/tags/NoSQL/"},{"name":"Phoenix","slug":"Phoenix","permalink":"http://linbingdong.com/tags/Phoenix/"}]},{"title":"Phoenix查询经验总结","slug":"Phoenix查询测试经验总结","date":"2017-03-11T12:33:42.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2017/03/11/Phoenix查询测试经验总结/","link":"","permalink":"http://linbingdong.com/2017/03/11/Phoenix查询测试经验总结/","excerpt":"适当的索引能够让极大提升查询速度，因此在Phoenix查询的测试用例中包括了对有索引跟无索引的查询性能的比较。测试过程中遇到一些问题，经过探索，得到一些结论，在此记录下来。","text":"适当的索引能够让极大提升查询速度，因此在Phoenix查询的测试用例中包括了对有索引跟无索引的查询性能的比较。测试过程中遇到一些问题，经过探索，得到一些结论，在此记录下来。 1. 背景适当的索引能够让极大提升查询速度，因此在Phoenix查询的测试用例中包括了对有索引跟无索引的查询性能的比较。测试过程中遇到一些问题，在此记录下来。 2. 问题及解决2.1. 创建索引时报错，报错如下：//创建索引语句：0: jdbc:phoenix:localhost&gt; CREATE INDEX ind_1 ON TESTINPUT(ff1);//报错：Error: ERROR 1029 (42Y88): Mutable secondary indexes must have the hbase.regionserver.wal.codec property set to org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec in the hbase-sites.xml of every region server tableName=IND_1 (state=42Y88,code=1029)java.sql.SQLException: ERROR 1029 (42Y88): Mutable secondary indexes must have the hbase.regionserver.wal.codec property set to org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec in the hbase-sites.xml of every region server tableName=IND_1 at org.apache.phoenix.exception.SQLExceptionCode$Factory$1.newException(SQLExceptionCode.java:396) at org.apache.phoenix.exception.SQLExceptionInfo.buildException(SQLExceptionInfo.java:145) at org.apache.phoenix.schema.MetaDataClient.createIndex(MetaDataClient.java:1162) at org.apache.phoenix.compile.CreateIndexCompiler$1.execute(CreateIndexCompiler.java:95) at org.apache.phoenix.jdbc.PhoenixStatement$2.call(PhoenixStatement.java:322) at org.apache.phoenix.jdbc.PhoenixStatement$2.call(PhoenixStatement.java:314) at org.apache.phoenix.call.CallRunner.run(CallRunner.java:53) at org.apache.phoenix.jdbc.PhoenixStatement.executeMutation(PhoenixStatement.java:312) at org.apache.phoenix.jdbc.PhoenixStatement.execute(PhoenixStatement.java:1435) at sqlline.Commands.execute(Commands.java:822) at sqlline.Commands.sql(Commands.java:732) at sqlline.SqlLine.dispatch(SqlLine.java:808) at sqlline.SqlLine.begin(SqlLine.java:681) at sqlline.SqlLine.start(SqlLine.java:398) at sqlline.SqlLine.main(SqlLine.java:292) 原因：Phoenix支持两种索引：可变索引跟不可变索引。在可变表上建的索引是可变索引，在不可变表上建的索引是不可变索引。可变索引是指插入或删除数据的时候会同时更新索引；不可变索引适用于只写入一次不再更改的表，索引只建立一次，再插入数据不会更新索引。上面使用的语句是创建可变索引，需要在hbase-site.xml中进行相关配置使其支持可变索引（不可变索引无需另外配置，默认支持）。 解决：对HMaster和HRegionserver节点分别增加配置,然后重启HBase集群 HMaster &lt;property&gt; &lt;name&gt;hbase.regionserver.wal.codec&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.master.loadbalancer.class&lt;/name&gt; &lt;value&gt;org.apache.phoenix.hbase.index.balancer.IndexLoadBalancer&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.coprocessor.master.classes&lt;/name&gt; &lt;value&gt;org.apache.phoenix.hbase.index.master.IndexMasterObserver&lt;/value&gt;&lt;/property&gt; HRegionserver &lt;property&gt; &lt;name&gt;hbase.regionserver.wal.codec&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.region.server.rpc.scheduler.factory.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hbase.ipc.PhoenixRpcSchedulerFactory&lt;/value&gt; &lt;description&gt;Factory to create the Phoenix RPC Scheduler that usesseparate queues for index and metadata updates&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.rpc.controllerfactory.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hbase.ipc.controller.ServerRpcControllerFactory&lt;/value&gt; &lt;description&gt;Factory to create the Phoenix RPCScheduler that uses separate queues for index and metadataupdates&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.coprocessor.regionserver.classes&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hbase.regionserver.LocalIndexMerger&lt;/value&gt;&lt;/property&gt; 2.2. 对10亿数据查询时，报错如下：16/11/29 10:33:50 WARN client.ScannerCallable: Ignore, probably already closedorg.apache.hadoop.hbase.regionserver.LeaseException: org.apache.hadoop.hbase.regionserver.LeaseException: lease &apos;1132&apos; does not exist at org.apache.hadoop.hbase.regionserver.Leases.removeLease(Leases.java:221) at org.apache.hadoop.hbase.regionserver.Leases.cancelLease(Leases.java:206)...org.apache.phoenix.exception.PhoenixIOException: org.apache.phoenix.exception.PhoenixIOException: Failed after attempts=36, exceptions:Tue Nov 29 10:33:50 CST 2016, null, java.net.SocketTimeoutException: callTimeout=60000, callDuration=60321: row &apos;��s,d&apos; on table &apos;TEST11&apos; at region=TEST11,\\x11\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00,1479985615575.c3adb68acea8d88d223bffd3acc16c2e., hostname=node-20-105,60020,1480385981798, seqNum=1244662...Caused by: org.apache.hadoop.hbase.ipc.CallTimeoutException: Call id=18173, waitTime=60001, operationTimeout=60000 expired. at org.apache.hadoop.hbase.ipc.Call.checkAndSetTimeout(Call.java:70) at org.apache.hadoop.hbase.ipc.RpcClientImpl.call(RpcClientImpl.java:1197)... 原因： 某些查询需要很长时间才能返回结果，被HBase的超时机制杀掉了。 思路： 增大超时时间，在hbase-site.xml里增加了如下配置： &lt;property&gt; &lt;name&gt;hbase.rpc.timeout&lt;/name&gt; &lt;value&gt;600000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.client.operation.timeout&lt;/name&gt; &lt;value&gt;600000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.client.scanner.timeout.period&lt;/name&gt; &lt;value&gt;600000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.regionserver.lease.period&lt;/name&gt; &lt;value&gt;600000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;phoenix.query.timeoutMs&lt;/name&gt; &lt;value&gt;600000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;phoenix.query.keepAliveMs&lt;/name&gt; &lt;value&gt;600000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.client.ipc.pool.type&lt;/name&gt; &lt;value&gt;RoundRobinPool&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.client.ipc.pool.size&lt;/name&gt; &lt;value&gt;10&lt;/value&gt;&lt;/property&gt; 最终虽然配置生效了，但是还是报同样的错。已经将网上说的可能的配置项都配了还是无法解决超时问题。等增加了机器，查询时间变短，10亿数据的查询应该就没有超时问题了。 3. 特性 不可变索引默认支持，不需要另外配置；可变索引需要如上添加配置才能支持 创建不可变表： CREATE TABLE TABLENAME (pk long PRIMARY KEY,col1 int) IMMUTABLE_ROWS=true; 创建索引有以下几种方式： CREATE INDEX ind_name ON TABLENAME(COLUMN1);CREATE INDEX ind_name ON TABLENAME(COLUMN1,COLUMN2);CREATE INDEX ind_name ON TABLENAME(COLUMN1) INCLUDE(COLUMN2); 执行查询的时候，Phoenix查询优化器将选择合适的索引。可以使用explain plan进行查看 0: jdbc:phoenix:localhost&gt; explain select ff3,if1 from testinput where ff3 &gt;= 0.7 and ff3 &lt; 0.9 order by if1;+------------------------------------------+| PLAN |+------------------------------------------+| CLIENT 1-CHUNK PARALLEL 1-WAY RANGE SCAN OVER IND_4 [0.7] - [0.9] || SERVER SORTED BY [&quot;IF1&quot;] || CLIENT MERGE SORT |+------------------------------------------+ 除非所有查询使用的列被索引或者覆盖列，否则二级索引不会被使用 建索引的时候不要包括primary key，否则索引不会被使用；可以单独对primary key建索引 where条件里有primary key的时候会使用Range Scan，因为表本来就是按照primary key的顺序排列的 primary key在插入时是自动排序的，插入完成后primary key保持有序（如果该表只有一个分区，则全局有序；如果有多个分区，则在每个分区内部有序，并非全局有序） 对某几个（1个或多个）列建索引，则会生成一张索引表，该表由创建索引的这几个列组成，并在最后一列添加primary key列。也就是说索引表也是一张表，只不过该表列数比原表少。 索引表的第一列是有序的 upsert into一个跟之前一样的primary key，会将之前那个primary key的记录替换成新的。 phoenix虽然不支持update语句，但是可以用upsert into tablename(id,columnname) values(id,newvalue)来实现同样的功能。 local index 对应的索引表的分区跟表的分区在同一个region server上（索引表分区数必须跟表分区数一样） global index 对应的索引表的分区跟表的分区不一定在同一个region server上（索引表分区数必须跟表分区数一样） 对一张表建了多个local index，对于HBase来讲，其实只存了一张索引表。但是global index则不同。 4. 参考资料 https://github.com/forcedotcom/phoenix/wiki/Secondary-Indexing http://phoenix.apache.org/language/index.html#create_index http://blog.csdn.net/jiangshouzhuang/article/details/52387718","raw":null,"content":null,"categories":[{"name":"Phoenix","slug":"Phoenix","permalink":"http://linbingdong.com/categories/Phoenix/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://linbingdong.com/tags/大数据/"},{"name":"NoSQL","slug":"NoSQL","permalink":"http://linbingdong.com/tags/NoSQL/"},{"name":"Phoenix","slug":"Phoenix","permalink":"http://linbingdong.com/tags/Phoenix/"}]},{"title":"Phoenix表和索引分区数对插入和查询性能的影响","slug":"Phoenix表和索引分区数对插入和查询性能的影响","date":"2017-03-11T12:33:42.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2017/03/11/Phoenix表和索引分区数对插入和查询性能的影响/","link":"","permalink":"http://linbingdong.com/2017/03/11/Phoenix表和索引分区数对插入和查询性能的影响/","excerpt":"Phoenix在建表和建索引的时候可以指定SALT_BUCKETS数，即分区数，从而提高插入和查询性能。\n通过指定分区，可以将对一张表的操作分配给多个Region Server进行处理，从而提高效率。 ","text":"Phoenix在建表和建索引的时候可以指定SALT_BUCKETS数，即分区数，从而提高插入和查询性能。 通过指定分区，可以将对一张表的操作分配给多个Region Server进行处理，从而提高效率。 1. 概述1.1 HBase概述HBase由master节点和region server节点组成。在100-105集群上，100和101是master节点，102-105是region server节点。 每个region server管理很多region，每个region只会属于一个region server。一个region的大小可以自己配置（100-105集群上一个region大小为100GB）。 在HBase中创建一张表时，刚开始默认是一个region，若表越来越大，超过一个region的大小，则会split成两个region。当然也可以在建表的时候预分区几个region，如果表的大小超过了预分区的region的大小也会split。 1.2 Phoenix分区Phoenix在建表和建索引的时候可以指定SALT_BUCKETS数，即分区数，从而提高插入和查询性能。方式如下： CREATE TABLE IF NOT EXISTS test1 (pk BIGINT PRIMARY KEY,ff1 DOUBLE,sf1 VARCHAR,if1 INTEGER) SALT_BUCKETS=20; 通过指定分区，可以将对一张表的操作分配给多个Region Server进行处理，从而提高效率。 但是官网上并没有对分区数应设为多少比较合适给出建议，网上有博客说应该指定为region server总CPU核数的0.5~1倍之间。测试集群region sever每个节点40核，4个节点共160核。 1.3 目的测试并确定分区数对Phoenix插入和查询性能的影响，确定当前集群在表分区和索引分区数分别为多少的情况下性能最优，接下来Phoenix性能测试采用该最优方案。 2. 方案2.1 方案设计创建多张表，除了表和索引的分区数不同外其他条件均相同，分别执行插入和查询操作，对比插入和查询时间。 相同的条件有： 用90*2线程执行插入 用1个线程进行查询 每轮查询执行42条查询语句，重复10次 表的总记录数为1000万 表的字段数和每个字段的类型 每个表都建3个global index 示例： CREATE TABLE IF NOT EXISTS test4 (pk BIGINT PRIMARY KEY,ff1 DOUBLE,ff2 DOUBLE,sf1 VARCHAR,if1 INTEGER,if2 INTEGER,if3 INTEGER,if4 INTEGER,if5 INTEGER,if6 INTEGER) SALT_BUCKETS=40;CREATE INDEX ind_l4 ON test4(pk) SALT_BUCKETS=20;CREATE INDEX ind_l4_1 ON test4(ff1,ff2,sf1) SALT_BUCKETS=20;CREATE INDEX ind_l4_2 ON test4(if1) SALT_BUCKETS=20; 注：若只指定了表的分区数，未指定索引的分区数，则默认索引的分区数跟表分区数一样。 2.2 测试用例 用例编号 表分区数 索引分区数 插入时间 查询时间 T1 0 0 T2 12 12 T3 20 1 T4 20 20 T5 40 20 T6 40 40 T7 60 60 T8 80 12 T9 80 40 T10 80 80 T11 120 40 T12 120 120 3. 结果3.1 软硬件环境 Master服务器 IP：192.168.20.100,192.168.20.101 硬件： CPU：Intel E5-2670v3 * 2（2.3GHz、L3 Cache 25M）vCore:40 内存：16G DDR4 * 16 2133 MHz Cache 28G 硬盘：SAS硬盘 2（300G、2.5吋、10K） SAS硬盘 12（3T、2.5吋、7200转） 网卡： 曙光万兆双口光纤（含光模块） * 1 软件： Centos 7 jdk-7u65-linux-x64 Phoenix 4.6 HBase 1.0.0 Region服务器 IP：192.168.20.102, 192.168.20.103, 192.168.20.104, 192.168.20.105 硬件： CPU：Intel E5-2670v3 * 2（2.3GHz、L3 Cache 25M）vCore:40 内存：16G DDR4 * 16 2133 MHz Cache 28G 硬盘：SAS硬盘 2（300G、2.5吋、10K） SAS硬盘 12（3T、2.5吋、7200转） 网卡： 曙光万兆双口光纤（含光模块） * 1 软件： Centos 7 jdk-7u65-linux-x64 HBase 1.0.0 客户端服务器 IP：192.168.20.100,192.168.20.102 硬件： CPU：Intel E5-2670v3 * 2（2.3GHz、L3 Cache 25M）vCore:40 内存：16G DDR4 * 16 2133 MHz Cache 28G 硬盘：SAS硬盘 2（300G、2.5吋、10K） SAS硬盘 12（3T、2.5吋、7200转） 网卡： 曙光万兆双口光纤（含光模块） * 1 软件： Centos 7 jdk-7u65-linux-x64 HBase 1.0.0 Phoenix客户端 IP：192.168.20.101 硬件： CPU：Intel E5-2670v3 * 2（2.3GHz、L3 Cache 25M）vCore:40 内存：16G DDR4 * 16 2133 MHz Cache 28G 硬盘：SAS硬盘 2（300G、2.5吋、10K） SAS硬盘 12（3T、2.5吋、7200转） 网卡： 曙光万兆双口光纤（含光模块） * 1 软件： Centos 7 jdk-7u65-linux-x64 Phoenix 4.6 HBase 1.0.0 3.2 结果 用例编号 表分区数 索引分区数 插入时间 查询时间 T1 0 0 779 4490 T2 12 12 303 735 T3 20 1 203 1200 T4 20 20 319 697 T5 40 20 380 592 T6 40 40 369 531 T7 60 60 441 587 T8 80 12 384 714 T9 80 40 522 653 T10 80 80 478 623 T11 120 40 512 764 T12 120 120 526 753 插入时间和查询时间单位均为秒。查询时间为执行42个查询每个查询10次的总时间。 4. 分析通过观察3.2结果中的数据，可得出以下结论： 对比T1和其他可知，有分区相对无分区在插入和查询上都有极大的性能提升。 对比T3和T4,T8和T9可知，在一定范围内，增大索引分区数使插入变慢，查询变快。 对比T11和T12可知，当索引分区达到一定大小后，再增加分区数已经无法提升查询性能。 综合比较来看，当前集群在表分区数和索引分区数均为40时插入和查询的综合性能最好。 因此，Phoenix的性能测试中将采用表分区数和索引分区数均为40的方案。","raw":null,"content":null,"categories":[{"name":"Phoenix","slug":"Phoenix","permalink":"http://linbingdong.com/categories/Phoenix/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://linbingdong.com/tags/大数据/"},{"name":"NoSQL","slug":"NoSQL","permalink":"http://linbingdong.com/tags/NoSQL/"},{"name":"Phoenix","slug":"Phoenix","permalink":"http://linbingdong.com/tags/Phoenix/"}]},{"title":"PostgreSQL单机部署（CentOS7）","slug":"PostgreSQL单机部署（CentOS7）","date":"2017-03-11T12:33:42.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2017/03/11/PostgreSQL单机部署（CentOS7）/","link":"","permalink":"http://linbingdong.com/2017/03/11/PostgreSQL单机部署（CentOS7）/","excerpt":"本文介绍Contos7上用yum部署PostgreSQL9.4的过程。","text":"本文介绍Contos7上用yum部署PostgreSQL9.4的过程。 1. 安装PostgreSQL源rpm -Uvh http://yum.postgresql.org/9.4/redhat/rhel-7-x86_64/pgdg-centos94-9.4-1.noarch.rpm 2. 执行安装命令yum updateyum install postgresql94-server postgresql94-contrib 3. 验证是否安装成功执行： rpm -qa | grep postgres 结果： postgresql94-9.4.10-1PGDG.rhel7.x86_64postgresql94-server-9.4.10-1PGDG.rhel7.x86_64postgresql94-libs-9.4.10-1PGDG.rhel7.x86_64postgresql94-contrib-9.4.10-1PGDG.rhel7.x86_64 说明安装成功 4. 初始化数据库先创建数据存放目录： mkdir -p /opt/pgsql/data 赋予postgres用户该目录的权限： chown postgres /opt/pgsql/data 切换到postgres用户： su postgres 执行初始化： initdb -D /opt/pgsql/data 注： -D 后面是数据库文件存放的目录，如果不指定则默认在/var/lib/pgsql/9.4/data下 初始化的日志如下： 属于此数据库系统的文件宿主为用户 &quot;postgres&quot;.此用户也必须为服务器进程的宿主.数据库簇将使用本地化语言 &quot;zh_CN.UTF-8&quot;进行初始化.默认的数据库编码已经相应的设置为 &quot;UTF8&quot;.initdb: 无法为本地化语言环境&quot;zh_CN.UTF-8&quot;找到合适的文本搜索配置缺省的文本搜索配置将会被设置到&quot;simple&quot;禁止为数据页生成校验和.修复已存在目录 /opt/pgsql/data 的权限 ... 成功正在创建子目录 ... 成功选择默认最大联接数 (max_connections) ... 100选择默认共享缓冲区大小 (shared_buffers) ... 128MB选择动态共享内存实现 ......posix创建配置文件 ... 成功在 /opt/pgsql/data/base/1 中创建 template1 数据库 ... 成功初始化 pg_authid ... 成功初始化dependencies ... 成功创建系统视图 ... 成功正在加载系统对象描述 ...成功创建(字符集)校对规则 ... 成功创建字符集转换 ... 成功正在创建字典 ... 成功对内建对象设置权限 ... 成功创建信息模式 ... 成功正在装载PL/pgSQL服务器端编程语言...成功清理数据库 template1 ... 成功拷贝 template1 到 template0 ... 成功拷贝 template1 到 template0 ... 成功同步数据到磁盘...成功成功. 您现在可以用下面的命令运行数据库服务器: /usr/pgsql-9.4/bin/postmaster -D /opt/pgsql/data/或者 /usr/pgsql-9.4/bin/pg_ctl -D /opt/pgsql/data/ -l logfile start 5. 启动服务1.切换到postgres用户 su postgres 这个步骤同样必须以PostgreSQL用户帐户登录来做。 2.启动服务 没有-D选项，服务器将使用环境变量PGDATA命名的目录； 如果这个环境变量也没有，将导致失败。通常，最好在后台启动postgres，使用下面的 Unix shell 语法： pg_ctl -D /opt/pgsql/data/ -l logfile start 3.设置开机自动启动 在Linux系统里，要么往/etc/rc.d/rc.local或 /etc/rc.local文件里加上下面几行： /usr/local/pgsql/bin/pg_ctl start -l logfile -D /usr/local/pgsql/data 6. 创建用户PostgreSQL使用角色的概念管理数据库访问权限。 根据角色自身的设置不同，一个角色可以看做是一个数据库用户，或者一组数据库用户。 角色可以拥有数据库对象(比如表)以及可以把这些对象上的权限赋予其它角色， 以控制谁拥有访问哪些对象的权限。另外，我们也可以把一个角色的成员 权限赋予其它角色，这样就允许成员角色使用分配给另一个角色的权限。角色的概念替换了”用户”和”组”。在PostgreSQL 版本 8.1 之前，用户和组是独立类型的记录，但现在它们只是角色。 任何角色都可以是一个用户、一个组、或者两者。 数据库角色从概念上与操作系统用户是完全无关的。在实际使用中把它们对应起来可能比较方便， 但这不是必须的。数据库角色在整个数据库集群中是全局的(而不是每个库不同)。 要创建一个角色，使用 SQL 命令CREATE ROLE执行： CREATE ROLE name; name遵循 SQL 标识的规则：要么完全没有特殊字符， 要么用双引号包围(实际上你通常会给命令增加额外的选项，比如LOGIN。 下面显示更多细节)。要删除一个现有角色，使用类似的DROP ROLE命令： DROP ROLE name; 为了方便，程序createuser和dropuser 提供了对了这些 SQL 命令的封装。我们可以在 shell 命令上直接调用它们： 直接在shell里输入： createuser lbd; 这样就创建了lbd这个角色。 dropuser lbd; 这样就创建了lbd这个角色。 要检查现有角色的集合，可以检查pg_roles系统表，比如： SELECT rolname FROM pg_roles; 结果如下： postgres=# SELECT rolname FROM pg_roles; rolname---------- postgres lbd(2 行记录) psql的元命令\\du也可以用于列出现有角色。 结果如下： postgres=# \\du 角色列表 角色名称 | 属性 | 成员属于---------+----------------------------------+---------- lbd | | &#123;&#125; postgres| 超级用户, 建立角色, 建立 DB, 复制 | &#123;&#125;","raw":null,"content":null,"categories":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"http://linbingdong.com/categories/PostgreSQL/"}],"tags":[{"name":"关系数据库","slug":"关系数据库","permalink":"http://linbingdong.com/tags/关系数据库/"},{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"http://linbingdong.com/tags/PostgreSQL/"}]},{"title":"PostgreSQL主从流复制部署","slug":"PostgreSQL主从流复制部署","date":"2017-03-11T12:33:42.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2017/03/11/PostgreSQL主从流复制部署/","link":"","permalink":"http://linbingdong.com/2017/03/11/PostgreSQL主从流复制部署/","excerpt":"PostgreSQL在9.0之后引入了主从的流复制机制，所谓流复制，就是从服务器通过tcp流从主服务器中同步相应的数据。这样当主服务器数据丢失时从服务器中仍有备份。","text":"PostgreSQL在9.0之后引入了主从的流复制机制，所谓流复制，就是从服务器通过tcp流从主服务器中同步相应的数据。这样当主服务器数据丢失时从服务器中仍有备份。 192.168.20.93上部署主服务器，192.168.20.94上部署从服务器。 1. 简介PostgreSQL在9.0之后引入了主从的流复制机制，所谓流复制，就是从服务器通过tcp流从主服务器中同步相应的数据。这样当主服务器数据丢失时从服务器中仍有备份。 与基于文件日志传送相比，流复制允许保持从服务器更新。 从服务器连接主服务器，其产生的流WAL记录到从服务器， 而不需要等待主服务器写完WAL文件。 PostgreSQL流复制默认是异步的。在主服务器上提交事务和从服务器上变化可见之间有一个小的延迟，这个延迟远小于基于文件日志传送，通常1秒能完成。如果主服务器突然崩溃，可能会有少量数据丢失。 同步复制必须等主服务器和从服务器都写完WAL后才能提交事务。这样在一定程度上会增加事务的响应时间。 配置同步复制仅需要一个额外的配置步骤： synchronous_standby_names必须设置为一个非空值。synchronous_commit也必须设置为on。 这里部署的是异步的流复制。 注:主从服务器所在节点的系统、环境\u0001等最好一致。PostgreSQL版本也最好一致，否则可能会有问题。 2. 安装部署先在192.168.20.93和192.168.20.94均安装PostgreSQL。 具体安装部署步骤见：PostgreSQL单机部署（CentOS7） 2.1 主服务器主服务器为192.168.20.93 先创建一个新目录： mkdir /opt/pgsql/pg_archive 1.首先需要创建一个数据库用户进行主从同步。创建用户replica，并赋予登录和复制的权限。 postgres# CREATE ROLE replica login replication encrypted password &apos;replica&apos; 2.修改pg_hba.conf，允许replica用户来同步。 在pg_hba.conf里增加两行： host all all 192.168.20.94/32 trust #允许94连接到主服务器host replication replica 192.168.20.94/32 md5 #允许94使用replica用户来复制 这样，就设置了replica这个用户可以从192.168.20.93进行流复制请求。 注：第二个字段必须要填replication 4.修改postgresql.conf listen_addresses = &apos;*&apos; # 监听所有IParchive_mode = on # 允许归档archive_command = &apos;cp %p /opt/pgsql/pg_archive/%f&apos; # 用该命令来归档logfile segmentwal_level = hot_standby max_wal_senders = 32 # 这个设置了可以最多有几个流复制连接，差不多有几个从，就设置几个wal_keep_segments = 256 ＃ 设置流复制保留的最多的xlog数目wal_sender_timeout = 60s ＃ 设置流复制主机发送数据的超时时间max_connections = 100 # 这个设置要注意下，从库的max_connections必须要大于主库的 配置完两个文件后重启服务器。 pg_ctl stop -D /opt/pgsql/datapg_ctl start -D /opt/pgsql/data 3.测试94能否连接93数据库。在94上运行如下命令： psql -h 192.168.20.93 -U postgres 看看是否能进入数据库。若可以，则正常。 2.2 从服务器1.从主节点拷贝数据到从节点 su - postgresrm -rf /opt/pgsql/data/* #先将data目录下的数据都清空pg_basebackup -h 192.168.20.93 -U replica -D /opt/pgsql/data -X stream -P # 从93拷贝数据到94（基础备份）mkdir /opt/pgsql/pg_archive 2.配置recovery.conf 复制/usr/pgsql-9.4/share/recovery.conf.sample 到 /opt/pgsql/data/recovery.conf cp /usr/pgsql-9.4/share/recovery.conf.sample /opt/pgsql/data/recovery.conf 修改recovery.conf standby_mode = on # 说明该节点是从服务器primary_conninfo = &apos;host=192.168.20.93 port=5432 user=replica password=replica&apos; # 主服务器的信息以及连接的用户recovery_target_timeline = &apos;latest&apos; 3.配置postgresql.conf wal_level = hot_standbymax_connections = 1000 ＃ 一般查多于写的应用从库的最大连接数要比较大hot_standby = on ＃ 说明这台机器不仅仅是用于数据归档，也用于数据查询max_standby_streaming_delay = 30s # 数据流备份的最大延迟时间wal_receiver_status_interval = 10s # 多久向主报告一次从的状态，当然从每次数据复制都会向主报告状态，这里只是设置最长的间隔时间hot_standby_feedback = on # 如果有错误的数据复制，是否向主进行反馈 配置完后重启从服务器 pg_ctl stop -D /opt/pgsql/datapg_ctl start -D /opt/pgsql/data 3. 验证是否部署成功在主节点上执行： select client_addr,sync_state from pg_stat_replication; 结果如下： postgres=# select client_addr,sync_state from pg_stat_replication; client_addr | sync_state---------------+------------ 192.168.20.94 | async(1 行记录) 说明94是从服务器，在接收流，而且是异步流复制。 此外，还可以分别在主、从节点上运行 ps aux | grep postgres 来查看进程： 主服务器（93）上： postgres 262270 0.0 0.0 337844 2832 ? Ss 10:14 0:00 postgres: wal sender process replica 192.168.20.94(13059) streaming 0/A002A88 可以看到有一个 wal sender 进程。 从服务器（94）上： postgres 569868 0.0 0.0 384604 2960 ? Ss 10:14 0:02 postgres: wal receiver process streaming 0/A002B60 可以看到有一个 wal receiver 进程。 至此，PostgreSQL主从流复制安装部署完成。 在主服务器上插入数据或删除数据，在从服务器上能看到相应的变化。从服务器上只能查询，不能插入或删除。","raw":null,"content":null,"categories":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"http://linbingdong.com/categories/PostgreSQL/"}],"tags":[{"name":"关系数据库","slug":"关系数据库","permalink":"http://linbingdong.com/tags/关系数据库/"},{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"http://linbingdong.com/tags/PostgreSQL/"}]},{"title":"Java 7新机制之自动关闭资源——try-with-resources","slug":"Java 7新机制之自动关闭资源——try-with-resources","date":"2017-03-11T12:33:42.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2017/03/11/Java 7新机制之自动关闭资源——try-with-resources/","link":"","permalink":"http://linbingdong.com/2017/03/11/Java 7新机制之自动关闭资源——try-with-resources/","excerpt":"在Java 7之前，IO操作为了保证资源能被关闭，往往会在try代码块后加上finally代码块来处理资源的关闭。","text":"在Java 7之前，IO操作为了保证资源能被关闭，往往会在try代码块后加上finally代码块来处理资源的关闭。 如下： BufferedReader br = new BufferedReader(new FileReader(path));try &#123; return br.readLine();&#125; finally &#123; if (br != null) br.close();&#125; Java 7新增了try-with-resources语法来保证资源的关闭。现在你可以这么写： try(BufferedReader br = new BufferedReader(new FileReader(path));) &#123; return br.readLine();&#125; 代码块里处理完后会自动关闭资源，不需要显示调用close()方法。也不需要finally。代码简洁了很多。 注意： 被关闭的资源类需要实现AutoClosable接口或者是Closable接口。 需要自动关闭的资源在try后面的括号里声明。允许声明多个被关闭的资源，关闭的顺序是与创建资源的顺序相反。","raw":null,"content":null,"categories":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/tags/Java/"}]},{"title":"Git常用命令","slug":"Git常用命令","date":"2017-03-11T12:33:42.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2017/03/11/Git常用命令/","link":"","permalink":"http://linbingdong.com/2017/03/11/Git常用命令/","excerpt":"记录 Git 常用的命令。","text":"记录 Git 常用的命令。 命令 说明 git init 初始化git仓库 git add filename 将文件添加到暂存区 git commit -m “blablabla…” 提交到本地仓库 git push origin master 将本地master分支推到远程 git status 查看那些文件被修改过 git diff 查看修改内容 git reset –hard HEAD^ 回到某个版本 （HEAD表示当前版本（已commit的最新版本），HEAD^表示上一个版本，HEAD^^表示上上个版本，依此类推） git log –oneline 查看提交历史（–oneline 简洁地打印提交历史） git reflog 查看命令历史，包括commit和reset等 git diff HEAD – readme.txt 查看工作区和版本库里最新版本的区别 git checkout – filename 撤销工作区中对该文件的修改 让该文件回到最近一次 commit 或 add 时的状态 git rm filename 从版本库中删除文件 git reset HEAD filename 把暂存区对该文件的修改撤销掉（unstage），重新放回工作区 git branch 查看分支 git branch branchname 创建分支 git checkout branchname 切换分支 git checkout -b branchname 创建并切换分支 git merge branchname 将分支合并到当前分支 git branch -d branchname 删除分支 git log –graph –oneline 分支合并图 git merge –no-ff -m “describe something” dev 跟dev合并，不使用ff，然后提交 git stash 把工作现场“储存”起来，等以后恢复现场再继续工作 git stash list 查看“储存”的工作现场 git stash pop 恢复工作现场并把stash内容删除 相当于 git stash apply stash@{0};git stash drop stash@{0} git branch -D branchname 将没有被merge过的分支强制删除 git remote 查看远程仓库信息 git remote -v 显示更详细的远程仓库信息 git push origin master 把本地该分支推送到远程库对应的远程分支上。如果推送失败，先用 git pull 拉取最新的再 git push git checkout -b brahch-name origin/branch-name 在本地创建和远程分支对应的分支 git branch –set-upstream branch-name origin/branch-name 建立本地分支和远程分支的关联 git tag v1.0 打上v1.0标签，默认打在最新提交的commit上 git tag 查看所有标签 git tag v0.9 9c4ef20 为 9c4ef20 这次commit打上v0.9标签 git show v0.9 查看标签（显示具体的commit内容） git tag -a -m “blablabla…” 可以指定标签信息 git tag -d v0.1 删除v0.1标签 git push origin v1.0 推送v1.0标签到远程 ，对应github上的一个release git push origin –tags 推送本地所有标签到远程 git tag -d v0.9 ; git push origin :refs/tags/v0.9 删除远程v0.9标签 git config –global color.ui true 设置让git显示颜色 git add -f xxx.pyc 强制添加被忽略的文件 git check-ignore -v xxx.pyc 查看该文件是被哪一条规则忽略的 git config –global alias.st status 设置status的别名为st 如果要忽略某些文件（比如.pyc文件），可以通过添加.gitignore文件来实现（写上 *.pyc）。.gitignore文件也需要 add 和 commit。 常用别名： git config –global alias.ci commit git config –global alias.st status git config –global alias.br branch git config –global alias.unstage ‘reset HEAD’ git config –global alias.last ‘log -1’ git config –global alias.lg “log –color –graph –pretty=format:’%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset’ –abbrev-commit” 如果想从版本库里删除某个文件： rm filename git rm filename git commit 在本地新建一个仓库 git init 初始化本地仓库 git remote add origin git@github.com:linbingdong/blog.git 关联一个远程库 git push -u origin master 第一次推送master分支的所有内容","raw":null,"content":null,"categories":[{"name":"编程工具","slug":"编程工具","permalink":"http://linbingdong.com/categories/编程工具/"}],"tags":[{"name":"Git","slug":"Git","permalink":"http://linbingdong.com/tags/Git/"}]},{"title":"Java I/O 操作示例","slug":"Java I:O 操作示例","date":"2017-03-11T12:33:42.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2017/03/11/Java I:O 操作示例/","link":"","permalink":"http://linbingdong.com/2017/03/11/Java I:O 操作示例/","excerpt":"给出几个Java I/O 操作的示例代码。","text":"给出几个Java I/O 操作的示例代码。 创建文件或目录import java.io.File;import java.io.IOException;public class TestFileIO &#123; public static void main(String[] args) &#123; File dir = new File(\"dir1\"); dir.mkdir(); //创建目录 File file = new File(dir,\"file1\"); //目录加文件名 File file2 = new File(\"dir1/file2\"); //完整路径 try &#123; file.createNewFile(); //创建文件,若存在同名文件,不会覆盖 file2.createNewFile(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 删除文件import java.io.File;public class TestFileIO &#123; public static void main(String[] args) &#123; File file = new File(\"dir1/file2\"); if (file.delete())&#123; System.out.println(file.getName() + \" is deleted!\"); &#125;else &#123; System.out.println(\"File is not deleted!\"); &#125; &#125;&#125; 向文件逐行写入内容(覆盖写） FileOutputStream import java.io.*;public class TestFileIO &#123; public static void main(String[] args) throws IOException &#123; File fout = new File(\"dir1/file1\"); FileOutputStream fos = new FileOutputStream(fout); BufferedWriter bw = new BufferedWriter(new OutputStreamWriter(fos)); for (int i = 0; i &lt; 10; i++)&#123; bw.write(\"something\"); bw.newLine(); &#125; bw.close(); &#125;&#125; FileWriter import java.io.*;public class TestFileIO &#123; public static void main(String[] args) throws IOException &#123; File fout = new File(\"dir1/file1\"); FileWriter fw = new FileWriter(fout); for (int i = 0; i &lt; 10; i++)&#123; fw.write(\"something\" + System.getProperty(\"line.separator\")); &#125; fw.close(); &#125;&#125; PrintWriter import java.io.*;public class TestFileIO &#123; public static void main(String[] args) throws IOException &#123; File fout = new File(\"dir1/file1\"); PrintWriter pw = new PrintWriter(new FileWriter(fout)); for (int i = 0; i &lt; 10; i++)&#123; pw.println(\"something\"); &#125; pw.close(); &#125;&#125; OutputStreamWriter import java.io.*;public class TestFileIO &#123; public static void main(String[] args) throws IOException &#123; File fout = new File(\"dir1/file1\"); FileOutputStream fos = new FileOutputStream(fout); OutputStreamWriter osw = new OutputStreamWriter(fos); for (int i = 0; i &lt; 10; i++) &#123; osw.write(\"something\" + System.getProperty(\"line.separator\")); &#125; osw.close(); &#125;&#125; 注：往文本文件里写内容用FileWriter即可，比较方便。但是如果要自己定义字符编号和byte-buffer大小的话就要用FileOutputStream。 PrintWriter跟FileWriter的主要区别是PrintWriter可以格式化输出。该类实现了PrintStream的所有print方法。 追加写import java.io.*;public class TestFileIO &#123; public static void main(String[] args) throws IOException &#123; File fout = new File(\"dir1/file1\"); FileOutputStream fos = new FileOutputStream(fout,true); //跟覆盖写唯一的区别是这里加了个true参数。 OutputStreamWriter osw = new OutputStreamWriter(fos); for (int i = 0; i &lt; 10; i++) &#123; osw.write(\"something\" + System.getProperty(\"line.separator\")); &#125; osw.close(); &#125;&#125; 拷贝文件import java.io.*;import java.nio.file.Files;import java.nio.file.Path;import java.nio.file.Paths;public class TestFileIO &#123; public static void main(String[] args) throws IOException &#123; Path sour = Paths.get(\"dir1/file1\"); Path des = Paths.get(\"dir1/file2\"); Files.copy(sour,des); //Files.copy(a,b)。 &#125;&#125; 合并多个文件读取多个文件的内容，写入一个文件。 import java.io.*;/** * Created by lbd on 2017/1/13. */public class MergeFiles &#123; public static void main(String[] args) throws IOException &#123; String sourceFile1Path = \"dir1/file1\"; String sourceFile2Path = \"dir1/file2\"; String mergedFilePath = \"dir1/mergedFile.txt\"; File[] files = new File[2]; files[0] = new File(sourceFile1Path); files[1] = new File(sourceFile2Path); File mergedFile = new File(mergedFilePath); mergeFiles(files,mergedFile); &#125; public static void mergeFiles(File[] files,File mergedFile) throws IOException &#123; FileWriter fw = new FileWriter(mergedFile,true); BufferedWriter bw = new BufferedWriter(fw); for (File f : files)&#123; System.out.println(\"merging: \" + f.getName()); FileReader fr = new FileReader(f); BufferedReader br = new BufferedReader(fr); String aLine; while ((aLine = br.readLine()) != null)&#123; bw.write(aLine); bw.newLine(); &#125; br.close(); &#125; bw.close(); &#125;&#125; 移动文件调用的是File.renameTo()方法。 import java.io.*;public class MoveFile &#123; public static void main(String[] args) throws IOException &#123; File f1 = new File(\"dir1/file1\"); File f2 = new File(\"dir1/dir2/file3\"); //dir2目录必须存在,否则无法移动成功 f1.renameTo(f2); &#125;&#125; 对文件内容排序file1内容如下： dog cat--windows--kankanppsgame--annot be guaranteed as it is, generally speaking, --impossible to make any hard guarantees in the p--resence of unsynchr 对行进行排序，以上面的文本为例，排序后arantees in the p应该在第一行 import java.io.*;import java.util.ArrayList;import java.util.Collections;/** * Created by lbd on 2017/1/13. */public class TestJavaIO &#123; public static void main(String[] args) throws IOException &#123; File fin = new File(\"file1\"); File fout = new File(\"file2\"); String s; FileWriter fw = new FileWriter(fout); FileReader fr = new FileReader(fin); BufferedReader br = new BufferedReader(fr); BufferedWriter bw = new BufferedWriter(fw); ArrayList&lt;String&gt; al = new ArrayList&lt;&gt;(); while ((s = br.readLine()) != null )&#123; if (!s.trim().startsWith(\"-\") &amp;&amp; s.trim().length() &gt; 0)&#123; al.add(s); &#125; &#125; Collections.sort(al); for (String line : al)&#123; bw.write(line); bw.newLine(); bw.write(\"------------------------------\"); bw.newLine(); &#125; br.close(); bw.close(); &#125;&#125; file2内容如下： arantees in the p------------------------------as it is, generally speaking, ------------------------------cat------------------------------dog ------------------------------game------------------------------pps------------------------------","raw":null,"content":null,"categories":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/tags/Java/"}]},{"title":"Python日志模块示例","slug":"Python日志模块示例","date":"2017-03-11T12:33:42.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2017/03/11/Python日志模块示例/","link":"","permalink":"http://linbingdong.com/2017/03/11/Python日志模块示例/","excerpt":"给出Python日志模块显示配置和文件配置两种方式的示例","text":"给出Python日志模块显示配置和文件配置两种方式的示例 显示配置在程序中直接调用函数来设置参数 #!/usr/bin/python# -*- coding: utf-8 -*-import logging# 创建一个loggerlogger = logging.getLogger('example')logger.setLevel(logging.DEBUG)# 创建一个输出到控制台的handlersh = logging.StreamHandler()sh.setLevel(logging.ERROR)# 创建一个输出到文件的handlerfh = logging.FileHandler('loggingtest.log')fh.setLevel(logging.INFO)# 设置输出格式fmt = logging.Formatter('%(asctime)s - %(threadName)s - [%(levelname)s] : %(message)s')# handler设置formattersh.setFormatter(fmt)fh.setFormatter(fmt)# logger添加handlerlogger.addHandler(sh)logger.addHandler(fh)# 写日志logger.debug('debug message')logger.info('info message')logger.warn('warn message')logger.error('error message')logger.critical('critical message') 文件配置通过配置文件进行配置，使用fileConfig()函数读取配置文件 配置文件 logging.conf : [loggers]keys=root,example01[logger_root]level=DEBUGhandlers=hand01,hand02[logger_example01]handlers=hand01,hand02qualname=example01propagate=0[handlers]keys=hand01,hand02[handler_hand01]class=StreamHandlerlevel=INFOformatter=form02args=(sys.stderr,)[handler_hand02]class=FileHandlerlevel=DEBUGformatter=form01args=(&apos;log.log&apos;,&apos;a&apos;)[formatters]keys=form01,form02[formatter_form01]format=%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s[formatter_form02]format=%(asctime)s - %(threadName)s - [%(levelname)s] : %(message)s 程序 LogByFile.py : #!/usr/bin/python# -*- coding: utf-8 -*-import loggingimport logging.configlogging.config.fileConfig('logging.conf')logger = logging.getLogger('example01')# 写日志logger.debug('debug message')logger.info('info message')logger.warn('warn message')logger.error('error message')logger.critical('critical message')","raw":null,"content":null,"categories":[{"name":"Python","slug":"Python","permalink":"http://linbingdong.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://linbingdong.com/tags/Python/"}]},{"title":"Java I/O 总结","slug":"Java I:O总结","date":"2017-03-11T12:33:42.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2017/03/11/Java I:O总结/","link":"","permalink":"http://linbingdong.com/2017/03/11/Java I:O总结/","excerpt":"Java中I/O操作主要是指使用Java进行输入，输出操作. Java所有的I/O机制都是基于数据流进行输入输出，这些数据流表示了字符或者字节数据的流动序列。","text":"Java中I/O操作主要是指使用Java进行输入，输出操作. Java所有的I/O机制都是基于数据流进行输入输出，这些数据流表示了字符或者字节数据的流动序列。 数据流是一串连续不断的数据的集合，就象水管里的水流，在水管的一端一点一点地供水，而在水管的另一端看到的是一股连续不断的水流。数据写入程序可以是一段、一段地向数据流管道中写入数据，这些数据段会按先后顺序形成一个长的数据流。对数据读取程序来说，看不到数据流在写入时的分段情况，每次可以读取其中的任意长度的数据，但只能先读取前面的数据后，再读取后面的数据（不能随机读取）。不管写入时是将数据分多次写入，还是作为一个整体一次写入，读取时的效果都是完全一样的。 简而言之：数据流是一组有序，有起点和终点的字节的数据序列。包括输入流和输出流。 当程序需要读取数据的时候，就会建立一个通向数据源的连接，这个数据源可以是文件，内存，或是网络连接。类似的，当程序需要写入数据的时候，就会建立一个通向目的地的连接。 数据流分类： 流序列中的数据既可以是未经加工的原始二进制数据，也可以是经一定编码处理后符合某种格式规定的特定数据。因此Java中的流分为两种： 1) 字节流：数据流中最小的数据单元是字节 2) 字符流：数据流中最小的数据单元是字符， Java中的字符是Unicode编码，一个字符占用两个字节。 概览Java.io包中最重要的就是5个类和一个接口。5个类指的是File、OutputStream、InputStream、Writer、Reader；一个接口指的是Serializable。掌握了这些就掌握了Java I/O的精髓了。 Java I/O主要包括如下3层次： 流式部分——最主要的部分。如：OutputStream、InputStream、Writer、Reader等 非流式部分——如：File类、RandomAccessFile类和FileDescriptor等类 其他——文件读取部分的与安全相关的类，如：SerializablePermission类，以及与本地操作系统相关的文件系统的类，如：FileSystem类和Win32FileSystem类和WinNTFileSystem类。 主要类如下： File（文件特征与管理）：用于文件或者目录的描述信息，例如生成新目录，修改文件名，删除文件，判断文件所在路径等。 InputStream（字节流，二进制格式操作）：抽象类，基于字节的输入操作，是所有输入流的父类。定义了所有输入流都具有的共同特征。 OutputStream（字节流，二进制格式操作）：抽象类。基于字节的输出操作。是所有输出流的父类。定义了所有输出流都具有的共同特征。 Reader（字符流，文本格式操作）：抽象类，基于字符的输入操作。 Writer（字符流，文本格式操作）：抽象类，基于字符的输出操作。 RandomAccessFile（随机文件操作）：它的功能丰富，可以从文件的任意位置进行存取（输入输出）操作。 I/O流java.io包里有4个基本类：InputStream、OutputStream及Reader、Writer类，它们分别处理字节流和字符流。 其他各种各样的流都是由这4个派生出来的。 按来源/去向分类： File（文件）： FileInputStream, FileOutputStream, FileReader, FileWriter byte[]：ByteArrayInputStream, ByteArrayOutputStream Char[]: CharArrayReader, CharArrayWriter String: StringBufferInputStream, StringReader, StringWriter 网络数据流：InputStream, OutputStream, Reader, Writer InputStreamInputStream 为字节输入流，它本身为一个抽象类，必须依靠其子类实现各种功能，此抽象类是表示字节输入流的所有类的超类。 继承自InputStream 的流都是向程序中输入数据的，且数据单位为字节（8bit）； InputStream是输入字节数据用的类，所以InputStream类提供了3种重载的read方法.Inputstream类中的常用方法： public abstract int read( )：读取一个byte的数据，返回值是高位补0的int类型值。若返回值=-1说明没有读取到任何字节读取工作结束。 public int read(byte b[ ])：读取b.length个字节的数据放到b数组中。返回值是读取的字节数。该方法实际上是调用下一个方法实现的 public int read(byte b[ ], int off, int len)：从输入流中最多读取len个字节的数据，存放到偏移量为off的b数组中。 public int available( )：返回输入流中可以读取的字节数。注意：若输入阻塞，当前线程将被挂起，如果InputStream对象调用这个方法的话，它只会返回0，这个方法必须由继承InputStream类的子类对象调用才有用， public long skip(long n)：忽略输入流中的n个字节，返回值是实际忽略的字节数, 跳过一些字节来读取 public int close( ) ：使用完后，必须对我们打开的流进行关闭。 来看看几种不同的InputStream： FileInputStream把一个文件作为InputStream，实现对文件的读取操作 ByteArrayInputStream：把内存中的一个缓冲区作为InputStream使用 StringBufferInputStream：把一个String对象作为InputStream PipedInputStream：实现了pipe的概念，主要在线程中使用 SequenceInputStream：把多个InputStream合并为一个InputStream OutputStreamOutputStream提供了3个write方法来做数据的输出，这个是和InputStream是相对应的。 public void write(byte b[ ])：将参数b中的字节写到输出流。 public void write(byte b[ ], int off, int len) ：将参数b的从偏移量off开始的len个字节写到输出流。 public abstract void write(int b) ：先将int转换为byte类型，把低字节写入到输出流中。 public void flush( ) : 将数据缓冲区中数据全部输出，并清空缓冲区。 public void close( ) : 关闭输出流并释放与流相关的系统资源。 几种不同的OutputStream： ByteArrayOutputStream：把信息存入内存中的一个缓冲区中 FileOutputStream：把信息存入文件中 PipedOutputStream：实现了pipe的概念，主要在线程中使用 SequenceOutputStream：把多个OutStream合并为一个OutStream Reader和InputStream类似；Writer和OutputStream类似。 有两个需要注意的： InputStreamReader ： 从输入流读取字节，在将它们转换成字符。 BufferReader :接受Reader对象作为参数，并对其添加字符缓冲器，使用readline()方法可以读取一行。 如何选择I/O流 确定是输入还是输出输入:输入流 InputStream Reader输出:输出流 OutputStream Writer 明确操作的数据对象是否是纯文本是:字符流 Reader，Writer否:字节流 InputStream，OutputStream 明确具体的设备。 文件： 读：FileInputStream,, FileReader, 写：FileOutputStream，FileWriter 数组： byte[ ]：ByteArrayInputStream, ByteArrayOutputStream char[ ]：CharArrayReader, CharArrayWriter String： StringBufferInputStream(已过时，因为其只能用于String的每个字符都是8位的字符串), StringReader, StringWriter Socket流 键盘：用System.in（是一个InputStream对象）读取，用System.out（是一个OutoutStream对象）打印 是否需要转换流是，就使用转换流，从Stream转化为Reader、Writer：InputStreamReader，OutputStreamWriter 是否需要缓冲提高效率是就加上Buffered：BufferedInputStream, BufferedOuputStream, BufferedReader, BufferedWriter 是否需要格式化输出 示例代码 将标准输入（键盘输入）显示到标准输出（显示器），支持字符。 char ch;BufferedReader in = new BufferedReader(new InputStreamReader(System.in)); //将字节流转为字符流，带缓冲try &#123; while ((ch = (char) in.read()) != -1)&#123; System.out.print(ch); &#125;&#125; catch (IOException e) &#123; e.printStackTrace();&#125; 将AtomicityTest.java的内容打印到显示器 方法一：BufferedReader in = new BufferedReader(new FileReader(\"AtomicityTest.java\"));String s;try &#123; while ((s = in.readLine()) != null)&#123; System.out.println(s); &#125; in.close();&#125; catch (IOException e) &#123; e.printStackTrace();&#125; 方法二： FileReader in = new FileReader(\"AtomicityTest.java\");int b;try &#123; while ((b = in.read()) != -1)&#123; System.out.print((char)b); &#125; in.close();&#125; catch (IOException e) &#123; e.printStackTrace();&#125; 方法三：(有可能出现乱码） FileInputStream in = new FileInputStream(\"AtomicityTest.java\");int n = 50;byte[] buffer = new byte[n];try &#123; while ((in.read(buffer,0,n) != -1 &amp;&amp; n &gt; 0))&#123; System.out.print(new String(buffer)); &#125; in.close();&#125; catch (IOException e) &#123; e.printStackTrace();&#125; 将文件A的内容拷贝到文件B FileInputStream in = new FileInputStream(\"AtomicityTest.java\");FileOutputStream out = new FileOutputStream(\"copy.txt\");int b;while ((b = in.read()) != -1)&#123; out.write(b);&#125;out.flush();in.close();out.close(); 将标准输入的内容写入文件 Scanner in = new Scanner(System.in);FileWriter out = new FileWriter(\"systemIn.log\");String s;while (!(s = in.nextLine()).equals(\"Q\"))&#123; out.write(s + \"\\n\");&#125;out.flush();out.close();in.close();","raw":null,"content":null,"categories":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/tags/Java/"}]},{"title":"大数据基准测试工具TPCx-BB源码分析","slug":"大数据基准测试工具TPCx-BB源码分析","date":"2017-03-11T12:33:42.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2017/03/11/大数据基准测试工具TPCx-BB源码分析/","link":"","permalink":"http://linbingdong.com/2017/03/11/大数据基准测试工具TPCx-BB源码分析/","excerpt":"TPCx-BB是大数据基准测试工具,它通过模拟零售商的30个应用场景，执行30个查询来衡量基于Hadoop的大数据系统的包括硬件和软件的性能。其中一些场景还用到了机器学习算法（聚类、线性回归等）。为了更好地了解被测试的系统的性能，需要对TPCx-BB整个测试流程深入了解。本文详细分析了整个TPCx-BB测试工具的源码，希望能够对大家理解TPCx-BB有所帮助。","text":"TPCx-BB是大数据基准测试工具,它通过模拟零售商的30个应用场景，执行30个查询来衡量基于Hadoop的大数据系统的包括硬件和软件的性能。其中一些场景还用到了机器学习算法（聚类、线性回归等）。为了更好地了解被测试的系统的性能，需要对TPCx-BB整个测试流程深入了解。本文详细分析了整个TPCx-BB测试工具的源码，希望能够对大家理解TPCx-BB有所帮助。 代码结构主目录（$BENCH_MARK_HOME）下有： bin conf data-generator engines tools 几个子目录。 bin下有几个 module ,是执行时需要用到的脚本：bigBench、cleanLogs、logEnvInformation、runBenchmark、zipLogs等 conf下有两个配置文件：bigBench.properties 和 userSettings.conf bigBench.properties 主要设置 workload（执行的benchmarkPhases）和 power_test_0（POWER_TEST 阶段需要执行的SQL查询） 默认 workload ： workload=CLEAN_ALL,ENGINE_VALIDATION_DATA_GENERATION,ENGINE_VALIDATION_LOAD_TEST,ENGINE_VALIDATION_POWER_TEST,ENGINE_VALIDATION_RESULT_VALIDATION,CLEAN_DATA,DATA_GENERATION,BENCHMARK_START,LOAD_TEST,POWER_TEST,THROUGHPUT_TEST_1,BENCHMARK_STOP,VALIDATE_POWER_TEST,VALIDATE_THROUGHPUT_TEST_1 默认 power_test_0 ：1-30 userSetting.conf 是一些基本设置，包括JAVA environment 、default settings for benchmark（database、engine、map_tasks、scale_factor …）、HADOOP environment、HDFS config and paths、Hadoop data generation options(DFS_REPLICATION、HADOOP_JVM_ENV…) data-generator下是跟数据生成相关的脚本及配置文件。详细内容在下面介绍。 engines下是TPCx-BB支持的4种引擎：biginsights、hive、impala、spark_sql。默认引擎为hive。实际上，只有hive目录下不为空，其他三个目录下均为空，估计是现在还未完善。 tools下有两个jar包：HadoopClusterExec.jar 和 RunBigBench.jar 。其中 RunBigBench.jar 是执行TPCx-BB测试的一个非常重要的文件，大部分程序都在该jar包内。 数据生成数据生成相关程序和配置都在 data-generator 目录下。该目录下有一个 pdgf.jar 包和 config、dicts、extlib 三个子目录。 pdgf.jar是数据生成的Java程序，代码量很大。config下有两个配置文件：bigbench-generation.xml 和 bigbench-schema.xml 。 bigbench-generation.xml 主要设置生成的原始数据（不是数据库表）包含哪几张表、每张表的表名以及表输出的目录、表文件的后缀、分隔符、字符编码等。 &lt;schema name=\"default\"&gt; &lt;tables&gt; &lt;!-- not refreshed tables --&gt; &lt;!-- tables not used in benchmark, but some tables have references to them. not refreshed. Kept for legacy reasons --&gt; &lt;table name=\"income_band\"&gt;&lt;/table&gt; &lt;table name=\"reason\"&gt;&lt;/table&gt; &lt;table name=\"ship_mode\"&gt;&lt;/table&gt; &lt;table name=\"web_site\"&gt;&lt;/table&gt; &lt;!-- /tables not used in benchmark --&gt; &lt;!-- Static tables (fixed small size, generated only on node 1, skipped on others, not generated during refresh) --&gt; &lt;table name=\"date_dim\" static=\"true\"&gt;&lt;/table&gt; &lt;table name=\"time_dim\" static=\"true\"&gt;&lt;/table&gt; &lt;table name=\"customer_demographics\" static=\"true\"&gt;&lt;/table&gt; &lt;table name=\"household_demographics\" static=\"true\"&gt;&lt;/table&gt; &lt;!-- /static tables --&gt; &lt;!-- \"normal\" tables. split over all nodes. not generated during refresh --&gt; &lt;table name=\"store\"&gt;&lt;/table&gt; &lt;table name=\"warehouse\"&gt;&lt;/table&gt; &lt;table name=\"promotion\"&gt;&lt;/table&gt; &lt;table name=\"web_page\"&gt;&lt;/table&gt; &lt;!-- /\"normal\" tables.--&gt; &lt;!-- /not refreshed tables --&gt; &lt;!-- refreshed tables. Generated on all nodes. Refresh tables generate extra data during refresh (e.g. add new data to the existing tables) In \"normal\"-Phase generate table rows: [0,REFRESH_PERCENTAGE*Table.Size]; In \"refresh\"-Phase generate table rows: [REFRESH_PERCENTAGE*Table.Size+1, Table.Size] .Has effect only if $&#123;REFRESH_SYSTEM_ENABLED&#125;==1. --&gt; &lt;table name=\"customer\"&gt; &lt;scheduler name=\"DefaultScheduler\"&gt; &lt;partitioner name=\"pdgf.core.dataGenerator.scheduler.TemplatePartitioner\"&gt; &lt;prePartition&gt;&lt;![CDATA[ if($&#123;REFRESH_SYSTEM_ENABLED&#125;&gt;0)&#123; int tableID = table.getTableID(); int timeID = 0; long lastTableRow=table.getSize()-1; long rowStart; long rowStop; boolean exclude=false; long refreshRows=table.getSize()*(1.0-$&#123;REFRESH_PERCENTAGE&#125;); if($&#123;REFRESH_PHASE&#125;&gt;0)&#123; //Refresh part rowStart = lastTableRow - refreshRows +1; rowStop = lastTableRow; if(refreshRows&lt;=0)&#123; exclude=true; &#125; &#125;else&#123; //\"normal\" part rowStart = 0; rowStop = lastTableRow - refreshRows; &#125; return new pdgf.core.dataGenerator.scheduler.Partition(tableID, timeID,rowStart,rowStop,exclude); &#125;else&#123; //DEFAULT return getParentPartitioner().getDefaultPrePartition(project, table); &#125; ]]&gt;&lt;/prePartition&gt; &lt;/partitioner&gt; &lt;/scheduler&gt; &lt;/table&gt; &lt;output name=\"SplitFileOutputWrapper\"&gt; &lt;!-- DEFAULT output for all Tables, if no table specific output is specified--&gt; &lt;output name=\"CSVRowOutput\"&gt; &lt;fileTemplate&gt;&lt;![CDATA[outputDir + table.getName() +(nodeCount!=1?\"_\"+pdgf.util.StaticHelper.zeroPaddedNumber(nodeNumber,nodeCount):\"\")+ fileEnding]]&gt;&lt;/fileTemplate&gt; &lt;outputDir&gt;output/&lt;/outputDir&gt; &lt;fileEnding&gt;.dat&lt;/fileEnding&gt; &lt;delimiter&gt;|&lt;/delimiter&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;sortByRowID&gt;true&lt;/sortByRowID&gt; &lt;/output&gt; &lt;output name=\"StatisticsOutput\" active=\"1\"&gt; &lt;size&gt;$&#123;item_size&#125;&lt;/size&gt;&lt;!-- a counter per item .. initialize later--&gt; &lt;fileTemplate&gt;&lt;![CDATA[outputDir + table.getName()+\"_audit\" +(nodeCount!=1?\"_\"+pdgf.util.StaticHelper.zeroPaddedNumber(nodeNumber,nodeCount):\"\")+ fileEnding]]&gt;&lt;/fileTemplate&gt; &lt;outputDir&gt;output/&lt;/outputDir&gt; &lt;fileEnding&gt;.csv&lt;/fileEnding&gt; &lt;delimiter&gt;,&lt;/delimiter&gt; &lt;header&gt;&lt;!--\"\" + pdgf.util.Constants.DEFAULT_LINESEPARATOR--&gt; &lt;/header&gt; &lt;footer&gt;&lt;/footer&gt; bigbench-schema.xml 设置了很多参数，有跟表的规模有关的，比如每张表的大小（记录的条数）;绝大多数是跟表的字段有关的，比如时间的起始、结束、性别比例、结婚比例、指标的上下界等。还具体定义了每个字段是怎么生成的，以及限制条件。示例如下： 生成的数据大小由 SCALE_FACTOR（-f） 决定。如 -f 1，则生成的数据总大小约为1G；-f 100，则生成的数据总大小约为100G。那么SCALE_FACTOR（-f） 是怎么精确控制生成的数据的大小呢？ 原因是 SCALE_FACTOR（-f） 决定了每张表的记录数。如下，customer 表的记录数为 100000.0d * ${SF_sqrt}，即如果 -f 1 则 customer 表的记录数为 100000*sqrt(1)= 10万条 ;如果 -f 100 则 customer 表的记录数为 100000*sqrt(100)= 100万条 &lt;property name=\"$&#123;customer_size&#125;\" type=\"long\"&gt;100000.0d * $&#123;SF_sqrt&#125;&lt;/property&gt; &lt;property name=\"$&#123;DIMENSION_TABLES_START_DAY&#125;\" type=\"datetime\"&gt;2000-01-03 00:00:00&lt;/property&gt;&lt;property name=\"$&#123;DIMENSION_TABLES_END_DAY&#125;\" type=\"datetime\"&gt;2004-01-05 00:00:00&lt;/property&gt; &lt;property name=\"$&#123;gender_likelihood&#125;\" type=\"double\"&gt;0.5&lt;/property&gt;&lt;property name=\"$&#123;married_likelihood&#125;\" type=\"double\"&gt;0.3&lt;/property&gt; &lt;property name=\"$&#123;WP_LINK_MIN&#125;\" type=\"double\"&gt;2&lt;/property&gt;&lt;property name=\"$&#123;WP_LINK_MAX&#125;\" type=\"double\"&gt;25&lt;/property&gt; &lt;field name=\"d_date\" size=\"13\" type=\"CHAR\" primary=\"false\"&gt; &lt;gen_DateTime&gt; &lt;disableRng&gt;true&lt;/disableRng&gt; &lt;useFixedStepSize&gt;true&lt;/useFixedStepSize&gt; &lt;startDate&gt;$&#123;date_dim_begin_date&#125;&lt;/startDate&gt; &lt;endDate&gt;$&#123;date_dim_end_date&#125;&lt;/endDate&gt; &lt;outputFormat&gt;yyyy-MM-dd&lt;/outputFormat&gt; &lt;/gen_DateTime&gt;&lt;/field&gt; &lt;field name=\"t_time_id\" size=\"16\" type=\"CHAR\" primary=\"false\"&gt; &lt;gen_ConvertNumberToString&gt; &lt;gen_Id/&gt; &lt;size&gt;16.0&lt;/size&gt; &lt;characters&gt;ABCDEFGHIJKLMNOPQRSTUVWXYZ&lt;/characters&gt; &lt;/gen_ConvertNumberToString&gt;&lt;/field&gt; &lt;field name=\"cd_dep_employed_count\" size=\"10\" type=\"INTEGER\" primary=\"false\"&gt; &lt;gen_Null probability=\"$&#123;NULL_CHANCE&#125;\"&gt; &lt;gen_WeightedListItem filename=\"dicts/bigbench/ds-genProbabilities.txt\" list=\"dependent_count\" valueColumn=\"0\" weightColumn=\"0\" /&gt; &lt;/gen_Null&gt; &lt;/field&gt; dicts下有city.dict、country.dict、male.dict、female.dict、state.dict、mail_provider.dict等字典文件，表里每一条记录的各个字段应该是从这些字典里生成的。 extlib下是引用的外部程序jar包。有 lucene-core-4.9.0.jar、commons-net-3.3.jar、xml-apis.jar和log4j-1.2.15.jar等 总结： pdgf.jar根据bigbench-generation.xml 和 bigbench-schema.xml两个文件里的配置（表名、字段名、表的记录条数、每个字段生成的规则），从 dicts 目录下对应的 .dict文件获取表中每一条记录、每个字段的值，生成原始数据。 customer 表里的某条记录如下： 0 AAAAAAAAAAAAAAAA 1824793 3203 2555 28776 14690 Ms. Marisa Harrington N 17 4 1988 UNITED ARAB EMIRATES RRCyuY3XfE3a Marisa.Harrington@lawyer.com gdMmGdU9 如果执行 TPCx-BB 测试时指定 -f 1（SCALE_FACTOR = 1） 则最终生成的原始数据总大小约为 1G(977M+8.6M) [root@node-20-100 ~]# hdfs dfs -du -h /user/root/benchmarks/bigbench/data12.7 M 38.0 M /user/root/benchmarks/bigbench/data/customer5.1 M 15.4 M /user/root/benchmarks/bigbench/data/customer_address74.2 M 222.5 M /user/root/benchmarks/bigbench/data/customer_demographics14.7 M 44.0 M /user/root/benchmarks/bigbench/data/date_dim151.5 K 454.4 K /user/root/benchmarks/bigbench/data/household_demographics327 981 /user/root/benchmarks/bigbench/data/income_band405.3 M 1.2 G /user/root/benchmarks/bigbench/data/inventory6.5 M 19.5 M /user/root/benchmarks/bigbench/data/item4.0 M 12.0 M /user/root/benchmarks/bigbench/data/item_marketprices53.7 M 161.2 M /user/root/benchmarks/bigbench/data/product_reviews45.3 K 135.9 K /user/root/benchmarks/bigbench/data/promotion3.0 K 9.1 K /user/root/benchmarks/bigbench/data/reason1.2 K 3.6 K /user/root/benchmarks/bigbench/data/ship_mode3.3 K 9.9 K /user/root/benchmarks/bigbench/data/store4.1 M 12.4 M /user/root/benchmarks/bigbench/data/store_returns88.5 M 265.4 M /user/root/benchmarks/bigbench/data/store_sales4.9 M 14.6 M /user/root/benchmarks/bigbench/data/time_dim584 1.7 K /user/root/benchmarks/bigbench/data/warehouse170.4 M 511.3 M /user/root/benchmarks/bigbench/data/web_clickstreams7.9 K 23.6 K /user/root/benchmarks/bigbench/data/web_page5.1 M 15.4 M /user/root/benchmarks/bigbench/data/web_returns127.6 M 382.8 M /user/root/benchmarks/bigbench/data/web_sales8.6 K 25.9 K /user/root/benchmarks/bigbench/data/web_site 执行流程要执行TPCx-BB测试，首先需要切换到TPCx-BB源程序的目录下，然后进入bin目录，执行以下语句： ./bigBench runBenchmark -f 1 -m 8 -s 2 -j 5 其中，-f、-m、-s、-j都是参数，用户可根据集群的性能以及自己的需求来设置。如果不指定，则使用默认值，默认值在 conf 目录下的 userSetting.conf 文件指定，如下： export BIG_BENCH_DEFAULT_DATABASE=\"bigbench\"export BIG_BENCH_DEFAULT_ENGINE=\"hive\"export BIG_BENCH_DEFAULT_MAP_TASKS=\"80\"export BIG_BENCH_DEFAULT_SCALE_FACTOR=\"1000\"export BIG_BENCH_DEFAULT_NUMBER_OF_PARALLEL_STREAMS=\"2\"export BIG_BENCH_DEFAULT_BENCHMARK_PHASE=\"run_query\" 默认 MAP_TASKS 为 80（-m 80）、SCALE_FACTOR 为 1000（-f 1000）、NUMBER_OF_PARALLEL_STREAMS 为 2（-s 2）。 所有可选参数及其意义如下： General options:-d 使用的数据库 (默认: $BIG_BENCH_DEFAULT_DATABASE -&gt; bigbench)-e 使用的引擎 (默认: $BIG_BENCH_DEFAULT_ENGINE -&gt; hive)-f 数据集的规模因子（scale factor） (默认: $BIG_BENCH_DEFAULT_SCALE_FACTOR -&gt; 1000)-h 显示帮助-m 数据生成的`map tasks`数 (default: $BIG_BENCH_DEFAULT_MAP_TASKS)\"-s 并行的`stream`数 (默认: $BIG_BENCH_DEFAULT_NUMBER_OF_PARALLEL_STREAMS -&gt; 2)Driver specific options:-a 伪装模式执行-b 执行期间将调用的bash脚本在标准输出中打印出来-i 指定需要执行的阶段 (详情见$BIG_BENCH_CONF_DIR/bigBench.properties)-j 指定需要执行的查询 (默认：1-30共30个查询均执行)\"-U 解锁专家模式 若指定了-U,即解锁了专家模式，则： echo \"EXPERT MODE ACTIVE\"echo \"WARNING - INTERNAL USE ONLY:\"echo \"Only set manually if you know what you are doing!\"echo \"Ignoring them is probably the best solution\" echo \"Running individual modules:\"echo \"Usage: `basename $0` module [options]\"-D 指定需要debug的查询部分. 大部分查询都只有一个单独的部分-p 需要执行的benchmark phase (默认: $BIG_BENCH_DEFAULT_BENCHMARK_PHASE -&gt; run_query)\"-q 指定需要执行哪个查询-t 指定执行查询时的stream数-v metastore population的sql脚本 (默认: $&#123;USER_POPULATE_FILE:-\"$BIG_BENCH_POPULATION_DIR/hiveCreateLoad.sql\"&#125;)\"-w metastore refresh的sql脚本 (默认: $&#123;USER_REFRESH_FILE:-\"$BIG_BENCH_REFRESH_DIR/hiveRefreshCreateLoad.sql\"&#125;)\"-y 含额外的用户自定义查询参数的文件 (global: $BIG_BENCH_ENGINE_CONF_DIR/queryParameters.sql)\"-z 含额外的用户自定义引擎设置的文件 (global: $BIG_BENCH_ENGINE_CONF_DIR/engineSettings.sql)\"List of available modules: $BIG_BENCH_ENGINE_BIN_DIR 回到刚刚执行TPCx-BB测试的语句： ./bigBench runBenchmark -f 1 -m 8 -s 2 -j 5 bigBenchbigBench是主脚本，runBenchmark是module。 bigBench 里设置了很多环境变量，因为后面调用 runBigBench.jar 的时候需要在Java程序里读取这些环境变量。 bigBench 前面都是在做一些基本工作，如设置环境变量、解析用户输入参数、赋予文件权限、设置路径等等。到最后一步调用 runBenchmark 的 runModule() 方法： 1.设置基本路径 export BIG_BENCH_VERSION=\"1.0\"export BIG_BENCH_BIN_DIR=\"$BIG_BENCH_HOME/bin\"export BIG_BENCH_CONF_DIR=\"$BIG_BENCH_HOME/conf\"export BIG_BENCH_DATA_GENERATOR_DIR=\"$BIG_BENCH_HOME/data-generator\"export BIG_BENCH_TOOLS_DIR=\"$BIG_BENCH_HOME/tools\"export BIG_BENCH_LOGS_DIR=\"$BIG_BENCH_HOME/logs\" 2.指定 core-site.xml 和 hdfs-site.xml 的路径 数据生成时要用到Hadoop集群，生成在hdfs上 export BIG_BENCH_DATAGEN_CORE_SITE=\"$BIG_BENCH_HADOOP_CONF/core-site.xml\"export BIG_BENCH_DATAGEN_HDFS_SITE=\"$BIG_BENCH_HADOOP_CONF/hdfs-site.xml\" 3.赋予整个包下所有可执行文件权限（.sh/.jar/.py） find \"$BIG_BENCH_HOME\" -name '*.sh' -exec chmod 755 &#123;&#125; +find \"$BIG_BENCH_HOME\" -name '*.jar' -exec chmod 755 &#123;&#125; +find \"$BIG_BENCH_HOME\" -name '*.py' -exec chmod 755 &#123;&#125; + 4.设置 userSetting.conf 的路径并 source USER_SETTINGS=\"$BIG_BENCH_CONF_DIR/userSettings.conf\"if [ ! -f \"$USER_SETTINGS\" ]then echo \"User settings file $USER_SETTINGS not found\" exit 1else source \"$USER_SETTINGS\"fi 5.解析输入参数和选项并根据选项的内容作设置 第一个参数必须是module_name 如果没有输入参数或者第一个参数以”-“开头，说明用户没有输入需要运行的module。 if [[ $# -eq 0 || \"`echo \"$1\" | cut -c1`\" = \"-\" ]]then export MODULE_NAME=\"\" SHOW_HELP=\"1\"else export MODULE_NAME=\"$1\" shiftfiexport LIST_OF_USER_OPTIONS=\"$@\" 解析用户输入的参数 根据用户输入的参数来设置环境变量 while getopts \":d:D:e:f:hm:p:q:s:t:Uv:w:y:z:abi:j:\" OPT; do case \"$OPT\" in # script options d) #echo \"-d was triggered, Parameter: $OPTARG\" &gt;&amp;2 USER_DATABASE=\"$OPTARG\" ;; D) #echo \"-D was triggered, Parameter: $OPTARG\" &gt;&amp;2 DEBUG_QUERY_PART=\"$OPTARG\" ;; e) #echo \"-e was triggered, Parameter: $OPTARG\" &gt;&amp;2 USER_ENGINE=\"$OPTARG\" ;; f) #echo \"-f was triggered, Parameter: $OPTARG\" &gt;&amp;2 USER_SCALE_FACTOR=\"$OPTARG\" ;; h) #echo \"-h was triggered, Parameter: $OPTARG\" &gt;&amp;2 SHOW_HELP=\"1\" ;; m) #echo \"-m was triggered, Parameter: $OPTARG\" &gt;&amp;2 USER_MAP_TASKS=\"$OPTARG\" ;; p) #echo \"-p was triggered, Parameter: $OPTARG\" &gt;&amp;2 USER_BENCHMARK_PHASE=\"$OPTARG\" ;; q) #echo \"-q was triggered, Parameter: $OPTARG\" &gt;&amp;2 QUERY_NUMBER=\"$OPTARG\" ;; s) #echo \"-t was triggered, Parameter: $OPTARG\" &gt;&amp;2 USER_NUMBER_OF_PARALLEL_STREAMS=\"$OPTARG\" ;; t) #echo \"-s was triggered, Parameter: $OPTARG\" &gt;&amp;2 USER_STREAM_NUMBER=\"$OPTARG\" ;; U) #echo \"-U was triggered, Parameter: $OPTARG\" &gt;&amp;2 USER_EXPERT_MODE=\"1\" ;; v) #echo \"-v was triggered, Parameter: $OPTARG\" &gt;&amp;2 USER_POPULATE_FILE=\"$OPTARG\" ;; w) #echo \"-w was triggered, Parameter: $OPTARG\" &gt;&amp;2 USER_REFRESH_FILE=\"$OPTARG\" ;; y) #echo \"-y was triggered, Parameter: $OPTARG\" &gt;&amp;2 USER_QUERY_PARAMS_FILE=\"$OPTARG\" ;; z) #echo \"-z was triggered, Parameter: $OPTARG\" &gt;&amp;2 USER_ENGINE_SETTINGS_FILE=\"$OPTARG\" ;; # driver options a) #echo \"-a was triggered, Parameter: $OPTARG\" &gt;&amp;2 export USER_PRETEND_MODE=\"1\" ;; b) #echo \"-b was triggered, Parameter: $OPTARG\" &gt;&amp;2 export USER_PRINT_STD_OUT=\"1\" ;; i) #echo \"-i was triggered, Parameter: $OPTARG\" &gt;&amp;2 export USER_DRIVER_WORKLOAD=\"$OPTARG\" ;; j) #echo \"-j was triggered, Parameter: $OPTARG\" &gt;&amp;2 export USER_DRIVER_QUERIES_TO_RUN=\"$OPTARG\" ;; \\?) echo \"Invalid option: -$OPTARG\" &gt;&amp;2 exit 1 ;; :) echo \"Option -$OPTARG requires an argument.\" &gt;&amp;2 exit 1 ;; esacdone 6.设置全局变量 如果用户指定了某个参数的值，则采用该值，否则使用默认值。 export BIG_BENCH_EXPERT_MODE=\"$&#123;USER_EXPERT_MODE:-\"0\"&#125;\"export SHOW_HELP=\"$&#123;SHOW_HELP:-\"0\"&#125;\"export BIG_BENCH_DATABASE=\"$&#123;USER_DATABASE:-\"$BIG_BENCH_DEFAULT_DATABASE\"&#125;\"export BIG_BENCH_ENGINE=\"$&#123;USER_ENGINE:-\"$BIG_BENCH_DEFAULT_ENGINE\"&#125;\"export BIG_BENCH_MAP_TASKS=\"$&#123;USER_MAP_TASKS:-\"$BIG_BENCH_DEFAULT_MAP_TASKS\"&#125;\"export BIG_BENCH_SCALE_FACTOR=\"$&#123;USER_SCALE_FACTOR:-\"$BIG_BENCH_DEFAULT_SCALE_FACTOR\"&#125;\"export BIG_BENCH_NUMBER_OF_PARALLEL_STREAMS=\"$&#123;USER_NUMBER_OF_PARALLEL_STREAMS:-\"$BIG_BENCH_DEFAULT_NUMBER_OF_PARALLEL_STREAMS\"&#125;\"export BIG_BENCH_BENCHMARK_PHASE=\"$&#123;USER_BENCHMARK_PHASE:-\"$BIG_BENCH_DEFAULT_BENCHMARK_PHASE\"&#125;\"export BIG_BENCH_STREAM_NUMBER=\"$&#123;USER_STREAM_NUMBER:-\"0\"&#125;\"export BIG_BENCH_ENGINE_DIR=\"$BIG_BENCH_HOME/engines/$BIG_BENCH_ENGINE\"export BIG_BENCH_ENGINE_CONF_DIR=\"$BIG_BENCH_ENGINE_DIR/conf\" 7.检测 -s -m -f -j的选项是否为数字 if [ -n \"`echo \"$BIG_BENCH_MAP_TASKS\" | sed -e 's/[0-9]*//g'`\" ]then echo \"$BIG_BENCH_MAP_TASKS is not a number\"fiif [ -n \"`echo \"$BIG_BENCH_SCALE_FACTOR\" | sed -e 's/[0-9]*//g'`\" ]then echo \"$BIG_BENCH_SCALE_FACTOR is not a number\"fiif [ -n \"`echo \"$BIG_BENCH_NUMBER_OF_PARALLEL_STREAMS\" | sed -e 's/[0-9]*//g'`\" ]then echo \"$BIG_BENCH_NUMBER_OF_PARALLEL_STREAMS is not a number\"fiif [ -n \"`echo \"$BIG_BENCH_STREAM_NUMBER\" | sed -e 's/[0-9]*//g'`\" ]then echo \"$BIG_BENCH_STREAM_NUMBER is not a number\"fi 8.检查引擎是否存在 if [ ! -d \"$BIG_BENCH_ENGINE_DIR\" ]then echo \"Engine directory $BIG_BENCH_ENGINE_DIR not found. Aborting script...\" exit 1fiif [ ! -d \"$BIG_BENCH_ENGINE_CONF_DIR\" ]then echo \"Engine configuration directory $BIG_BENCH_ENGINE_CONF_DIR not found. Aborting script...\" exit 1fi 9.设置 engineSetting.conf 路径并 source ENGINE_SETTINGS=\"$BIG_BENCH_ENGINE_CONF_DIR/engineSettings.conf\"if [ ! -f \"$ENGINE_SETTINGS\" ]then echo \"Engine settings file $ENGINE_SETTINGS not found\" exit 1else source \"$ENGINE_SETTINGS\"fi 10.检查module是否存在 当输入某个module时，系统会先到$BIG_BENCH_ENGINE_BIN_DIR/目录下去找该module是否存在，如果存在，就source &quot;$MODULE&quot;；如果该目录下不存在指定的module，再到export MODULE=&quot;$BIG_BENCH_BIN_DIR/&quot;目录下找该module，如果存在，就source &quot;$MODULE&quot;；否则，输出Module $MODULE not found, aborting script. export MODULE=\"$BIG_BENCH_ENGINE_BIN_DIR/$MODULE_NAME\"if [ -f \"$MODULE\" ]then source \"$MODULE\"else export MODULE=\"$BIG_BENCH_BIN_DIR/$MODULE_NAME\"if [ -f \"$MODULE\" ]then source \"$MODULE\"else echo \"Module $MODULE not found, aborting script.\" exit 1fifi 11.检查module里的runModule（）、helpModule ( )、runEngineCmd()方法是否有定义 MODULE_RUN_METHOD=\"runModule\"if ! declare -F \"$MODULE_RUN_METHOD\" &gt; /dev/null 2&gt;&amp;1then echo \"$MODULE_RUN_METHOD was not implemented, aborting script\" exit 1fi 12.运行module 如果module是runBenchmark，执行runCmdWithErrorCheck &quot;$MODULE_RUN_METHOD&quot;也就是runCmdWithErrorCheck runModule（） 由上可以看出，bigBench脚本主要执行一些如设置环境变量、赋予权限、检查并解析输入参数等基础工作，最终调用runBenchmark的runModule()方法继续往下执行。 runBenchmark接下来看看runBenchmark脚本。 runBenchmark里有两个函数：helpModule ()和runModule ()。 helpModule ()就是显示帮助。 runModule ()是运行runBenchmark模块时真正调用的函数。该函数主要做四件事： 清除之前生成的日志 调用RunBigBench.jar来执行 logEnvInformation 将日志文件夹打包成zip 源码如下： runModule () &#123; #check input parameters if [ \"$BIG_BENCH_NUMBER_OF_PARALLEL_STREAMS\" -le 0 ] then echo \"The number of parallel streams -s must be greater than 0\" return 1 fi \"$&#123;BIG_BENCH_BIN_DIR&#125;/bigBench\" cleanLogs -U $LIST_OF_USER_OPTIONS \"$BIG_BENCH_JAVA\" -jar \"$&#123;BIG_BENCH_TOOLS_DIR&#125;/RunBigBench.jar\" \"$&#123;BIG_BENCH_BIN_DIR&#125;/bigBench\" logEnvInformation -U $LIST_OF_USER_OPTIONS \"$&#123;BIG_BENCH_BIN_DIR&#125;/bigBench\" zipLogs -U $LIST_OF_USER_OPTIONS return $?&#125; 相当于运行runBenchmark模块时又调用了cleanLogs、logEnvInformation、zipLogs三个模块以及RunBigBench.jar。其中RunBigBench.jar是TCPx-BB测试执行的核心代码，用Java语言编写。接下来分析RunBigBench.jar源码。 runModule()runModule()函数用来执行某个module。我们已知，执行某个module需要切换到主目录下的bin目录，然后执行： ./bigBench module_name arguments 在runModule()函数里，cmdLine用来生成如上命令。 ArrayList cmdLine = new ArrayList();cmdLine.add(\"bash\");cmdLine.add(this.runScript);cmdLine.add(benchmarkPhase.getRunModule());cmdLine.addAll(arguments); 其中，this.runScript为： this.runScript = (String)env.get(\"BIG_BENCH_BIN_DIR\") + \"/bigBench\"; benchmarkPhase.getRunModule()用来获得需要执行的module。 arguments为用户输入的参数。 至此，cmdLine为： bash $BIG_BENCH_BIN_DIR/bigBench module_name arguments 那么，怎么让系统执行该bash命令呢？答案是调用runCmd()方法。 boolean successful = this.runCmd(this.homeDir, benchmarkPhase.isPrintStdOut(), (String[])cmdLine.toArray(new String[0])); 接下来介绍rumCmd()方法 runCmd()runCmd()方法通过ProcessBuilder来创建一个操作系统进程，并用该进程执行以上的bash命令。 ProcessBuilder还可以设置工作目录和环境。 ProcessBuilder pb = new ProcessBuilder(command);pb.directory(new File(workingDirectory));Process p = null;---p = pb.start(); getQueryList()getQueryList()用来获得需要执行的查询列表。从$BIG_BENCH_LOGS_DIR/bigBench.properties文件中读取。与$BIG_BENCH_HOME/conf/bigBench.properties内容一致。 bigBench.properties里power_test_0=1-30规定了powter_test_0阶段需要执行的查询及其顺序。 可以用区间如 5-12 或者单个数字如 21 表示，中间用 , 隔开。 power_test_0=28-25,2-5,10,22,30 表示powter_test_0阶段需要执行的查询及其顺序为：25,26,27,28,2,3,4,5,10,22,30 如果想让30个查询按顺序执行，则： power_test_0=1-30 获得查询列表的源码如下： private List&lt;Integer&gt; getQueryList(BigBench.BenchmarkPhase benchmarkPhase, int streamNumber) &#123; String SHUFFLED_NAME_PATTERN = \"shuffledQueryList\"; BigBench.BenchmarkPhase queryOrderBasicPhase = BigBench.BenchmarkPhase.POWER_TEST; String propertyKey = benchmarkPhase.getQueryListProperty(streamNumber); boolean queryOrderCached = benchmarkPhase.isQueryOrderCached(); if(queryOrderCached &amp;&amp; this.queryListCache.containsKey(propertyKey)) &#123; return new ArrayList((Collection)this.queryListCache.get(propertyKey)); &#125; else &#123; Object queryList; String basicPhaseNamePattern; if(!this.properties.containsKey(propertyKey)) &#123; if(benchmarkPhase.isQueryOrderRandom()) &#123; if(!this.queryListCache.containsKey(\"shuffledQueryList\")) &#123; basicPhaseNamePattern = queryOrderBasicPhase.getQueryListProperty(0); if(!this.properties.containsKey(basicPhaseNamePattern)) &#123; throw new IllegalArgumentException(\"Property \" + basicPhaseNamePattern + \" is not defined, but is the basis for shuffling the query list.\"); &#125; this.queryListCache.put(\"shuffledQueryList\", this.getQueryList(queryOrderBasicPhase, 0)); &#125; queryList = (List)this.queryListCache.get(\"shuffledQueryList\"); this.shuffleList((List)queryList, this.rnd); &#125; else &#123; queryList = this.getQueryList(queryOrderBasicPhase, 0); &#125; &#125; else &#123; queryList = new ArrayList(); String[] var11; int var10 = (var11 = this.properties.getProperty(propertyKey).split(\",\")).length; label65: for(int var9 = 0; var9 &lt; var10; ++var9) &#123; basicPhaseNamePattern = var11[var9]; String[] queryRange = basicPhaseNamePattern.trim().split(\"-\"); switch(queryRange.length) &#123; case 1: ((List)queryList).add(Integer.valueOf(Integer.parseInt(queryRange[0].trim()))); break; case 2: int startQuery = Integer.parseInt(queryRange[0]); int endQuery = Integer.parseInt(queryRange[1]); int i; if(startQuery &gt; endQuery) &#123; i = startQuery; while(true) &#123; if(i &lt; endQuery) &#123; continue label65; &#125; ((List)queryList).add(Integer.valueOf(i)); --i; &#125; &#125; else &#123; i = startQuery; while(true) &#123; if(i &gt; endQuery) &#123; continue label65; &#125; ((List)queryList).add(Integer.valueOf(i)); ++i; &#125; &#125; default: throw new IllegalArgumentException(\"Query numbers must be in the form X or X-Y, comma separated.\"); &#125; &#125; &#125; if(queryOrderCached) &#123; this.queryListCache.put(propertyKey, new ArrayList((Collection)queryList)); &#125; return new ArrayList((Collection)queryList); &#125;&#125; parseEnvironment()parseEnvironment()读取系统的环境变量并解析。 Map env = System.getenv();this.version = (String)env.get(\"BIG_BENCH_VERSION\");this.homeDir = (String)env.get(\"BIG_BENCH_HOME\");this.confDir = (String)env.get(\"BIG_BENCH_CONF_DIR\");this.runScript = (String)env.get(\"BIG_BENCH_BIN_DIR\") + \"/bigBench\";this.datagenDir = (String)env.get(\"BIG_BENCH_DATA_GENERATOR_DIR\");this.logDir = (String)env.get(\"BIG_BENCH_LOGS_DIR\");this.dataGenLogFile = (String)env.get(\"BIG_BENCH_DATAGEN_STAGE_LOG\");this.loadLogFile = (String)env.get(\"BIG_BENCH_LOADING_STAGE_LOG\");this.engine = (String)env.get(\"BIG_BENCH_ENGINE\");this.database = (String)env.get(\"BIG_BENCH_DATABASE\");this.mapTasks = (String)env.get(\"BIG_BENCH_MAP_TASKS\");this.numberOfParallelStreams = Integer.parseInt((String)env.get(\"BIG_BENCH_NUMBER_OF_PARALLEL_STREAMS\"));this.scaleFactor = Long.parseLong((String)env.get(\"BIG_BENCH_SCALE_FACTOR\"));this.stopAfterFailure = ((String)env.get(\"BIG_BENCH_STOP_AFTER_FAILURE\")).equals(\"1\"); 并自动在用户指定的参数后面加上 -U (解锁专家模式) this.userArguments.add(\"-U\"); 如果用户指定了 PRETEND_MODE、PRINT_STD_OUT、WORKLOAD、QUERIES_TO_RUN，则以用户指定的参数为准，否则使用默认值。 if(env.containsKey(\"USER_PRETEND_MODE\")) &#123; this.properties.setProperty(\"pretend_mode\", (String)env.get(\"USER_PRETEND_MODE\"));&#125;if(env.containsKey(\"USER_PRINT_STD_OUT\")) &#123; this.properties.setProperty(\"show_command_stdout\", (String)env.get(\"USER_PRINT_STD_OUT\"));&#125;if(env.containsKey(\"USER_DRIVER_WORKLOAD\")) &#123; this.properties.setProperty(\"workload\", (String)env.get(\"USER_DRIVER_WORKLOAD\"));&#125;if(env.containsKey(\"USER_DRIVER_QUERIES_TO_RUN\")) &#123; this.properties.setProperty(BigBench.BenchmarkPhase.POWER_TEST.getQueryListProperty(0), (String)env.get(\"USER_DRIVER_QUERIES_TO_RUN\"));&#125; 读取 workload 并赋值 benchmarkPhases。如果 workload 里不包含 BENCHMARK_START 和 BENCHMARK_STOP，自动在 benchmarkPhases 的首位和末位分别加上 BENCHMARK_START 和 BENCHMARK_STOP。 this.benchmarkPhases = new ArrayList();Iterator var7 = Arrays.asList(this.properties.getProperty(\"workload\").split(\",\")).iterator(); while(var7.hasNext()) &#123; String benchmarkPhase = (String)var7.next(); this.benchmarkPhases.add(BigBench.BenchmarkPhase.valueOf(benchmarkPhase.trim()));&#125; if(!this.benchmarkPhases.contains(BigBench.BenchmarkPhase.BENCHMARK_START)) &#123; this.benchmarkPhases.add(0, BigBench.BenchmarkPhase.BENCHMARK_START);&#125; if(!this.benchmarkPhases.contains(BigBench.BenchmarkPhase.BENCHMARK_STOP)) &#123; this.benchmarkPhases.add(BigBench.BenchmarkPhase.BENCHMARK_STOP);&#125; run()run() 方法是 RunBigBench.jar 里核心的方法。所有的执行都是通过 run() 方法调用的。比如 runQueries()、runModule()、generateData()等。runQueries()、runModule()、generateData() 又通过调用 runCmd() 方法来创建操作系统进程，执行bash命令，调用bash脚本。 run() 方法里通过一个 while 循环来逐一执行 workload 里的每一个 benchmarkPhase。 不同的 benchmarkPhase 会调用 runQueries()、runModule()、generateData()…中的不同方法。 try &#123;long e = 0L;this.log.finer(\"Benchmark phases: \" + this.benchmarkPhases);Iterator startCheckpoint = this.benchmarkPhases.iterator();long throughputStart;while(startCheckpoint.hasNext()) &#123; BigBench.BenchmarkPhase children = (BigBench.BenchmarkPhase)startCheckpoint.next(); if(children.isPhaseDone()) &#123; this.log.info(\"The phase \" + children.name() + \" was already performed earlier. Skipping this phase\"); &#125; else &#123; try &#123; switch($SWITCH_TABLE$io$bigdatabenchmark$v1$driver$BigBench$BenchmarkPhase()[children.ordinal()]) &#123; case 1: case 20: throw new IllegalArgumentException(\"The value \" + children.name() + \" is only used internally.\"); case 2: this.log.info(children.getConsoleMessage()); e = System.currentTimeMillis(); break; case 3: if(!BigBench.BenchmarkPhase.BENCHMARK_START.isPhaseDone()) &#123; throw new IllegalArgumentException(\"Error: Cannot stop the benchmark before starting it\"); &#125; throughputStart = System.currentTimeMillis(); this.log.info(String.format(\"%-55s finished. Time: %25s\", new Object[]&#123;children.getConsoleMessage(), BigBench.Helper.formatTime(throughputStart - e)&#125;)); this.logTreeRoot.setCheckpoint(new BigBench.Checkpoint(BigBench.BenchmarkPhase.BENCHMARK, -1L, -1L, e, throughputStart, this.logTreeRoot.isSuccessful())); break; case 4: case 15: case 18: case 22: case 27: case 28: case 29: this.runModule(children, this.userArguments); break; case 5: case 10: case 11: this.runQueries(children, 1, validationArguments); break; case 6: case 9: this.runModule(children, validationArguments); break; case 7: this.generateData(children, false, validationArguments); break; case 8: this.generateData(children, true, validationArguments); break; case 12: case 19: case 24: this.runQueries(children, 1, this.userArguments); break; case 13: case 14: case 21: case 23: case 25: case 26: this.runQueries(children, this.numberOfParallelStreams, this.userArguments); break; case 16: this.generateData(children, false, this.userArguments); break; case 17: this.generateData(children, true, this.userArguments); &#125; children.setPhaseDone(true); &#125; catch (IOException var21) &#123; this.log.info(\"==============\\nBenchmark run terminated\\nReason: An error occured while running a command in phase \" + children + \"\\n==============\"); var21.printStackTrace(); if(this.stopAfterFailure || children.mustSucceed()) &#123; break; &#125; &#125; &#125;&#125; 这里的 case 1-29 并不是 1-29 条查询，而是枚举类型里的 1-29 个 benmarkPhase 。如下所示： private static enum BenchmarkPhase &#123;BENCHMARK((String)null, \"benchmark\", false, false, false, false, \"BigBench benchmark\"),BENCHMARK_START((String)null, \"benchmark_start\", false, false, false, false, \"BigBench benchmark: Start\"),BENCHMARK_STOP((String)null, \"benchmark_stop\", false, false, false, false, \"BigBench benchmark: Stop\"),CLEAN_ALL(\"cleanAll\", \"clean_all\", false, false, false, false, \"BigBench clean all\"),ENGINE_VALIDATION_CLEAN_POWER_TEST(\"cleanQuery\", \"engine_validation_power_test\", false, false, false, false, \"BigBench engine validation: Clean power test queries\"),ENGINE_VALIDATION_CLEAN_LOAD_TEST(\"cleanMetastore\", \"engine_validation_metastore\", false, false, false, false, \"BigBench engine validation: Clean metastore\"),ENGINE_VALIDATION_CLEAN_DATA(\"cleanData\", \"engine_validation_data\", false, false, false, false, \"BigBench engine validation: Clean data\"),ENGINE_VALIDATION_DATA_GENERATION(\"dataGen\", \"engine_validation_data\", false, false, false, true, \"BigBench engine validation: Data generation\"),ENGINE_VALIDATION_LOAD_TEST(\"populateMetastore\", \"engine_validation_metastore\", false, false, false, true, \"BigBench engine validation: Populate metastore\"),ENGINE_VALIDATION_POWER_TEST(\"runQuery\", \"engine_validation_power_test\", false, false, false, false, \"BigBench engine validation: Power test\"),ENGINE_VALIDATION_RESULT_VALIDATION(\"validateQuery\", \"engine_validation_power_test\", false, false, true, false, \"BigBench engine validation: Check all query results\"),CLEAN_POWER_TEST(\"cleanQuery\", \"power_test\", false, false, false, false, \"BigBench clean: Clean power test queries\"),CLEAN_THROUGHPUT_TEST_1(\"cleanQuery\", \"throughput_test_1\", false, false, false, false, \"BigBench clean: Clean first throughput test queries\"),CLEAN_THROUGHPUT_TEST_2(\"cleanQuery\", \"throughput_test_2\", false, false, false, false, \"BigBench clean: Clean second throughput test queries\"),CLEAN_LOAD_TEST(\"cleanMetastore\", \"metastore\", false, false, false, false, \"BigBench clean: Load test\"),CLEAN_DATA(\"cleanData\", \"data\", false, false, false, false, \"BigBench clean: Data\"),DATA_GENERATION(\"dataGen\", \"data\", false, false, false, true, \"BigBench preparation: Data generation\"),LOAD_TEST(\"populateMetastore\", \"metastore\", false, false, false, true, \"BigBench phase 1: Load test\"),POWER_TEST(\"runQuery\", \"power_test\", false, true, false, false, \"BigBench phase 2: Power test\"),THROUGHPUT_TEST((String)null, \"throughput_test\", false, false, false, false, \"BigBench phase 3: Throughput test\"),THROUGHPUT_TEST_1(\"runQuery\", \"throughput_test_1\", true, true, false, false, \"BigBench phase 3: First throughput test run\"),THROUGHPUT_TEST_REFRESH(\"refreshMetastore\", \"throughput_test_refresh\", false, false, false, false, \"BigBench phase 3: Throughput test data refresh\"),THROUGHPUT_TEST_2(\"runQuery\", \"throughput_test_2\", true, true, false, false, \"BigBench phase 3: Second throughput test run\"),VALIDATE_POWER_TEST(\"validateQuery\", \"power_test\", false, false, true, false, \"BigBench validation: Power test results\"),VALIDATE_THROUGHPUT_TEST_1(\"validateQuery\", \"throughput_test_1\", false, false, true, false, \"BigBench validation: First throughput test results\"),VALIDATE_THROUGHPUT_TEST_2(\"validateQuery\", \"throughput_test_2\", false, false, true, false, \"BigBench validation: Second throughput test results\"),SHOW_TIMES(\"showTimes\", \"show_times\", false, false, true, false, \"BigBench: show query times\"),SHOW_ERRORS(\"showErrors\", \"show_errors\", false, false, true, false, \"BigBench: show query errors\"),SHOW_VALIDATION(\"showValidation\", \"show_validation\", false, false, true, false, \"BigBench: show query validation results\");private String runModule;private String namePattern;private boolean queryOrderRandom;private boolean queryOrderCached;private boolean printStdOut;private boolean mustSucceed;private String consoleMessage;private boolean phaseDone;private BenchmarkPhase(String runModule, String namePattern, boolean queryOrderRandom, boolean queryOrderCached, boolean printStdOut, boolean mustSucceed, String consoleMessage) &#123; this.runModule = runModule; this.namePattern = namePattern; this.queryOrderRandom = queryOrderRandom; this.queryOrderCached = queryOrderCached; this.printStdOut = printStdOut; this.mustSucceed = mustSucceed; this.consoleMessage = consoleMessage; this.phaseDone = false;&#125; 3对应 BENCHMARK_STOP，4对应 CLEAN_ALL,29对应 SHOW_VALIDATION，依此类推… 可以看出： CLEAN_ALL、CLEAN_LOAD_TEST、LOAD_TEST、THROUGHPUT_TEST_REFRESH、SHOW_TIMES、SHOW_ERRORS、SHOW_VALIDATION等benchmarkPhases调用的是 this.runModule(children, this.userArguments); 方法是 runModule ，参数是 this.userArguments。 ENGINE_VALIDATION_CLEAN_POWER_TEST、ENGINE_VALIDATION_POWER_TEST、ENGINE_VALIDATION_RESULT_VALIDATION 调用的是 this.runQueries(children, 1, validationArguments); 方法是 runQueries ，参数是 1(stream number) 和 validationArguments。 ENGINE_VALIDATION_CLEAN_LOAD_TEST 和 ENGINE_VALIDATION_LOAD_TEST 调用的是 this.runModule(children, validationArguments); ENGINE_VALIDATION_CLEAN_DATA 调用的是 this.generateData(children, false, validationArguments); ENGINE_VALIDATION_DATA_GENERATION 调用的是 this.generateData(children, true, validationArguments); CLEAN_POWER_TEST、POWER_TEST、VALIDATE_POWER_TEST 调用的是 this.runQueries(children, 1, this.userArguments); CLEAN_THROUGHPUT_TEST_1``CLEAN_THROUGHPUT_TEST_2``THROUGHPUT_TEST_1``THROUGHPUT_TEST_2``VALIDATE_THROUGHPUT_TEST_1 VALIDATE_THROUGHPUT_TEST_2 调用的是 this.runQueries(children, this.numberOfParallelStreams, this.userArguments); CLEAN_DATA 调用的是 this.generateData(children, false, this.userArguments); DATA_GENERATION 调用的是 this.generateData(children, true, this.userArguments); 总结一下以上的方法调用可以发现： 跟 ENGINE_VALIDATION 相关的benchmarkPhase用的参数都是 validationArguments。其余用的是 userArguments 跟 POWER_TEST 相关的都是调用 runQueries() 方法，因为 POWER_TEST 就是执行SQL查询 跟 CLEAN_DATA DATA_GENERATION 相关的都是调用 generateData() 方法 跟 LOAD_TEST SHOW 相关的都是调用 runModule() 方法 benchmarkPhase 和 module 对应关系具体每个 benchmarkPhase 跟 module（执行的脚本）的对应关系如下： CLEAN_ALL -&gt; \"cleanAll\"ENGINE_VALIDATION_CLEAN_POWER_TEST -&gt; \"cleanQuery\"ENGINE_VALIDATION_CLEAN_LOAD_TEST -&gt; \"cleanMetastore\",ENGINE_VALIDATION_CLEAN_DATA -&gt; \"cleanData\"ENGINE_VALIDATION_DATA_GENERATION -&gt; \"dataGen\"ENGINE_VALIDATION_LOAD_TEST -&gt; \"populateMetastore\"ENGINE_VALIDATION_POWER_TEST -&gt; \"runQuery\"ENGINE_VALIDATION_RESULT_VALIDATION -&gt; \"validateQuery\"CLEAN_POWER_TEST -&gt; \"cleanQuery\"CLEAN_THROUGHPUT_TEST_1 -&gt; \"cleanQuery\"CLEAN_THROUGHPUT_TEST_2 -&gt; \"cleanQuery\"CLEAN_LOAD_TEST -&gt; \"cleanMetastore\"CLEAN_DATA -&gt; \"cleanData\"DATA_GENERATION -&gt; \"dataGen\"LOAD_TEST -&gt; \"populateMetastore\"POWER_TEST -&gt; \"runQuery\"THROUGHPUT_TEST -&gt; (String)nullTHROUGHPUT_TEST_1 -&gt; \"runQuery\"THROUGHPUT_TEST_REFRESH -&gt; \"refreshMetastore\"THROUGHPUT_TEST_2 -&gt; \"runQuery\"VALIDATE_POWER_TEST -&gt; \"validateQuery\"VALIDATE_THROUGHPUT_TEST_1 -&gt; \"validateQuery\"VALIDATE_THROUGHPUT_TEST_2 -&gt; \"validateQuery\"SHOW_TIMES -&gt; \"showTimes\"SHOW_ERRORS -&gt; \"showErrors\"SHOW_VALIDATION -&gt; \"showValidation\" 当执行某个 benchmarkPhase 时会去调用如上该 benchmarkPhase 对应的 module （脚本位于 $BENCH_MARK_HOME/engines/hive/bin 目录下） cmdLine.add(benchmarkPhase.getRunModule()); 程序调用流程 接下来介绍每个module的功能 modulecleanAll1. DROP DATABASE 2. 删除hdfs上的源数据 echo \"dropping database (with all tables)\"runCmdWithErrorCheck runEngineCmd -e \"DROP DATABASE IF EXISTS $BIG_BENCH_DATABASE CASCADE;\"echo \"cleaning $&#123;BIG_BENCH_HDFS_ABSOLUTE_HOME&#125;\"hadoop fs -rm -r -f -skipTrash \"$&#123;BIG_BENCH_HDFS_ABSOLUTE_HOME&#125;\" cleanQuery1. 删除对应的 Query 生成的临时表 2. 删除对应的 Query 生成的结果表 runCmdWithErrorCheck runEngineCmd -e \"DROP TABLE IF EXISTS $TEMP_TABLE1; DROP TABLE IF EXISTS $TEMP_TABLE2; DROP TABLE IF EXISTS $RESULT_TABLE;\"return $? cleanMetastore1. 调用 `dropTables.sql` 将23张表依次DROP echo \"cleaning metastore tables\"runCmdWithErrorCheck runEngineCmd -f \"$BIG_BENCH_CLEAN_METASTORE_FILE\" export BIG_BENCH_CLEAN_METASTORE_FILE=\"$BIG_BENCH_CLEAN_DIR/dropTables.sql\" dropTables.sql 将23张表依次DROP,源码如下： DROP TABLE IF EXISTS $&#123;hiveconf:customerTableName&#125;;DROP TABLE IF EXISTS $&#123;hiveconf:customerAddressTableName&#125;;DROP TABLE IF EXISTS $&#123;hiveconf:customerDemographicsTableName&#125;;DROP TABLE IF EXISTS $&#123;hiveconf:dateTableName&#125;;DROP TABLE IF EXISTS $&#123;hiveconf:householdDemographicsTableName&#125;;DROP TABLE IF EXISTS $&#123;hiveconf:incomeTableName&#125;;DROP TABLE IF EXISTS $&#123;hiveconf:itemTableName&#125;;DROP TABLE IF EXISTS $&#123;hiveconf:promotionTableName&#125;;DROP TABLE IF EXISTS $&#123;hiveconf:reasonTableName&#125;;DROP TABLE IF EXISTS $&#123;hiveconf:shipModeTableName&#125;;DROP TABLE IF EXISTS $&#123;hiveconf:storeTableName&#125;;DROP TABLE IF EXISTS $&#123;hiveconf:timeTableName&#125;;DROP TABLE IF EXISTS $&#123;hiveconf:warehouseTableName&#125;;DROP TABLE IF EXISTS $&#123;hiveconf:webSiteTableName&#125;;DROP TABLE IF EXISTS $&#123;hiveconf:webPageTableName&#125;;DROP TABLE IF EXISTS $&#123;hiveconf:inventoryTableName&#125;;DROP TABLE IF EXISTS $&#123;hiveconf:storeSalesTableName&#125;;DROP TABLE IF EXISTS $&#123;hiveconf:storeReturnsTableName&#125;;DROP TABLE IF EXISTS $&#123;hiveconf:webSalesTableName&#125;;DROP TABLE IF EXISTS $&#123;hiveconf:webReturnsTableName&#125;;DROP TABLE IF EXISTS $&#123;hiveconf:marketPricesTableName&#125;;DROP TABLE IF EXISTS $&#123;hiveconf:clickstreamsTableName&#125;;DROP TABLE IF EXISTS $&#123;hiveconf:reviewsTableName&#125;; cleanData1. 删除hdfs上 /user/root/benchmarks/bigbench/data 目录 2. 删除hdfs上 /user/root/benchmarks/bigbench/data_refresh 目录 echo \"cleaning $&#123;BIG_BENCH_HDFS_ABSOLUTE_INIT_DATA_DIR&#125;\"hadoop fs -rm -r -f -skipTrash \"$&#123;BIG_BENCH_HDFS_ABSOLUTE_INIT_DATA_DIR&#125;\"echo \"cleaning $&#123;BIG_BENCH_HDFS_ABSOLUTE_REFRESH_DATA_DIR&#125;\"hadoop fs -rm -r -f -skipTrash \"$&#123;BIG_BENCH_HDFS_ABSOLUTE_REFRESH_DATA_DIR&#125;\"``` ### dataGen 1. 创建目录 /user/root/benchmarks/bigbench/data 并赋予权限 2. 创建目录 /user/root/benchmarks/bigbench/data_refresh 并赋予权限 3. 调用 HadoopClusterExec.jar 和 pdgf.jar 生成 base data 到 /user/root/benchmarks/bigbench/data 目录下 4. 调用 HadoopClusterExec.jar 和 pdgf.jar 生成 refresh data 到 /user/root/benchmarks/bigbench/data_refresh 目录下创建目录 /user/root/benchmarks/bigbench/data 并赋予权限```bashrunCmdWithErrorCheck hadoop fs -mkdir -p \"$&#123;BIG_BENCH_HDFS_ABSOLUTE_INIT_DATA_DIR&#125;\"runCmdWithErrorCheck hadoop fs -chmod 777 \"$&#123;BIG_BENCH_HDFS_ABSOLUTE_INIT_DATA_DIR&#125;\" 创建目录 /user/root/benchmarks/bigbench/data_refresh 并赋予权限 runCmdWithErrorCheck hadoop fs -mkdir -p \"$&#123;BIG_BENCH_HDFS_ABSOLUTE_REFRESH_DATA_DIR&#125;\"runCmdWithErrorCheck hadoop fs -chmod 777 \"$&#123;BIG_BENCH_HDFS_ABSOLUTE_REFRESH_DATA_DIR&#125;\" 调用 HadoopClusterExec.jar 和 pdgf.jar 生成 base data runCmdWithErrorCheck hadoop jar \"$&#123;BIG_BENCH_TOOLS_DIR&#125;/HadoopClusterExec.jar\" -archives \"$&#123;PDGF_ARCHIVE_PATH&#125;\" $&#123;BIG_BENCH_DATAGEN_HADOOP_EXEC_DEBUG&#125; -taskFailOnNonZeroReturnValue -execCWD \"$&#123;PDGF_DISTRIBUTED_NODE_DIR&#125;\" $&#123;HadoopClusterExecOptions&#125; -exec $&#123;BIG_BENCH_DATAGEN_HADOOP_JVM_ENV&#125; -cp \"$&#123;HADOOP_CP&#125;:pdgf.jar\" $&#123;PDGF_CLUSTER_CONF&#125; pdgf.Controller -nc HadoopClusterExec.tasks -nn HadoopClusterExec.taskNumber -ns -c -sp REFRESH_PHASE 0 -o \"'$&#123;BIG_BENCH_HDFS_ABSOLUTE_INIT_DATA_DIR&#125;/'+table.getName()+'/'\" $&#123;BIG_BENCH_DATAGEN_HADOOP_OPTIONS&#125; -s $&#123;BIG_BENCH_DATAGEN_TABLES&#125; $&#123;PDGF_OPTIONS&#125; \"$@\" 2&gt;&amp;1 | tee -a \"$BIG_BENCH_DATAGEN_STAGE_LOG\" 2&gt;&amp;1 调用 HadoopClusterExec.jar 和 pdgf.jar 生成 refresh data runCmdWithErrorCheck hadoop jar \"$&#123;BIG_BENCH_TOOLS_DIR&#125;/HadoopClusterExec.jar\" -archives \"$&#123;PDGF_ARCHIVE_PATH&#125;\" $&#123;BIG_BENCH_DATAGEN_HADOOP_EXEC_DEBUG&#125; -taskFailOnNonZeroReturnValue -execCWD \"$&#123;PDGF_DISTRIBUTED_NODE_DIR&#125;\" $&#123;HadoopClusterExecOptions&#125; -exec $&#123;BIG_BENCH_DATAGEN_HADOOP_JVM_ENV&#125; -cp \"$&#123;HADOOP_CP&#125;:pdgf.jar\" $&#123;PDGF_CLUSTER_CONF&#125; pdgf.Controller -nc HadoopClusterExec.tasks -nn HadoopClusterExec.taskNumber -ns -c -sp REFRESH_PHASE 1 -o \"'$&#123;BIG_BENCH_HDFS_ABSOLUTE_REFRESH_DATA_DIR&#125;/'+table.getName()+'/'\" $&#123;BIG_BENCH_DATAGEN_HADOOP_OPTIONS&#125; -s $&#123;BIG_BENCH_DATAGEN_TABLES&#125; $&#123;PDGF_OPTIONS&#125; \"$@\" 2&gt;&amp;1 | tee -a \"$BIG_BENCH_DATAGEN_STAGE_LOG\" 2&gt;&amp;1 populateMetastore 该过程是真正的创建数据库表的过程。建表的过程调用的是 $BENCH_MARK_HOME/engines/hive/population/ 目录下的 hiveCreateLoad.sql ,通过该sql文件来建数据库表。 从 /user/root/benchmarks/bigbench/data 路径下读取 .dat 的原始数据，生成 TEXTFILE 格式的外部临时表 用 select * from 临时表 来创建最终的 ORC 格式的数据库表 删除外部临时表。 从 /user/root/benchmarks/bigbench/data 路径下读取 .dat 的原始数据，生成 TEXTFILE 格式的外部临时表 DROP TABLE IF EXISTS $&#123;hiveconf:customerTableName&#125;$&#123;hiveconf:temporaryTableSuffix&#125;;CREATE EXTERNAL TABLE $&#123;hiveconf:customerTableName&#125;$&#123;hiveconf:temporaryTableSuffix&#125; ( c_customer_sk bigint --not null , c_customer_id string --not null , c_current_cdemo_sk bigint , c_current_hdemo_sk bigint , c_current_addr_sk bigint , c_first_shipto_date_sk bigint , c_first_sales_date_sk bigint , c_salutation string , c_first_name string , c_last_name string , c_preferred_cust_flag string , c_birth_day int , c_birth_month int , c_birth_year int , c_birth_country string , c_login string , c_email_address string , c_last_review_date string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '$&#123;hiveconf:fieldDelimiter&#125;' STORED AS TEXTFILE LOCATION '$&#123;hiveconf:hdfsDataPath&#125;/$&#123;hiveconf:customerTableName&#125;'; 用 select * from 临时表 来创建最终的 ORC 格式的数据库表 DROP TABLE IF EXISTS $&#123;hiveconf:customerTableName&#125;;CREATE TABLE $&#123;hiveconf:customerTableName&#125;STORED AS $&#123;hiveconf:tableFormat&#125;ASSELECT * FROM $&#123;hiveconf:customerTableName&#125;$&#123;hiveconf:temporaryTableSuffix&#125;; 删除外部临时表 DROP TABLE $&#123;hiveconf:customerTableName&#125;$&#123;hiveconf:temporaryTableSuffix&#125;; 总结下执行以下 TPCx-BB 测试命令代码的执行步骤: ./bigBench runBenchmark -f 1 -m 8 -s 2 -j 5 runQuery1. runQuery 调用每个query下的 run.sh 里的 `query_run_main_method()` 方法 2. `query_run_main_method()` 调用 `runEngineCmd` 来执行query脚本（qxx.sql） runQuery 调用每个query下的 run.sh 里的 query_run_main_method() 方法 QUERY_MAIN_METHOD=\"query_run_main_method\"-----------------------------------------\"$QUERY_MAIN_METHOD\" 2&gt;&amp;1 | tee -a \"$LOG_FILE_NAME\" 2&gt;&amp;1 query_run_main_method() 调用 runEngineCmd 来执行query脚本（qxx.sql） query_run_main_method () &#123; QUERY_SCRIPT=\"$QUERY_DIR/$QUERY_NAME.sql\" if [ ! -r \"$QUERY_SCRIPT\" ] then echo \"SQL file $QUERY_SCRIPT can not be read.\" exit 1 fi runCmdWithErrorCheck runEngineCmd -f \"$QUERY_SCRIPT\" return $?&#125; 一般情况下 query_run_main_method () 方法只是执行对应的query脚本，但是像 q05、q20… 这些查询，用到了机器学习算法，所以在执行对应的query脚本后会把生成的结果表作为输入，然后调用执行机器学习算法（如聚类、逻辑回归）的jar包继续执行，得到最终的结果。 runEngineCmd () &#123; if addInitScriptsToParams then \"$BINARY\" \"$&#123;BINARY_PARAMS[@]&#125;\" \"$&#123;INIT_PARAMS[@]&#125;\" \"$@\" else return 1 fi&#125;BINARY=\"/usr/bin/hive\"BINARY_PARAMS+=(--hiveconf BENCHMARK_PHASE=$BIG_BENCH_BENCHMARK_PHASE --hiveconf STREAM_NUMBER=$BIG_BENCH_STREAM_NUMBER --hiveconf QUERY_NAME=$QUERY_NAME --hiveconf QUERY_DIR=$QUERY_DIR --hiveconf RESULT_TABLE=$RESULT_TABLE --hiveconf RESULT_DIR=$RESULT_DIR --hiveconf TEMP_TABLE=$TEMP_TABLE --hiveconf TEMP_DIR=$TEMP_DIR --hiveconf TABLE_PREFIX=$TABLE_PREFIX)INIT_PARAMS=(-i \"$BIG_BENCH_QUERY_PARAMS_FILE\" -i \"$BIG_BENCH_ENGINE_SETTINGS_FILE\")INIT_PARAMS+=(-i \"$LOCAL_QUERY_ENGINE_SETTINGS_FILE\")if [ -n \"$USER_QUERY_PARAMS_FILE\" ]thenif [ -r \"$USER_QUERY_PARAMS_FILE\" ]then echo \"User defined query parameter file found. Adding $USER_QUERY_PARAMS_FILE to hive init.\" INIT_PARAMS+=(-i \"$USER_QUERY_PARAMS_FILE\")else echo \"User query parameter file $USER_QUERY_PARAMS_FILE can not be read.\" return 1fifiif [ -n \"$USER_ENGINE_SETTINGS_FILE\" ]thenif [ -r \"$USER_ENGINE_SETTINGS_FILE\" ]then echo \"User defined engine settings file found. Adding $USER_ENGINE_SETTINGS_FILE to hive init.\" INIT_PARAMS+=(-i \"$USER_ENGINE_SETTINGS_FILE\")else echo \"User hive settings file $USER_ENGINE_SETTINGS_FILE can not be read.\" return 1fifireturn 0 validateQuery1. 调用每个query下的 run.sh 里的 `query_run_validate_method()` 方法 2. `query_run_validate_method()` 比较 `$BENCH_MARK_HOME/engines/hive/queries/qxx/results/qxx-result` 和hdfs上 `/user/root/benchmarks/bigbench/queryResults/qxx_hive_${BIG_BENCH_BENCHMARK_PHASE}_${BIG_BENCH_STREAM_NUMBER}_result` 两个文件，如果一样，则验证通过，否则验证失败。 if diff -q \"$VALIDATION_RESULTS_FILENAME\" &lt;(hadoop fs -cat \"$RESULT_DIR/*\")then echo \"Validation of $VALIDATION_RESULTS_FILENAME passed: Query returned correct results\"else echo \"Validation of $VALIDATION_RESULTS_FILENAME failed: Query returned incorrect results\" VALIDATION_PASSED=\"0\"fi SF为1时(-f 1)，用上面的方法比较，SF不为1（&gt;1）时,只要hdfs上的结果表中行数大于等于1即验证通过 if [ `hadoop fs -cat \"$RESULT_DIR/*\" | head -n 10 | wc -l` -ge 1 ]then echo \"Validation passed: Query returned results\"else echo \"Validation failed: Query did not return results\" return 1fi refreshMetastore1. 调用 `$BENCH_MARK_HOME/engines/hive/refresh/` 目录下的 `hiveRefreshCreateLoad.sql` 脚本 2. `hiveRefreshCreateLoad.sql` 将hdfs上 `/user/root/benchmarks/bigbench/data_refresh/` 目录下每个表数据插入外部临时表 3. 外部临时表再将每个表的数据插入Hive数据库对应的表中 hiveRefreshCreateLoad.sql 将hdfs上 /user/root/benchmarks/bigbench/data_refresh/ 目录下每个表数据插入外部临时表 DROP TABLE IF EXISTS $&#123;hiveconf:customerTableName&#125;$&#123;hiveconf:temporaryTableSuffix&#125;;CREATE EXTERNAL TABLE $&#123;hiveconf:customerTableName&#125;$&#123;hiveconf:temporaryTableSuffix&#125; ( c_customer_sk bigint --not null , c_customer_id string --not null , c_current_cdemo_sk bigint , c_current_hdemo_sk bigint , c_current_addr_sk bigint , c_first_shipto_date_sk bigint , c_first_sales_date_sk bigint , c_salutation string , c_first_name string , c_last_name string , c_preferred_cust_flag string , c_birth_day int , c_birth_month int , c_birth_year int , c_birth_country string , c_login string , c_email_address string , c_last_review_date string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '$&#123;hiveconf:fieldDelimiter&#125;' STORED AS TEXTFILE LOCATION '$&#123;hiveconf:hdfsDataPath&#125;/$&#123;hiveconf:customerTableName&#125;';set hdfsDataPath=$&#123;env:BIG_BENCH_HDFS_ABSOLUTE_REFRESH_DATA_DIR&#125;;``` 外部临时表再将每个表的数据插入Hive数据库对应的表中```sqlINSERT INTO TABLE $&#123;hiveconf:customerTableName&#125;SELECT * FROM $&#123;hiveconf:customerTableName&#125;$&#123;hiveconf:temporaryTableSuffix&#125;; 附录在测试之前会先用PDGF（并行数据生成框架）生成数据。指定Scale Factor为1（1GB），生成以下共23张表。以下是每张表的建表语句及每张表里的某一条记录。 23张表hive&gt; show tables;OKcustomercustomer_addresscustomer_demographicsdate_dimhousehold_demographicsincome_bandinventoryitemitem_marketpricesproduct_reviewspromotionreasonship_modestorestore_returnsstore_salestime_dimwarehouseweb_clickstreamsweb_pageweb_returnsweb_salesweb_siteTime taken: 0.017 seconds, Fetched: 23 row(s) customer （99000行）4.03MB建表语句： hive&gt; show create table customer;OKCREATE TABLE `customer`( `c_customer_sk` bigint, `c_customer_id` string, `c_current_cdemo_sk` bigint, `c_current_hdemo_sk` bigint, `c_current_addr_sk` bigint, `c_first_shipto_date_sk` bigint, `c_first_sales_date_sk` bigint, `c_salutation` string, `c_first_name` string, `c_last_name` string, `c_preferred_cust_flag` string, `c_birth_day` int, `c_birth_month` int, `c_birth_year` int, `c_birth_country` string, `c_login` string, `c_email_address` string, `c_last_review_date` string)ROW FORMAT SERDE \\'org.apache.hadoop.hive.ql.io.orc.OrcSerde\\'STORED AS INPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\\'OUTPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\\'LOCATION \\'hdfs://mycluster/usr/hive/warehouse/bigbench.db/customer\\'TBLPROPERTIES ( \\'COLUMN_STATS_ACCURATE\\'=\\'true\\', \\'numFiles\\'=\\'2\\', \\'numRows\\'=\\'99000\\', \\'rawDataSize\\'=\\'88084062\\', \\'totalSize\\'=\\'4221267\\', \\'transient_lastDdlTime\\'=\\'1473167154\\')Time taken: 0.116 seconds, Fetched: 34 row(s) 某一行： 0 AAAAAAAAAAAAAAAA 1824793 3203 2555 28776 14690 Ms. Marisa Harrington N 17 4 1988 UNITED ARAB EMIRATES RRCyuY3XfE3a Marisa.Harrington@lawyer.com gdMmGdU9 customer_address（49500行） 0.92MB建表语句： hive&gt; show create table customer_address;OKCREATE TABLE `customer_address`( `ca_address_sk` bigint, `ca_address_id` string, `ca_street_number` string, `ca_street_name` string, `ca_street_type` string, `ca_suite_number` string, `ca_city` string, `ca_county` string, `ca_state` string, `ca_zip` string, `ca_country` string, `ca_gmt_offset` decimal(5,2), `ca_location_type` string)ROW FORMAT SERDE \\'org.apache.hadoop.hive.ql.io.orc.OrcSerde\\'STORED AS INPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\\'OUTPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\\'LOCATION \\'hdfs://mycluster/usr/hive/warehouse/bigbench.db/customer_address\\'TBLPROPERTIES ( \\'COLUMN_STATS_ACCURATE\\'=\\'true\\', \\'numFiles\\'=\\'2\\', \\'numRows\\'=\\'49500\\', \\'rawDataSize\\'=\\'55836000\\', \\'totalSize\\'=\\'966548\\', \\'transient_lastDdlTime\\'=\\'1473167160\\')Time taken: 0.043 seconds, Fetched: 29 row(s) 某一行： 6187 AAAAAAAAAAAAAJDZ 536 6th Lake Drive UKL8bE5C Lowell Brule County SD 18464 United States -6 apartment customer_demographics (19200800行）6.84MB建表语句： hive&gt; show create table customer_demographics;OKCREATE TABLE `customer_demographics`( `cd_demo_sk` bigint, `cd_gender` string, `cd_marital_status` string, `cd_education_status` string, `cd_purchase_estimate` int, `cd_credit_rating` string, `cd_dep_count` int, `cd_dep_employed_count` int, `cd_dep_college_count` int)ROW FORMAT SERDE \\'org.apache.hadoop.hive.ql.io.orc.OrcSerde\\'STORED AS INPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\\'OUTPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\\'LOCATION \\'hdfs://mycluster/usr/hive/warehouse/bigbench.db/customer_demographics\\'TBLPROPERTIES ( \\'COLUMN_STATS_ACCURATE\\'=\\'true\\', \\'numFiles\\'=\\'1\\', \\'numRows\\'=\\'1920800\\', \\'rawDataSize\\'=\\'718371044\\', \\'totalSize\\'=\\'7167503\\', \\'transient_lastDdlTime\\'=\\'1473167169\\') 某一行： 0 F U Primary 6000 Good 0 5 0 date_dim （109573行） 1.67MB建表语句： hive&gt; show create table date_dim;OKCREATE TABLE `date_dim`( `d_date_sk` bigint, `d_date_id` string, `d_date` string, `d_month_seq` int, `d_week_seq` int, `d_quarter_seq` int, `d_year` int, `d_dow` int, `d_moy` int, `d_dom` int, `d_qoy` int, `d_fy_year` int, `d_fy_quarter_seq` int, `d_fy_week_seq` int, `d_day_name` string, `d_quarter_name` string, `d_holiday` string, `d_weekend` string, `d_following_holiday` string, `d_first_dom` int, `d_last_dom` int, `d_same_day_ly` int, `d_same_day_lq` int, `d_current_day` string, `d_current_week` string, `d_current_month` string, `d_current_quarter` string, `d_current_year` string)ROW FORMAT SERDE \\'org.apache.hadoop.hive.ql.io.orc.OrcSerde\\'STORED AS INPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\\'OUTPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\\'LOCATION \\'hdfs://mycluster/usr/hive/warehouse/bigbench.db/date_dim\\'TBLPROPERTIES ( \\'COLUMN_STATS_ACCURATE\\'=\\'true\\', \\'numFiles\\'=\\'1\\', \\'numRows\\'=\\'109573\\', \\'rawDataSize\\'=\\'123050479\\', \\'totalSize\\'=\\'1748707\\', \\'transient_lastDdlTime\\'=\\'1473167172\\') 某一行： 0 AAAAAAAAAAAAAAAA 1900-01-01 0 0 0 1900 1 1 1 1 1900 0 0 Monday 1900Q1 Y N N 2448812 2458802 2472542 2420941 N N NN N household_demographics （7200行） 14.31KB建表语句： hive&gt; show create table household_demographics;OKCREATE TABLE `household_demographics`( `hd_demo_sk` bigint, `hd_income_band_sk` bigint, `hd_buy_potential` string, `hd_dep_count` int, `hd_vehicle_count` int)ROW FORMAT SERDE \\'org.apache.hadoop.hive.ql.io.orc.OrcSerde\\'STORED AS INPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\\'OUTPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\\'LOCATION \\'hdfs://mycluster/usr/hive/warehouse/bigbench.db/household_demographics\\'TBLPROPERTIES ( \\'COLUMN_STATS_ACCURATE\\'=\\'true\\', \\'numFiles\\'=\\'1\\', \\'numRows\\'=\\'7200\\', \\'rawDataSize\\'=\\'835168\\', \\'totalSize\\'=\\'14655\\', \\'transient_lastDdlTime\\'=\\'1473167173\\') 某一行： 0 3 1001-5000 0 0 income_band （20行） 754B建表语句： hive&gt; show create table income_band;OKCREATE TABLE `income_band`( `ib_income_band_sk` bigint, `ib_lower_bound` int, `ib_upper_bound` int)ROW FORMAT SERDE \\'org.apache.hadoop.hive.ql.io.orc.OrcSerde\\'STORED AS INPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\\'OUTPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\\'LOCATION \\'hdfs://mycluster/usr/hive/warehouse/bigbench.db/income_band\\'TBLPROPERTIES ( \\'COLUMN_STATS_ACCURATE\\'=\\'true\\', \\'numFiles\\'=\\'2\\', \\'numRows\\'=\\'20\\', \\'rawDataSize\\'=\\'320\\', \\'totalSize\\'=\\'754\\', \\'transient_lastDdlTime\\'=\\'1473167179\\')Time taken: 0.032 seconds, Fetched: 19 row(s) 某一行： 0 1 10000 inventory （23255100行） 34.55MB建表语句： hive&gt; show create table inventory;OKCREATE TABLE `inventory`( `inv_date_sk` bigint, `inv_item_sk` bigint, `inv_warehouse_sk` bigint, `inv_quantity_on_hand` int)ROW FORMAT SERDE \\'org.apache.hadoop.hive.ql.io.orc.OrcSerde\\'STORED AS INPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\\'OUTPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\\'LOCATION \\'hdfs://mycluster/usr/hive/warehouse/bigbench.db/inventory\\'TBLPROPERTIES ( \\'COLUMN_STATS_ACCURATE\\'=\\'true\\', \\'numFiles\\'=\\'3\\', \\'numRows\\'=\\'23255100\\', \\'rawDataSize\\'=\\'651118804\\', \\'totalSize\\'=\\'36234106\\', \\'transient_lastDdlTime\\'=\\'1473167235\\')Time taken: 0.031 seconds, Fetched: 20 row(s) 某一行： 36890 0 0 503 item （17820行） 2.36MB建表语句： hive&gt; show create table item;OKCREATE TABLE `item`( `i_item_sk` bigint, `i_item_id` string, `i_rec_start_date` string, `i_rec_end_date` string, `i_item_desc` string, `i_current_price` decimal(7,2), `i_wholesale_cost` decimal(7,2), `i_brand_id` int, `i_brand` string, `i_class_id` int, `i_class` string, `i_category_id` int, `i_category` strong, `i_manufact_id` int, `i_manufact` string, `i_size` string, `i_formulation` string, `i_color` string, `i_units` string, `i_container` string, `i_manager_id` int, `i_product_name` string)ROW FORMAT SERDE \\'org.apache.hadoop.hive.ql.io.orc.OrcSerde\\'STORED AS INPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\\'OUTPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\\'LOCATION \\'hdfs://mycluster/usr/hive/warehouse/bigbench.db/item\\'TBLPROPERTIES ( \\'COLUMN_STATS_ACCURATE\\'=\\'true\\', \\'numFiles\\'=\\'1\\', \\'numRows\\'=\\'17820\\', \\'rawDataSize\\'=\\'31238428\\', \\'totalSize\\'=\\'2472973\\', \\'transient_lastDdlTime\\'=\\'1473167181\\') 某一行： 0 AAAAAAAAAAAAAAAA 2000-01-14 quickly even dinos beneath the frays must have to boost boldly careful bold escapades: stealthily even forges over the dependencies integrate always past the quiet sly decoys-- notornis sol 72.29 64.96 3898712 71TS7NSbvH1YbdiQMG6ttBHKAljiNoIRB 1 Fan Shop 9 Sports &amp; Outdoors 995 2VOxvrIWwlJQTSk6 small 99Ee1r6uFbZSSClAX3 dodger Oz Unknown 18 8m9n5Q7T33DNWidoA6nWlg6ydmpA1SKOoOJLXiLVb item_marketprices （89100行） 0.63MB建表语句： hive&gt; show create table item_marketprices;OKCREATE TABLE `item_marketprices`( `imp_sk` bigint, `imp_item_sk` bigint, `imp_competitor` string, `imp_competitor_price` decimal(7,2), `imp_start_date` bigint, `imp_end_date` bigint)ROW FORMAT SERDE \\'org.apache.hadoop.hive.ql.io.orc.OrcSerde\\'STORED AS INPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\\'OUTPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\\'LOCATION \\'hdfs://mycluster/usr/hive/warehouse/bigbench.db/item_marketprices\\'TBLPROPERTIES ( \\'COLUMN_STATS_ACCURATE\\'=\\'true\\', \\'numFiles\\'=\\'2\\', \\'numRows\\'=\\'89100\\', \\'rawDataSize\\'=\\'21736912\\', \\'totalSize\\'=\\'657024\\', \\'transient_lastDdlTime\\'=\\'1473167275\\') 某一行： 5 4737 AAAAAAAAAAAAAAIN 66.4 36890 36958 product_reviews （89991行） 24.36MB建表语句： hive&gt; show create table product_reviews;OKCREATE TABLE `product_reviews`( `pr_review_sk` bigint, `pr_review_date` string, `pr_review_time` string, `pr_review_rating` int, `pr_item_sk` bigint, `pr_user_sk` bigint, `pr_order_sk` bigint, `pr_review_content` string)ROW FORMAT SERDE \\'org.apache.hadoop.hive.ql.io.orc.OrcSerde\\'STORED AS INPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\\'OUTPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\\'LOCATION \\'hdfs://mycluster/usr/hive/warehouse/bigbench.db/product_reviews\\'TBLPROPERTIES ( \\'COLUMN_STATS_ACCURATE\\'=\\'true\\', \\'numFiles\\'=\\'2\\', \\'numRows\\'=\\'89991\\', \\'rawDataSize\\'=\\'79597043\\', \\'totalSize\\'=\\'25546821\\', \\'transient_lastDdlTime\\'=\\'1473167305\\') 某一行： 4 2004-02-25 21:43:07 1 3683 88783 41241 Had to offset the praise handed down by so many years is probably the result of some sin committed (obviously so grievous don\\&apos;t &quot;get&quot; it. Why all the original artwork by Stephen Gammel. promotion （300行） 15.83KB建表语句： hive&gt; show create table promotion;OKCREATE TABLE `promotion`( `p_promo_sk` bigint, `p_promo_id` string, `p_start_date_sk` bigint, `p_end_date_sk` bigint, `p_item_sk` bigint, `p_cost` decimal(15,2), `p_response_target` int, `p_promo_name` string, `p_channel_dmail` string, `p_channel_email` string, `p_channel_catalog` string, `p_channel_tv` string, `p_channel_radio` string, `p_channel_press` string, `p_channel_event` string, `p_channel_demo` string, `p_channel_details` string, `p_purpose` string, `p_discount_active` string)ROW FORMAT SERDE \\'org.apache.hadoop.hive.ql.io.orc.OrcSerde\\'STORED AS INPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\\'OUTPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\\'LOCATION \\'hdfs://mycluster/usr/hive/warehouse/bigbench.db/promotion\\'TBLPROPERTIES ( \\'COLUMN_STATS_ACCURATE\\'=\\'true\\', \\'numFiles\\'=\\'2\\', \\'numRows\\'=\\'300\\', \\'rawDataSize\\'=\\'404926\\', \\'totalSize\\'=\\'16204\\', \\'transient_lastDdlTime\\'=\\'1473167186\\') 某一行： 7 AAAAAAAAAAAAAAAH 94455 108529 13511 427.76 1 bar YN Y N N N N N blithe grouches past the blithe quick epitaphs print rut Unknown N reason （35行） 3.17KB建表语句: hive&gt; show create table reason;OKCREATE TABLE `reason`( `r_reason_sk` bigint, `r_reason_id` string, `r_reason_desc` string)ROW FORMAT SERDE \\'org.apache.hadoop.hive.ql.io.orc.OrcSerde\\'STORED AS INPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\\'OUTPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\\'LOCATION \\'hdfs://mycluster/usr/hive/warehouse/bigbench.db/reason\\'TBLPROPERTIES ( \\'COLUMN_STATS_ACCURATE\\'=\\'true\\', \\'numFiles\\'=\\'2\\', \\'numRows\\'=\\'35\\', \\'rawDataSize\\'=\\'9027\\', \\'totalSize\\'=\\'3240\\', \\'transient_lastDdlTime\\'=\\'1473167190\\') 某一行： 5 uy busily sly excuses hang: slow braids to the daring somas was toward the epitaphs-- gifts betw ship_mode （20行） 2.93KB建表语句： hive&gt; show create table ship_mode;OKCREATE TABLE `ship_mode`( `sm_ship_mode_sk` bigint, `sm_ship_mode_id` string, `sm_type` string, `sm_code` string, `sm_carrier` strong, `sm_contract` string)ROW FORMAT SERDE \\'org.apache.hadoop.hive.ql.io.orc.OrcSerde\\'STORED AS INPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\\'OUTPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\\'LOCATION \\'hdfs://mycluster/usr/hive/warehouse/bigbench.db/ship_mode\\'TBLPROPERTIES ( \\'COLUMN_STATS_ACCURATE\\'=\\'true\\', \\'numFiles\\'=\\'2\\', \\'numRows\\'=\\'20\\', \\'rawDataSize\\'=\\'9576\\', \\'totalSize\\'=\\'3001\\', \\'transient_lastDdlTime\\'=\\'1473167196\\') 某一行： 12 wGyozLRZ3uL iCTZsMlNzsQ cBRc FlWM9v tm0ehuQ2 store （12行） 8.03KB建表语句： hive&gt; show create table store;OKCREATE TABLE `store`( `s_store_sk` bigint, `s_store_id` string, `s_rec_start_date` string, `s_rec_end_date` string, `s_closed_date_sk` bigint, `s_store_name` string, `s_number_employees` int, `s_floor_space` int, `s_hours` string, `s_manager` string, `s_market_id` int, `s_geography_class` string, `s_market_desc` string, `s_market_manager` string, `s_division_id` int, `s_division_name` string, `s_company_id` int, `s_company_name` string, `s_street_number` string, `s_street_name` string, `s_street_type` string, `s_suite_number` string, `s_city` string, `s_county` string, `s_state` string, `s_zip` string, `s_country` string, `s_gmt_offset` decimal(5,2), `s_tax_precentage` decimal(5,2))ROW FORMAT SERDE \\'org.apache.hadoop.hive.ql.io.orc.OrcSerde\\'STORED AS INPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\\'OUTPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\\'LOCATION \\'hdfs://mycluster/usr/hive/warehouse/bigbench.db/store\\'TBLPROPERTIES ( \\'COLUMN_STATS_ACCURATE\\'=\\'true\\', \\'numFiles\\'=\\'2\\', \\'numRows\\'=\\'12\\', \\'rawDataSize\\'=\\'25962\\', \\'totalSize\\'=\\'8214\\', \\'transient_lastDdlTime\\'=\\'1473167201\\') 某一行： 10 AAAAAAAAAAAAAAAK 2000-11-01 6235 Avalon 254 6468537 8AM-12AM Michael Barlow 6 Unknown final sly gifts by the even final dependencies x-ray under the fluffy Barry Shaw 1 Unknown1Unknown 993 Mill Pkwy aQV Cold Springs Cumberland County TN 95692 United States -5 0.03 store_returns (37902行） 1.19MB建表语句： hive&gt; show create table store_returns;OKCREATE TABLE `store_returns`( `sr_returned_date_sk` bigint, `sr_return_time_sk` bigint, `sr_item_sk` bigint, `sr_customer_sk` bigint, `sr_cdemo_sk` bigint, `sr_hdemo_sk` bigint, `sr_addr_sk` bigint, `sr_store_sk` bigint, `sr_reason_sk` bigint, `sr_ticket_number` bigint, `sr_return_quantity` int, `sr_return_amt` decimal(7,2), `sr_return_tax` decimal(7,2), `sr_return_amt_inc_tax` decimal(7,2), `sr_fee` decimal(7,2), `sr_return_ship_cost` decimal(7,2), `sr_refunded_cash` decimal(7,2), `sr_reversed_charge` decimal(7,2), `sr_store_credit` decimal(7,2), `sr_net_loss` decimal(7,2))ROW FORMAT SERDE \\'org.apache.hadoop.hive.ql.io.orc.OrcSerde\\'STORED AS INPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\\'OUTPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\\'LOCATION \\'hdfs://mycluster/usr/hive/warehouse/bigbench.db/store_returns\\'TBLPROPERTIES ( \\'COLUMN_STATS_ACCURATE\\'=\\'true\\', \\'numFiles\\'=\\'2\\', \\'numRows\\'=\\'37902\\', \\'rawDataSize\\'=\\'41388272\\', \\'totalSize\\'=\\'1250563\\', \\'transient_lastDdlTime\\'=\\'1473167251\\') 某一行： 37375 38182 11520 7640 242073 6754 25731 2 22 20293 33 1990.89 119.45 2110.34 34.14 2433.41 477.81 559.84 953.24 2672.31 store_sales （667579行） 23.49MB建表语句： hive&gt; show create table store_sales;OKCREATE TABLE `store_sales`( `ss_sold_date_sk` bigint, `ss_sold_time_sk` bigint, `ss_item_sk` bigint, `ss_customer_sk` bigint, `ss_cdemo_sk` bigint, `ss_hdemo_sk` bigint, `ss_addr_sk` bigint, `ss_store_sk` bigint, `ss_promo_sk` bigint, `ss_ticket_number` bigint, `ss_quantity` int, `ss_wholesale_cost` decimal(7,2), `ss_list_price` decimal(7,2), `ss_sales_price` decimal(7,2), `ss_ext_discount_amt` decimal(7,2), `ss_ext_sales_price` decimal(7,2), `ss_ext_wholesale_cost` decimal(7,2), `ss_ext_list_price` decimal(7,2), `ss_ext_tax` decimal(7,2), `ss_coupon_amt` decimal(7,2), `ss_net_paid` decimal(7,2), `ss_net_paid_inc_tax` decimal(7,2), `ss_net_profit` decimal(7,2))ROW FORMAT SERDE \\'org.apache.hadoop.hive.ql.io.orc.OrcSerde\\'STORED AS INPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\\'OUTPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\\'LOCATION \\'hdfs://mycluster/usr/hive/warehouse/bigbench.db/store_sales\\'TBLPROPERTIES ( \\'COLUMN_STATS_ACCURATE\\'=\\'true\\', \\'numFiles\\'=\\'2\\', \\'numRows\\'=\\'667579\\', \\'rawDataSize\\'=\\'953293700\\', \\'totalSize\\'=\\'24629162\\', \\'transient_lastDdlTime\\'=\\'1473167245\\') 某一行： 37115 20244 16481 98676 1211207 5239 37107 9 38 11138 93 56.47 82.45 36.28 4293.81 3374.04 5251.71 7667.85 101.22 0 3374.04 3475.26 -1877.67 time_dim （86400行） 219.49KB建表语句： hive&gt; show create table time_dim;OKCREATE TABLE `time_dim`( `t_time_sk` bigint, `t_time_id` string, `t_time` int, `t_hour` int, `t_minute` int, `t_second` int, `t_am_pm` string, `t_shift` string, `t_sub_shift` string, `t_meal_time` string)ROW FORMAT SERDE \\'org.apache.hadoop.hive.ql.io.orc.OrcSerde\\'STORED AS INPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\\'OUTPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\\'LOCATION \\'hdfs://mycluster/usr/hive/warehouse/bigbench.db/time_dim\\'TBLPROPERTIES ( \\'COLUMN_STATS_ACCURATE\\'=\\'true\\', \\'numFiles\\'=\\'1\\', \\'numRows\\'=\\'86400\\', \\'rawDataSize\\'=\\'41040000\\', \\'totalSize\\'=\\'224757\\', \\'transient_lastDdlTime\\'=\\'1473167202\\') 某一行： 2 AAAAAAAAAAAAAAAC 2 0 0 2 AM third night warehouse （5行） 1.94KB建表语句: hive&gt; show create table warehouse;OKCREATE TABLE `warehouse`( `w_warehouse_sk` bigint, `w_warehouse_id` string, `w_warehouse_name` string, `w_warehouse_sq_ft` int, `w_street_number` string, `w_street_name` string, `w_street_type` string, `w_suite_number` string, `w_city` string, `w_county` string, `w_state` string, `w_zip` string, `w_country` string, `w_gmt_offset` decimal(5,2))ROW FORMAT SERDE \\'org.apache.hadoop.hive.ql.io.orc.OrcSerde\\'STORED AS INPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\\'OUTPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\\'LOCATION \\'hdfs://mycluster/usr/hive/warehouse/bigbench.db/warehouse\\'TBLPROPERTIES ( \\'COLUMN_STATS_ACCURATE\\'=\\'true\\', \\'numFiles\\'=\\'2\\', \\'numRows\\'=\\'5\\', \\'rawDataSize\\'=\\'5695\\', \\'totalSize\\'=\\'1980\\', \\'transient_lastDdlTime\\'=\\'1473167204\\') 某一行： 1 AAAAAAAAAAAAAAAB frets would sleep 845707 181 9th 13thCourt kQ Sherwood Forest Teton County WY 87891 United States -7 web_clickstreams （6770550行） 34.11MB建表语句： hive&gt; show create table web_clickstreams;OKCREATE TABLE `web_clickstreams`( `wcs_click_date_sk` bigint, `wcs_click_time_sk` bigint, `wcs_sales_sk` bigint, `wcs_item_sk` bigint, `wcs_web_page_sk` bigint, `wcs_user_sk` bigint)ROW FORMAT SERDE \\'org.apache.hadoop.hive.ql.io.orc.OrcSerde\\'STORED AS INPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\\'OUTPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\\'LOCATION \\'hdfs://mycluster/usr/hive/warehouse/bigbench.db/web_clickstreams\\'TBLPROPERTIES ( \\'COLUMN_STATS_ACCURATE\\'=\\'true\\', \\'numFiles\\'=\\'2\\', \\'numRows\\'=\\'6770550\\', \\'rawDataSize\\'=\\'253337416\\', \\'totalSize\\'=\\'35768476\\', \\'transient_lastDdlTime\\'=\\'1473167298\\') 某一行： 36890 27089 NULL 15465 47 NULL web_page （60行） 6.81KB建表语句： hive&gt; show create table web_page;OKCREATE TABLE `web_page`( `wp_web_page_sk` bigint, `wp_web_page_id` string, `wp_rec_start_date` string, `wp_rec_end_date` string, `wp_creation_date_sk` bigint, `wp_access_date_sk` bigint, `wp_autogen_flag` string, `wp_customer_sk` bigint, `wp_url` string, `wp_type` string, `wp_char_count` int, `wp_link_count` int, `wp_image_count` int, `wp_max_ad_count` int)ROW FORMAT SERDE \\'org.apache.hadoop.hive.ql.io.orc.OrcSerde\\'STORED AS INPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\\'OUTPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\\'LOCATION \\'hdfs://mycluster/usr/hive/warehouse/bigbench.db/web_page\\'TBLPROPERTIES ( \\'COLUMN_STATS_ACCURATE\\'=\\'true\\', \\'numFiles\\'=\\'2\\', \\'numRows\\'=\\'60\\', \\'rawDataSize\\'=\\'38763\\', \\'totalSize\\'=\\'6969\\', \\'transient_lastDdlTime\\'=\\'1473167215\\') 某一行： 24 AAAAAAAAAAAAAAAY 2000-12-19 38819 69008 0 44446 http://www.C8jdri37RmtbDNeFnXjmYbyBPzeO4WWK9pVYP6xtBJbaQ5yIj4s.com feedback 4340 23 5 2 web_returns （38487行） 1.40MB建表语句： hive&gt; show create table web_returns;OKCREATE TABLE `web_returns`( `wr_returned_date_sk` bigint, `wr_returned_time_sk` bigint, `wr_item_sk` bigint, `wr_refunded_customer_sk` bigint, `wr_refunded_cdemo_sk` bigint, `wr_refunded_hdemo_sk` bigint, `wr_refunded_addr_sk` bigint, `wr_returning_customer_sk` bigint, `wr_returning_cdemo_sk` bigint, `wr_returning_hdemo_sk` bigint, `wr_returning_addr_sk` bigint, `wr_web_page_sk` bigint, `wr_reason_sk` bigint, `wr_order_number` bigint, `wr_return_quantity` int, `wr_return_amt` decimal(7,2), `wr_return_tax` decimal(7,2), `wr_return_amt_inc_tax` decimal(7,2), `wr_fee` decimal(7,2), `wr_return_ship_cost` decimal(7,2), `wr_refunded_cash` decimal(7,2), `wr_reversed_charge` decimal(7,2), `wr_account_credit` decimal(7,2), `wr_net_loss` decimal(7,2))ROW FORMAT SERDE \\'org.apache.hadoop.hive.ql.io.orc.OrcSerde\\'STORED AS INPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\\'OUTPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\\'LOCATION \\'hdfs://mycluster/usr/hive/warehouse/bigbench.db/web_returns\\'TBLPROPERTIES ( \\'COLUMN_STATS_ACCURATE\\'=\\'true\\', \\'numFiles\\'=\\'2\\', \\'numRows\\'=\\'38487\\', \\'rawDataSize\\'=\\'43257220\\', \\'totalSize\\'=\\'1471571\\', \\'transient_lastDdlTime\\'=\\'1473167269\\') 某一行： 38952 68126 9590 52948 893223 2020 6942 52948 893223 2020 6942 32 13 63444 33 626.0125.04 651.05 18.74 958.4 244.14 286.4 95.47 1008.48 web_sales （668052行） 32.99MB建表语句： hive&gt; show create table web_sales;OKCREATE TABLE `web_sales`( `ws_sold_date_sk` bigint, `ws_sold_time_sk` bigint, `ws_ship_date_sk` bigint, `ws_item_sk` bigint, `ws_bill_customer_sk` bigint, `ws_bill_cdemo_sk` bigint, `ws_bill_hdemo_sk` bigint, `ws_bill_addr_sk` bigint, `ws_ship_customer_sk` bigint, `ws_ship_cdemo_sk` bigint, `ws_ship_hdemo_sk` bigint, `ws_ship_addr_sk` bigint, `ws_web_page_sk` bigint, `ws_web_site_sk` bigint, `ws_ship_mode_sk` bigint, `ws_warehouse_sk` bigint, `ws_promo_sk` bigint, `ws_order_number` bigint, `ws_quantity` int, `ws_wholesale_cost` decimal(7,2), `ws_list_price` decimal(7,2), `ws_sales_price` decimal(7,2), `ws_ext_discount_amt` decimal(7,2), `ws_ext_sales_price` decimal(7,2), `ws_ext_wholesale_cost` decimal(7,2), `ws_ext_list_price` decimal(7,2), `ws_ext_tax` decimal(7,2), `ws_coupon_amt` decimal(7,2), `ws_ext_ship_cost` decimal(7,2), `ws_net_paid` decimal(7,2), `ws_net_paid_inc_tax` decimal(7,2), `ws_net_paid_inc_ship` decimal(7,2), `ws_net_paid_inc_ship_tax` decimal(7,2), `ws_net_profit` decimal(7,2))ROW FORMAT SERDE \\'org.apache.hadoop.hive.ql.io.orc.OrcSerde\\'STORED AS INPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\\'OUTPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\\'LOCATION \\'hdfs://mycluster/usr/hive/warehouse/bigbench.db/web_sales\\'TBLPROPERTIES ( \\'COLUMN_STATS_ACCURATE\\'=\\'true\\', \\'numFiles\\'=\\'2\\', \\'numRows\\'=\\'668052\\', \\'rawDataSize\\'=\\'1221174888\\', \\'totalSize\\'=\\'34585166\\', \\'transient_lastDdlTime\\'=\\'1473167263\\') 某一行： 36890 26789 36993 794 85457 380790 2436 42649 29934 1583844 5251 525 2 14 1 3 120 0 93 32.62 81.22 59.29 2039.49 5513.97 3033.66 7553.46 330.84 0 1661.76 5513.97 5844.81 7175.73 7506.57 2480.31 web_site （30行）10.31KB建表语句： hive&gt; show create table web_site;OKCREATE TABLE `web_site`( `web_site_sk` bigint, `web_site_id` string, `web_rec_start_date` string, `web_rec_end_date` string, `web_name` string, `web_open_date_sk` bigint, `web_close_date_sk` bigint, `web_class` string, `web_manager` string, `web_mkt_id` int, `web_mkt_class` string, `web_mkt_desc` string, `web_market_manager` string, `web_company_id` int, `web_company_name` string, `web_street_number` string, `web_street_name` string, `web_street_type` string, `web_suite_number` string, `web_city` string, `web_county` string, `web_state` string, `web_zip` string, `web_country` string, `web_gmt_offset` decimal(5,2), `web_tax_percentage` decimal(5,2))ROW FORMAT SERDE \\'org.apache.hadoop.hive.ql.io.orc.OrcSerde\\'STORED AS INPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\\'OUTPUTFORMAT \\'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\\'LOCATION \\'hdfs://mycluster/usr/hive/warehouse/bigbench.db/web_site\\'TBLPROPERTIES ( \\'COLUMN_STATS_ACCURATE\\'=\\'true\\', \\'numFiles\\'=\\'2\\', \\'numRows\\'=\\'30\\', \\'rawDataSize\\'=\\'62832\\', \\'totalSize\\'=\\'10554\\', \\'transient_lastDdlTime\\'=\\'1473167210\\') 某一行： 2 AAAAAAAAAAAAAAAC 2002-07-17 site_0 16450 91500 Unknown Gregory George 1 sheaves despite the quietly sly asymp thin enticing frets except the sometimes final courts might promise blithe dino Frank Hernandez 1 ese 17 5th Ave EbDxJVL Georgetown Guadalupe County TX 75435 United States -6 0.01","raw":null,"content":null,"categories":[{"name":"源码分析","slug":"源码分析","permalink":"http://linbingdong.com/categories/源码分析/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://linbingdong.com/tags/大数据/"},{"name":"TPCx-BB","slug":"TPCx-BB","permalink":"http://linbingdong.com/tags/TPCx-BB/"},{"name":"基准测试","slug":"基准测试","permalink":"http://linbingdong.com/tags/基准测试/"}]},{"title":"关于Java Collections的几个常见问题","slug":"Stack Overflow上关于Java Collections的几个常见问题","date":"2017-03-11T12:33:42.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2017/03/11/Stack Overflow上关于Java Collections的几个常见问题/","link":"","permalink":"http://linbingdong.com/2017/03/11/Stack Overflow上关于Java Collections的几个常见问题/","excerpt":"列举几个关于Java Collections的常见问题并给出答案。","text":"列举几个关于Java Collections的常见问题并给出答案。 1. 什么时候用LinkedList，什么时候用ArrayList？ArrayList是使用数组实现的list，本质上就是数组。ArrayList中的元素可以通过索引随机获取一个元素。但是如果该数组已满，当添加新元素时需要分配一个新的数组然后将原来数组的元素移动过去，需要O(n)的时间复杂度。添加或删除一个元素需要移动数组中的其他元素。这是ArrayList最大的缺点。 LinkedList是一个双向链表。因此，当需要获取list中某个元素，需要从头到尾遍历list。另一方面，在链表中添加或删除元素很快，只需要O(1)的时间复杂度。从空间上来说，在链表中一个节点需要两个额外的指针来指向它的previous和next节点。 总结： 从时间复杂度来说，如果对list增加或删除操作较多，优先用LinkedList；如果查询操作较多，优先用ArrayList。 从空间复杂度来说，LinkedList会占用较多空间。 2. 如何边遍历边移除Collection中的元素边遍历边修改Collection的唯一正确方式是使用Iterator.remove()方法，如下： Iterator&lt;Integer&gt; it = list.iterator();while(it.hasNext())&#123; // do something it.remove();&#125; 一种最常见的错误代码如下： for(Integer i : list)&#123; list.remove(i)&#125; 运行以上错误代码会报ConcurrentModificationException异常。这是因为当使用foreach(for(Integer i : list))语句时，会自动生成一个iterator来遍历该list，但同时该list正在被Iterator.remove()修改。在Java中，一般不允许一个线程在遍历collection时另一个线程在修改它。 3. 如何将List转化成int[]？很多人可能认为只需用List.toArray()即可，其实不然。List.toArray()方法只可能得到Integer[]，无法得到int[]。 最简单的方法是使用Apache Commons Lang库中的ArrayUtils。 int[] array = ArrayUtils.toPrimitive(list.toArray(new Integer[0])); 在JDK中，没有捷径。需要注意的是，不能直接使用List.toArray(),因为这样会将List转化成Integer[]而不是int[]。正确的做法如下： int[] array = new int[list.size()];for(int i = 0; i &lt; list.size(); i++)&#123; array[i] = list.get(i);&#125; 4. 如何将int[]转化成List？同上，很多人以为只需用Arrays.asList()即可，其实不然。因为不能以int[]作为该方法的参数，要的话也只能是Integer[]。 关于Arrays.asList()方法有如下特性： 1.该方法对于基本数据类型的数组支持并不好,当数组是基本数据类型时不建议使用 2.当使用asList()方法时，数组就和列表链接在一起了。当更新其中之一时，另一个将自动获得更新。因为asList获得的List实际引用的就是数组 注意:仅仅针对对象数组类型,基本数据类型数组不具备该特性。 3.asList得到的数组是的没有add和remove方法的。因为asList返回的List是Arrays中的内部类,而该类并没有定义add和remove方法。 那么如何将int[]转化成List呢？ 还是得自己实现： int[] array = &#123;1,2,3,4,5&#125;;List&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;();for(int i: array) &#123; list.add(i);&#125; 5. 过滤一个Collection最好的方法是什么？如过滤掉list中大于5的整数。 Iterator&lt;Integer&gt; it = list.iterator();while(it.hasNext())&#123; int i = it.next(); if(i &gt; 5) &#123; //过滤掉大于5的整数 it.remove(); &#125;&#125; 6. 将List转化成Set最简单的方法？有两种方法，取决于你怎么要怎么定义两个元素相等。第一种方法是将list放入HashSet里，该方法元素是否相等是通过它们的hashCode()来比较的。如果需要自己定义比较的方法，需要用TreeSet。 Set&lt;Integer&gt; set = new HashSet&lt;Integer&gt;(list); Set&lt;Integer&gt; set = new TreeSet&lt;Integer&gt;(aComparator);set.addAll(list); 7. 如何删除ArrayList中重复的元素？如果不关心元素在ArrayList中的顺序，可以将list放入set中来删除重复元素，然后在放回list。 Set&lt;Integer&gt; set = new HashSet&lt;Integer&gt;(list);list.clear();list.addAll(set); 如果关心元素在ArrayList中的顺序，可以用LinkedHashSet。 8. 有序的collectionJava里有很多方法来维持一个collection有序。有的需要实现Comparable接口，有的需要自己指定Comparator。 Collections.sort()可以用来对list排序。该排序是稳定的，并且可以保证nlog(n)的性能。 PriorityQueue提供排序的队列。PriorityQueue和Collections.sort()的区别是，PriorityQueue动态维护一个有序的队列（每添加或删除一个元素就会重新排序），但是只能获队列中的头元素。 如果collection中没有重复的元素，TreeSet是另一个选择。跟PriorityQueue一样的是，TreeSet也动态维护一个有序的集合。可以从TreeSet中获取最大和最小的元素。 总结：Collections.sort()提供一个一次排序的list。PriorityQueue和TreeSet动态维护排序的collection。 9. 拷贝list有两种方法可以用来拷贝list。一种是使用ArrayList构造器。 ArrayList&lt;Integer&gt; dstList = new ArrayList&lt;Integer&gt;(srcList); 另一种是使用Collections.copy()。 ArrayList&lt;Integer&gt; dstList = new ArrayList&lt;Integer&gt;(srcList.size());Collections.copy(dstList, srcList); 需要注意的是，使用该方法的话目标list至少跟源list长度一样长。否则会报IndexOutOfBoundsException异常。 另外有两点需要注意： 两种方法都是浅拷贝 Collections.copy()方法的两个参数必须都是list，而ArrayList方法参数只要是collection即可，因此ArrayList方法更通用。","raw":null,"content":null,"categories":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/tags/Java/"}]},{"title":"ZeroMQ初探","slug":"ZeroMQ初探","date":"2017-03-11T12:33:42.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2017/03/11/ZeroMQ初探/","link":"","permalink":"http://linbingdong.com/2017/03/11/ZeroMQ初探/","excerpt":"ZeroMQ介绍、三种模式讲解及代码示例(Java &amp;&amp; Python)。","text":"ZeroMQ介绍、三种模式讲解及代码示例(Java &amp;&amp; Python)。 概述ZeroMQ（也称为 ØMQ，0MQ 或 zmq）是一个可嵌入的网络通讯库（对 Socket 进行了封装）。 它提供了携带跨越多种传输协议（如：进程内，进程间，TCP 和多播）的原子消息的 sockets 。 有了ZeroMQ，我们可以通过发布-订阅、任务分发、和请求-回复等模式来建立 N-N 的 socket 连接。 ZeroMQ 的异步 I / O 模型为我们提供可扩展的基于异步消息处理任务的多核应用程序。 它有一系列语言API（几乎囊括所有编程语言），并能够在大多数操作系统上运行。 传统的 TCP Socket 的连接是1对1的，可以认为“1个 socket = 1个连接”，每一个线程独立维护一个 socket 。但是 ZMQ 摒弃了这种1对1的模式，ZMQ的 Socket 可以很轻松地实现1对N和N对N的连接模式，一个 ZMQ 的 socket 可以自动地维护一组连接，用户无法操作这些连接，用户只能操作套接字而不是连接本身。所以说在 ZMQ 的世界里，连接是私有的。 三种基本模型ZMQ 提供了三种基本的通信模型，分别是 Request-Reply 、Publish-Subscribe 和 Parallel Pipeline ，接下来举例说明三种模型并给出相应的代码实现。 Request-Reply（请求-回复）以 “Hello World” 为例。客户端发起请求，并等待服务端回应请求。客户端发送一个简单的 “Hello”，服务端则回应一个 “World”。可以有 N 个客户端，一个服务端，因此是 1-N 连接。 服务端代码如下： hwserver.java import org.zeromq.ZMQ;public class hwserver &#123; public static void main(String[] args) throws InterruptedException &#123; ZMQ.Context context = ZMQ.context(1); ZMQ.Socket responder = context.socket(ZMQ.REP); responder.bind(\"tcp://*:5555\"); while (!Thread.currentThread().isInterrupted()) &#123; byte[] request = responder.recv(0); System.out.println(\"Received\" + new String(request)); Thread.sleep(1000); String reply = \"World\"; responder.send(reply.getBytes(),0); &#125; responder.close(); context.term(); &#125;&#125; hwserver.py import timeimport zmqcontext = zmq.Context()socket = context.socket(zmq.REP)socket.bind(\"tcp://*:5555\")while True: message = socket.recv() print(\"Received request: %s\" % message) # Do some 'work' time.sleep(1) socket.send(b\"World\") 客户端代码如下： hwclient.java import org.zeromq.ZMQ;public class hwclient &#123; public static void main(String[] args) &#123; ZMQ.Context context = ZMQ.context(1); ZMQ.Socket requester = context.socket(ZMQ.REQ); requester.connect(\"tcp://localhost:5555\"); for (int requestNbr = 0; requestNbr != 10; requestNbr++) &#123; String request = \"Hello\"; System.out.println(\"Sending Hello\" + requestNbr); requester.send(request.getBytes(),0); byte[] reply = requester.recv(0); System.out.println(\"Reveived \" + new String(reply) + \" \" + requestNbr); &#125; requester.close(); context.term(); &#125;&#125; hwclient.py import zmqcontext = zmq.Context()print(\"Connecting to hello world server...\")socket = context.socket(zmq.REQ)socket.connect(\"tcp://localhost:5555\")for request in range(10): print(\"Sending request %s ...\" % request) socket.send(b\"Hello\") message = socket.recv() print(\"Received reply %s [ %s ]\" % (request,message)) 从以上的过程，我们可以了解到使用 ZMQ 写基本的程序的方法，需要注意的是： 服务端和客户端无论谁先启动，效果是相同的，这点不同于 Socket。 在服务端收到信息以前，程序是阻塞的，会一直等待客户端连接上来。 服务端收到信息后，会发送一个 “World” 给客户端。值得注意的是一定是客户端连接上来以后，发消息给服务端，服务端接受消息然后响应客户端，这样一问一答。 ZMQ 的通信单元是消息，它只知道消息的长度，并不关心消息格式。因此，你可以使用任何你觉得好用的数据格式，如 Xml、Protocol Buffers、Thrift、json 等等。 Publish-Subscribe（发布-订阅）下面以一个天气预报的例子来介绍该模式。 服务端不断地更新各个城市的天气，客户端可以订阅自己感兴趣（通过一个过滤器）的城市的天气信息。 服务端代码如下： wuserver.java import org.zeromq.ZMQ;import java.util.Random;public class wuserver &#123; public static void main(String[] args) &#123; ZMQ.Context context = ZMQ.context(1); ZMQ.Socket publisher = context.socket(ZMQ.PUB); publisher.bind(\"tcp://*:5556\"); publisher.bind(\"icp://weather\"); Random srandom = new Random(System.currentTimeMillis()); while (!Thread.currentThread().isInterrupted()) &#123; int zipcode, temperature, relhumidity; zipcode = 10000 + srandom.nextInt(10000); temperature = srandom.nextInt(215) - 80 + 1; relhumidity = srandom.nextInt(50) + 10 + 1; String update = String.format(\"%05d %d %d\", zipcode, temperature, relhumidity); &#125; publisher.close(); context.term(); &#125;&#125; wuserver.py from random import randrangeimport zmqcontext = zmq.Context()socket = context.socket(zmq.PUB)socket.bind(\"tcp://*:5556\")while True: zipcode = randrange(1, 100000) temperature = randrange(-80, 135) relhumidity = randrange(10, 60) socket.send_string(\"%i %i %i\" % (zipcode, temperature, relhumidity)) 客户端代码如下： wuclient.java import org.zeromq.ZMQ;import java.util.StringTokenizer;public class wuclient &#123; public static void main(String[] args) &#123; ZMQ.Context context = ZMQ.context(1); ZMQ.Socket suscriber = context.socket(ZMQ.SUB); suscriber.connect(\"tcp://localhost:5556\"); String filter = (args.length &gt; 0) ? args[0] : \"10001\"; suscriber.suscribe(filter.getBytes()); //过滤条件 int update_nbr; long total_temp = 0; for (update_nbr = 0; update_nbr &lt; 100; update_nbr++) &#123; String string = suscriber.recvStr(0).trim(); StringTokenizer sscanf = new StringTokenizer(string, \" \"); int zipcode = Integer.valueOf(sscanf.nextToken()); int temperature = Integer.valueOf(sscanf.nextToken()); int relhumidity = Integer.valueOf(sscanf.nextToken()); total_temp += temperature; &#125; System.out.println(\"Average temperature for zipcode '\" + filter + \"' was \" + (int) (total_temp / update_nbr)); suscriber.close(); context.term(); &#125;&#125; wuclient.py import sysimport zmqcontext = zmq.Context()socket = context.socket(zmq.SUB)print(\"Collecting updates from weather server...\")socket.connect(\"tcp://localhost:5556\")zip_filter = sys.argv[1] if len(sys.argv) &gt; 1 else \"10001\"if isinstance(zip_filter, bytes): zip_filter = zip_filter.decode('ascii')socket.setsockopt_string(zmq.SUBSCRIBE, zip_filter)total_temp = 0for update_nbr in range(5): string = socket.recv_string() zipcode, temperature, relhumidity = string.split() total_temp += int(temperature)print(\"Average temperature for zipcode '%s' was %dF\" % (zip_filter, total_temp / (update_nbr + 1))) 服务器端生成随机数 zipcode、temperature、relhumidity 分别代表城市代码、温度值和湿度值，然后不断地广播信息。而客户端通过设置过滤参数，接受特定城市代码的信息，最终将收集到的温度求平均值。 需要注意的是： 与 “Hello World” 例子不同的是，Socket 的类型变成 ZMQ.PUB 和 ZMQ.SUB 。 客户端需要设置一个过滤条件，接收自己感兴趣的消息。 发布者一直不断地发布新消息，如果中途有订阅者退出，其他均不受影响。当订阅者再连接上来的时候，收到的就是后来发送的消息了。这样比较晚加入的或者是中途离开的订阅者必然会丢失掉一部分信息。这是该模式的一个问题，即所谓的 “Slow joiner” 。 Parallel PipelineParallel Pipeline 处理模式如下： ventilator 分发任务到各个 worker 每个 worker 执行分配到的任务 最后由 sink 收集从 worker 发来的结果 taskvent.java import org.zeromq.ZMQ;import java.io.IOException;import java.util.Random;import java.util.StringTokenizer;public class taskvent &#123; public static void main(String[] args) throws IOException &#123; ZMQ.Context context = new ZMQ.context(1); ZMQ.Socket sender = context.socket(ZMQ.PUSH); sender.bind(\"tcp://*:5557\"); ZMQ.Socket sink = context.socket(ZMQ.PUSH); sink.connect(\"tcp://localhost:5558\"); System.out.println(\"Please enter when the workers are ready: \"); System.in.read(); System.out.println(\"Sending task to workes\\n\"); sink.send(\"0\",0); Random srandom = new Random(System.currentTimeMillis()); int task_nbr; int total_msec = 0; for (task_nbr = 0; task_nbr &lt; 100; task_nbr++) &#123; int workload; workload = srandom.nextInt(100) + 1; total_msec += workload; System.out.print(workload + \".\"); String string = String.format(\"%d\", workload); sender.send(string, 0); &#125; System.out.println(\"Total expected cost: \" + total_msec + \" msec\"); sink.close(); sender.close(); context.term(); &#125;&#125; taskvent.py import zmqimport timeimport randomtry: raw_inputexcept NameError: raw_input = inputcontext = zmq.Context()sender = context.socket(zmq.PUSH)sender.bind(\"tcp://*:5557\")sink = context.socket(zmq.PUSH)sink.connect(\"tcp://localhost:5558\")print(\"Please enter when workers are ready: \")_ = raw_input()print(\"Sending tasks to workers...\")sink.send(b'0')random.seed()total_msec = 0for task_nbr in range(100): workload = random.randint(1, 100) total_msec += workload sender.send_string(u'%i' % workload)print(\"Total expected cost: %s msec\" % total_msec)time.sleep(1) taskwork.java import org.zeromq.ZMQ;public class taskwork &#123; public static void main(String[] args) &#123; ZMQ.Context context = ZMQ.context(1); ZMQ.Socket receiver = context.socket(ZMQ.PULL); receiver.connect(\"tcp://localhost:5557\"); ZMQ.Socket sender = context.socket(ZMQ.PUSH); sender.connect(\"tcp://localhost:5558\"); while (!Thread.currentThread().isInterrupted()) &#123; String string = receiver.recv(0).trim(); Long mesc = Long.parseLong(string); System.out.flush(); System.out.print(string + \".\"); Sleep(mesc); sender.send(\"\".getBytes(), 0); &#125; sender.close(); receiver.close(); context.term(); &#125;&#125; taskwork.py import zmqimport timeimport syscontext = zmq.Context()receiver = context.socket(zmq.PULL)receiver.connect(\"tcp://localhost:5557\")sender = context.socket(zmq.PUSH)sender.connect(\"tcp://localhost:5558\")while True: s = receiver.recv() sys.stdout.write('.') sys.stdout.flush() time.sleep(int(s) * 0.001) sender.send(b'') tasksink.java import org.zeromq.ZMQ;public class tasksink &#123; public static void main(String[] args) &#123; ZMQ.Context context = ZMQ.context(1); ZMQ.Socket receiver = context.socket(ZMQ.PULL); receiver.bind(\"tcp://*:5558\"); String string = new String(receiver.recv(0)); long tstart = System.currentTimeMillis(); int task_nbr; int total_mesc = 0; for (task_nbr = 0; task_nbr &lt; 100; task_nbr++) &#123; string = new String(receiver.recv(0).trim()); if ((task_nbr / 10) * 10 == task_nbr) &#123; System.out.print(\":\"); &#125; else &#123; System.out.print(\".\"); &#125; &#125; long tend = System.currentTimeMillis(); System.out.println(\"\\nTotal elapsed time: \" + (tend - tstart) + \"msec\"); receiver.close(); context.term(); &#125;&#125; tasksink.py import timeimport zmqimport syscontext = zmq.Context()receiver = context.socket(zmq.PULL)receiver.bind(\"tcp://*:5558\")s = receiver.recv()tstart = time.time()for task_nbr in range(1, 100): s = receiver.recv() if task_nbr % 10 == 0: sys.stdout.write(':') else: sys.stdout.write('.') sys.stdout.flush()tend = time.time()print(\"Total elapsed time: %d msec\" % ((tend - tstart) * 1000)) 以下两点需要注意： ventilator 使用 ZMQ.PUSH 来分发任务；worker 用 ZMQ.PULL 来接收任务，用 ZMQ.PUSH 来发送结果；sink 用 ZMQ.PULL 来接收 worker 发来的结果。 ventilator 既是服务端，也是客户端（此时服务端是 sink）；worker 在两个过程中都是客户端；sink 也一直都是服务端。 参考资料 ZeroMQ官方用户指南","raw":null,"content":null,"categories":[{"name":"ZeroMQ","slug":"ZeroMQ","permalink":"http://linbingdong.com/categories/ZeroMQ/"}],"tags":[{"name":"消息队列","slug":"消息队列","permalink":"http://linbingdong.com/tags/消息队列/"},{"name":"Socket编程","slug":"Socket编程","permalink":"http://linbingdong.com/tags/Socket编程/"},{"name":"ZeroMQ","slug":"ZeroMQ","permalink":"http://linbingdong.com/tags/ZeroMQ/"}]},{"title":"Java NIO之内存映射文件——MappedByteBuffer","slug":"Java NIO之内存映射文件——MappedByteBuffer","date":"2017-03-11T12:33:42.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2017/03/11/Java NIO之内存映射文件——MappedByteBuffer/","link":"","permalink":"http://linbingdong.com/2017/03/11/Java NIO之内存映射文件——MappedByteBuffer/","excerpt":"大多数操作系统都可以利用虚拟内存实现将一个文件或者文件的一部分”映射”到内存中。然后，这个文件就可以当作是内存数组来访问，这比传统的文件要快得多。","text":"大多数操作系统都可以利用虚拟内存实现将一个文件或者文件的一部分”映射”到内存中。然后，这个文件就可以当作是内存数组来访问，这比传统的文件要快得多。 内存映射文件的一个关键优势是操作系统负责真正的读写，即使你的程序在刚刚写入内存后就挂了，操作系统仍然会将内存中的数据写入文件系统。另外一个更突出的优势是共享内存，内存映射文件可以被多个进程同时访问，起到一种低时延共享内存的作用。 那么，如何将一个文件映射到内存呢？ 从文件中获得一个通道（channel） FileChannel channel = FileChannel.open(path,options); 这里options指定映射模式，支持的模式有三种： FileChannel.MapMode.READ_ONLY：所产生的缓冲区是只读的。 FileChannel.MapMode.READ_WRITE：所产生的缓冲区是可写的,任何修改都会在某个时刻写回到文件中。注意，其他映射同一个文件的程序可能不能立即看到这些修改，多个程序同时进行文件映射的确切行为是依赖于操作系统的。 FileChannel.MapMode.PRIVATE：所产生的缓冲区是可写的，但是任何修改对该缓冲区来说都是私有的，不会传播到文件中。 调用FileChannel的map方法 MappedByteBuffer buffer = channel.map(FileChannel.MapMode.READ_ONLY,0,length); 接下来通过计算一个40MB文件的CRC32校验和来比较传统的文件输入和内存映射文件的速度。 传统的文件输入包括： 普通输入流（InputStream） 带缓冲的输入流（BufferedInputStream） 随机访问文件（RandomAccessFile） 程序如下： import java.io.*;import java.nio.MappedByteBuffer;import java.nio.channels.FileChannel;import java.nio.file.Files;import java.nio.file.Path;import java.nio.file.Paths;import java.util.zip.CRC32;/** * Created by lbd on 2017/1/11. */public class MemoryMapTest &#123; public static long checksumInputStream(Path filename) throws IOException &#123; //普通输入流 try (InputStream in = Files.newInputStream(filename)) &#123; CRC32 crc = new CRC32(); int c; while ((c = in.read()) != -1) crc.update(c); return crc.getValue(); &#125; &#125; public static long checksumBufferedInputStream(Path filename) throws IOException &#123; //带缓冲的输入流 try (BufferedInputStream in = new BufferedInputStream(Files.newInputStream(filename)))&#123; CRC32 crc = new CRC32(); int c; while ((c = in.read()) != -1) crc.update(c); return crc.getValue(); &#125; &#125; public static long checksumRandomAccessFile(Path filename) throws IOException &#123; //随机访问文件 try (RandomAccessFile file = new RandomAccessFile(filename.toFile(),\"r\"))&#123; CRC32 crc = new CRC32(); long length = file.length(); for (long p = 0; p &lt; length; p++)&#123; file.seek(p); int c = file.readByte(); crc.update(c); &#125; return crc.getValue(); &#125; &#125; public static long checksumMappedFile(Path filename) throws IOException &#123; //内存映射文件 try (FileChannel channel = FileChannel.open(filename))&#123; CRC32 crc = new CRC32(); int length = (int)channel.size(); MappedByteBuffer buffer = channel.map(FileChannel.MapMode.READ_ONLY,0,length); for (int p = 0; p &lt; length; p++)&#123; int c = buffer.get(p); crc.update(c); &#125; return crc.getValue(); &#125; &#125; public static void main(String[] args) throws IOException &#123; System.out.println(\"Input Stream:\"); long start = System.currentTimeMillis(); Path filename = Paths.get(args[0]); long crcValue = checksumInputStream(filename); long end = System.currentTimeMillis(); System.out.println(Long.toHexString(crcValue)); System.out.println((end - start) + \" milliseconds\"); System.out.println(); System.out.println(\"Buffered Input Stream:\"); start = System.currentTimeMillis(); crcValue = checksumBufferedInputStream(filename); end = System.currentTimeMillis(); System.out.println(Long.toHexString(crcValue)); System.out.println((end - start) + \" milliseconds\"); System.out.println(); System.out.println(\"Random Access File:\"); start = System.currentTimeMillis(); crcValue = checksumRandomAccessFile(filename); end = System.currentTimeMillis(); System.out.println(Long.toHexString(crcValue)); System.out.println((end - start) + \" milliseconds\"); System.out.println(); System.out.println(\"Mapped File:\"); start = System.currentTimeMillis(); crcValue = checksumMappedFile(filename); end = System.currentTimeMillis(); System.out.println(Long.toHexString(crcValue)); System.out.println((end - start) + \" milliseconds\"); &#125;&#125; 输出结果如下： Input Stream:c644b1f142317 millisecondsBuffered Input Stream:c644b1f1329 millisecondsRandom Access File:c644b1f157781 millisecondsMapped File:c644b1f1207 milliseconds 可以明显看出，内存映射文件速度比普通输入流和随机访问文件快得多，比带缓冲的输入流稍微快一些。","raw":null,"content":null,"categories":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/tags/Java/"}]},{"title":"Java 中定义常量的几种方法","slug":"Java 中定义常量的几种方法","date":"2017-03-11T12:33:42.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2017/03/11/Java 中定义常量的几种方法/","link":"","permalink":"http://linbingdong.com/2017/03/11/Java 中定义常量的几种方法/","excerpt":"主要有三种：接口、类和枚举。","text":"主要有三种：接口、类和枚举。 接口： 在接口里定义域(接口中会对域自动加上 public static final ，使之成为常量) 类实现该接口即可在类中调用该常量 类： 直接用 public static final 定义常量。 枚举： 在枚举类中直接写上常量值。 示例： public class ConstTest &#123; public static void main(String[] args) &#123; Signal sg = new Signal(); System.out.println(\"interface method: \" + sg.RED); SignalClassField scf = new SignalClassField(); System.out.println(\"class method: \" + scf.RED); System.out.println(\"emun method: \" + SignalEnum.RED.toString()); &#125;&#125;//interface methodinterface SignalInterface &#123; String RED = \"InterfaceRED\"; String GREEN = \"GREEN\"; String YELLOW = \"YELLOW\";&#125;class Signal implements SignalInterface &#123;&#125;//class methodclass SignalClassField &#123; public static final String RED = \"ClassRED\"; public static final String GREEN = \"GREEN\"; public static final String YELLOW = \"YELLOW\";&#125;//enum methodenum SignalEnum &#123; RED, GREEN, YELLOW&#125; 输出： interface method: InterfaceREDclass method: ClassREDemun method: RED","raw":null,"content":null,"categories":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/tags/Java/"}]},{"title":"关于Java Map的几个常见问题","slug":"关于Java Map的几个常见问题","date":"2017-03-11T12:33:42.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2017/03/11/关于Java Map的几个常见问题/","link":"","permalink":"http://linbingdong.com/2017/03/11/关于Java Map的几个常见问题/","excerpt":"列举几个关于Java Map的常见问题并给出答案。","text":"列举几个关于Java Map的常见问题并给出答案。 1. 将Map转化成ListMap接口提供了三种collection：key set,value set 和 key-value set，每一种都可以转成List。如下： //mapHashMap&lt;Integer,Integer&gt; map = new HashMap&lt;&gt;();map.put(1,10);map.put(2,20);map.put(3,30);//key listArrayList&lt;Integer&gt; keyList = new ArrayList&lt;&gt;(map.keySet());//value listArrayList&lt;Integer&gt; valueList = new ArrayList&lt;&gt;(map.values());//key-value listArrayList&lt;Map.Entry&lt;Integer,Integer&gt;&gt; entryList = new ArrayList&lt;&gt;(map.entrySet()); 2. 迭代Map最高效的遍历map的每个entry的方法如下： for (Map.Entry entry : map.entrySet())&#123; int key = (int) entry.getKey(); int value = (int) entry.getValue();&#125; 也可以使用iterator，特别是JDK 1.5之前。 Iterator itr = map.entrySet().iterator();while(itr.hasNext())&#123; Map.Entry entry = itr.next(); int key = (int) entry.getKey(); int value = (int) entry.getValue();&#125; 3. 根据key对map进行排序可以将Map.Entry放入一个list，然后自己实现Comparator来对list排序。 ArrayList&lt;Map.Entry&lt;Integer,Integer&gt;&gt; list = new ArrayList&lt;&gt;(map.entrySet());Collections.sort(list, new Comparator&lt;Map.Entry&lt;Integer, Integer&gt;&gt;() &#123; @Override public int compare(Map.Entry&lt;Integer, Integer&gt; e1, Map.Entry&lt;Integer, Integer&gt; e2) &#123; return e1.getKey().compareTo(e2.getKey()); &#125;&#125;); 可以使用SortedMap。SortedMap的一个实现类是TreeMap。TreeMap的构造器可以接受一个Comparator参数。如下： SortedMap&lt;Integer,Integer&gt; sortedMap = new TreeMap&lt;&gt;(new Comparator&lt;Integer&gt;() &#123; @Override public int compare(Integer k1, Integer k2) &#123; return k1.compareTo(k2); &#125;&#125;);sortedMap.putAll(map); 注：TreeMap默认对key进行排序。 4. 根据value对map进行排序ArrayList&lt;Map.Entry&lt;Integer,Integer&gt;&gt; list = new ArrayList&lt;&gt;(map.entrySet());Collections.sort(list, new Comparator&lt;Map.Entry&lt;Integer, Integer&gt;&gt;() &#123; @Override public int compare(Map.Entry&lt;Integer, Integer&gt; e1, Map.Entry&lt;Integer, Integer&gt; e2) &#123; return e1.getValue().compareTo(e2.getValue()); &#125;&#125;); 如果map中的value不重复，可以通过反转key-value对为value-key对来用上面的3中的TreeMap方法对其排序。该方法不推荐。 5. 初始化一个不可变Map正确的做法： public class Test&#123; private static Map&lt;Integer,Integer&gt; map1 = new HashMap&lt;&gt;(); static &#123; map1.put(8,9); map1.put(88,99); map1 = Collections.unmodifiableMap(map1); &#125;&#125; 错误的做法： public class Test&#123; private static final Map&lt;Integer,Integer&gt; map1 = new HashMap&lt;&gt;(); static &#123; map1.put(8,9); map1.put(88,99); &#125;&#125; 加了final只能确保不能 map1 = new，但是可以修改map1中的元素。 6. HashMap、TreeMap和HashTable的区别Map接口有三个比较重要的实现类，分别是HashMap、TreeMap和HashTable。 TreeMap是有序的，HashMap和HashTable是无序的。 Hashtable的方法是同步的，HashMap的方法不是同步的。这是两者最主要的区别。 这就意味着Hashtable是线程安全的，HashMap不是线程安全的。HashMap效率较高，Hashtable效率较低。如果对同步性或与遗留代码的兼容性没有任何要求，建议使用HashMap。查看Hashtable的源代码就可以发现，除构造函数外，Hashtable的所有 public 方法声明中都有 synchronized关键字，而HashMap的源码中则没有。 Hashtable不允许null值，HashMap允许null值（key和value都允许） 父类不同：Hashtable的父类是Dictionary，HashMap的父类是AbstractMap Hashtable中hash数组默认大小是11，增加的方式是 old*2+1。HashMap中hash数组的默认大小是16，而且一定是2的指数。 | HashMap | Hashtable | TreeMap-------------------------------------------------------iteration order | no | no | yesnull key-value | yes-yes | no-no | no-yessynchronized | no | yes | notime performance | O(1) | O(1) | O(log n)implementation | buckets | buckets | red-black tree 7. 创建一个空的Map如果希望该map为不可变的，则： map = Collections.emptyMap(); 否则： map = new HashMap();","raw":null,"content":null,"categories":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/tags/Java/"}]},{"title":"关于Java字符串的几个常见问题","slug":"关于Java字符串的几个常见问题","date":"2017-03-11T12:33:42.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2017/03/11/关于Java字符串的几个常见问题/","link":"","permalink":"http://linbingdong.com/2017/03/11/关于Java字符串的几个常见问题/","excerpt":"列举几个关于Java字符串的常见问题并给出答案。","text":"列举几个关于Java字符串的常见问题并给出答案。 1. 如何比较两个字符串？用”==”还是”equals”? “==”对比的是引用是否相同（是否同一个对象） “equals”对比的是值是否相同 除非想要比较两个字符串是否是同一个对象，否则应该一直使用”equals”。 2. 为什么对安全性敏感的信息更喜欢用char[]而不是String来存储？因为String是不可变对象，这就意味着只要String被创建，它们就会一直存在直到被垃圾回收器收集。因此，用String保存的信息安全性会降低。 3. 在switch语句中可以使用字符串吗？从Java7开始，可以在switch语句中使用字符串。 switch (aString) &#123; case \"a\": value = 1; break; case \"b\": value = 2; break;&#125; 4. 如何将字符串转为int？int n = Integer.parseInt(\"10\"); 虽然很简单，但是使用频率很高。 5. 如何用空格切分字符串？String[] strArray = aString.split(\"\\\\s+\"); 6. substring()方法会创建一个新字符串吗？从Java7开始，substring()方法会创建一个新的char数组，而不是使用已经存在的。 在Java6中，substring()方法不会创建一个新的char数组，如果想创建新的，可以用以下代码： str.substring(m,n) + \"\"; 7. String vs StringBuilder vs StringBufferString是不可变的，StringBuilder和StringBuffer都是可变的。 StringBuffer是同步的，线程安全的，效率低。 StringBuilder是非同步的，非线程安全，效率比StringBuffer高。","raw":null,"content":null,"categories":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/tags/Java/"}]},{"title":"Java 关键字 static 、final 总结","slug":"Java 关键字 static 、final 总结","date":"2017-03-11T12:33:42.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2017/03/11/Java 关键字 static 、final 总结/","link":"","permalink":"http://linbingdong.com/2017/03/11/Java 关键字 static 、final 总结/","excerpt":"static 和 final 是 Java 里两个关键字，在此对它们的用法做个总结。","text":"static 和 final 是 Java 里两个关键字，在此对它们的用法做个总结。 static被 static 修饰的域或方法属于类，独立于具体的对象。通过类名就可以直接调用，不需要实例化。访问时直接用类名.static 域和类名.static 方法即可。 static 域： 如果将域定义为 static ，每个类中只有一个这样的域。该类的所有对象都共享该 static 域。 static 方法： static 方法可以通过类名直接调用，该类的任何对象也可以调用（但不推荐）。 static 方法不能使用 this 和 super 关键字。 static 方法只能访问 static 域，不能访问实例域，因为实例域是与特定的对象相关联的。 static 方法必须被实现，而不能是 abstract 方法。 以下两种情况可以考虑使用 static 方法： 一个方法不需要访问对象状态，所需参数都是通过显示参数提供（如：Math.pow） 一个方法只需访问类的 static 域。 static 代码块： static 代码块，是在类中独立于类成员的 static 语句块，可以有多个，位置可以随便放，它不在任何的方法体内，JVM 加载类时会执行这些静态的代码块，如果 static 代码块有多个，JVM 将按照它们在类中出现的先后顺序依次执行它们，每个代码块只会被执行一次。所有 static 代码块都在 main 方法之前执行。 finalfinal 可以修饰域、方法和类。 final 域： 被 final 修饰的域是常量，值一旦给定就无法改变。 final 方法： 如果某个类不允许其子类覆盖某个方法，可以将该方法声明为 final 方法。 使用 final 方法有两个原因： 防止子类覆盖该方法，修改它的意义和实现。 提高效率。编译器遇到 final 方法就会使用内联机制。 final 类： final 类不能被继承。final 类中的方法自动成为 final 方法，因为不能被继承，也就不会被覆盖。 使用 final 类和 final 方法的目的都是为了确保它们不会在子类中改变语义。","raw":null,"content":null,"categories":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/tags/Java/"}]},{"title":"分布式系列文章——从ACID到CAP/BASE","slug":"分布式系列文章——从ACID到CAP:BASE","date":"2017-03-11T12:33:42.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2017/03/11/分布式系列文章——从ACID到CAP:BASE/","link":"","permalink":"http://linbingdong.com/2017/03/11/分布式系列文章——从ACID到CAP:BASE/","excerpt":"本文先介绍传统关系数据库中事务的ACID特性，再介绍分布式系统中的经典理论——CAP定理和BASE理论。","text":"本文先介绍传统关系数据库中事务的ACID特性，再介绍分布式系统中的经典理论——CAP定理和BASE理论。 事务事务的定义： 事务（Transaction）是由一系列对系统中数据进行访问与更新的操作所组成的一个程序执行逻辑单元（Unit），狭义上的事务特指数据库事务。 事务的作用： 当多个应用程序并发访问数据库时，事务可以在这些应用程序之间提供一个隔离方法，以防止彼此的操作相互干扰。 事务为数据库操作序列提供了一个从失败中恢复到正常状态的方法，同时提供了数据库即使在异常状态下仍能保持数据一致性的方法。 事务具有四个特性，分别是原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）和持久性（Durability）,简称为事务的ACID特性。 ACID原子性事务的原子性是指事务必须是一个原子的操作序列单元。事务中包含的各项操作在一次执行过程中，要么全部执行，要么全部不执行。 任何一项操作失败都将导致整个事务失败，同时其他已经被执行的操作都将被撤销并回滚。只有所有的操作全部成功，整个事务才算是成功完成。 一致性事务的一致性是指事务的执行不能破坏数据库数据的完整性和一致性，一个事务在执行前后，数据库都必须处于一致性状态。换句话说，事务的执行结果必须是使数据库从一个一致性状态转变到另一个一致性状态。 举个例子 银行的转账操作就是一个事务。假设A和B原来账户都有100元。此时A转账给B50元，转账结束后，应该是A账户减去50元变成50元，B账户增加50元变成150元。A、B的账户总和还是200元。转账前后，数据库就是从一个一致性状态（A100元，B100元，A、B共200元）转变到另一个一致性状态（A50元，B150元，A、B共200元）。假设转账结束后只扣了A账户，没有增加B账户，这时数据库就处于不一致的状态。 隔离性事务的隔离性是指在并发环境中，并发的事务是相互隔离的，事务之间互不干扰。 在标准的SQL规范中，定义的4个事务隔离级别，不同隔离级别对事务的处理不同。4个隔离级别分别是：未授权读取、授权读取、可重复读取和串行化。 下表展示了不同隔离级别下事务访问数据的差异 隔离级别 脏读 可重复读 幻读 未授权读取 存在 不可以 存在 授权读取 不存在 不可以 存在 可重复读取 不存在 可以 存在 串行化 不存在 可以 不存在 以上4个级别的隔离性依次增强，分别解决不同的问题。事务隔离级别越高，就越能保证数据的完整性和一致性，但同时对并发性能的影响也越大。 通常，对于绝大多数的应用来说，可以优先考虑将数据库系统的隔离级别设置为授权读取，这能够在避免脏读的同时保证较好的并发性能。尽管这种事务隔离级别会导致不可重复读、幻读和第二类丢失更新等并发问题，但较为科学的做法是在可能出现这类问题的个别场合中，由应用程序主动采用悲观锁或乐观锁来进行事务控制。 持久性事务的持久性又称为永久性，是指一个事务一旦提交，对数据库中对应数据的状态变更就应该是永久性的。即使发生系统崩溃或机器宕机等故障，只要数据库能够重新启动，那么一定能够将其恢复到事务成功结束时的状态。 分布式事务事务在分布式计算领域也得到了广泛的应用。在单机数据库中，我们很容易能够实现一套满足ACID特性的事务处理系统，但是在分布式数据库中，数据分散在各台不同的机器上，如何对这些数据进行分布式事务处理具有非常大的挑战。 分布式事务是指事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于分布式系统的不同节点之上。通常一个分布式事务会涉及对多个数据源或业务系统的操作。 举个例子来说明分布式事务。一个最典型的分布式事务场景是跨行的转账操作。该操作涉及调用两个异地的银行服务。其中一个是本地银行提供的取款服务，另一个是目标银行提供的存款服务，这两个服务本身是无状态且相互独立的，共同构成了一个完整的分布式事务。取款和存款两个步骤要么都执行，要么都不执行。否则，如果从本地银行取款成功，但是因为某种原因存款服务失败了，那么必须回滚到取款之前的状态，否则就会导致数据不一致。 从上面的例子可以看出，一个分布式事务可以看作是由多个分布式操作序列组成的，例如上面例子中的取款服务和存款服务，通常可以把这一系列分布式的操作序列称为子事务。由于分布式事务中，各个子事务的执行是分布式的，因此要实现一种能够保证ACID特性的分布式事务处理系统就显得格外复杂。 CAP定理CAP定理： 一个分布式系统不可能同时满足一致性（C:Consistency）、可用性（A:Availability）和分区容错性（P:Partition tolerance）这三个基本要求，最多只能满足其中的两项。 一致性在分布式环境中，一致性是指数据在多个副本之间是否能够保持一致的特性（这点跟ACID中的一致性含义不同）。 对于一个将数据副本分布在不同节点上的分布式系统来说，如果对第一个节点的数据进行了更新操作并且更新成功后，却没有使得第二个节点上的数据得到相应的更新，于是在对第二个节点的数据进行读取操作时，获取的依然是更新前的数据（称为脏数据），这就是典型的分布式数据不一致情况。在分布式系统中，如果能够做到针对一个数据项的更新操作执行成功后，所有的用户都能读取到最新的值，那么这样的系统就被认为具有强一致性（或严格的一致性）。 可用性可用性是指系统提供的服务必须一直处于可用的状态，对于用户的每一个操作请求总是能够在有限的时间内返回结果，如果超过了这个时间范围，那么系统就被认为是不可用的。 『有限的时间内』是一个在系统设计之初就设定好的运行指标，不同的系统会有很大的差别。比如对于一个在线搜索引擎来说，通常在0.5秒内需要给出用户搜索关键词对应的检索结果。而对应Hive来说，一次正常的查询时间可能在20秒到30秒之间。 『返回结果』是可用性的另一个非常重要的指标，它要求系统在完成对用户请求的处理后，返回一个正常的响应结果。正常的响应结果通常能够明确地反映出对请求的处理结果，及成功或失败，而不是一个让用户感到困惑的返回结果。 让我们再来看看上面提到的在线搜索引擎的例子，如果用户输入指定的搜索关键词后，返回的结果是一个系统错误，比如”OutOfMemoryErroe”或”System Has Crashed”等提示语，那么我们认为此时系统是不可用的。 分区容错性分区容错性要求一个分布式系统需要具备如下特性：分布式系统在遇到任何网络分区故障的时候，仍然能够保证对外提供满足一致性和可用性的服务，除非是整个网络环境都发生了故障。 网络分区是指在分布式系统中，不同的节点分布在不同的子网络（机房或异地网络等）中，由于一些特殊的原因导致这些子网络之间出现网络不连通的状况，但各个子网络的内部网络是正常的，从而导致整个系统的网络环境被切分成了若干个孤立的区域。 以上就是对CAP定理中一致性、可用性和分区容错性的讲解。 既然一个分布式系统无法同时满足上述三个要求，而只能满足其中的两项，因此在对CAP定理应用时，我们就需要抛弃其中的一项，下表是抛弃CAP中任意一项特性的场景说明。 CAP 说明 放弃P 如果希望能够避免系统出现分区容错性问题，一种较为简单的做法是将所有的数据（或者仅仅是哪些与事务相关的数据）都放在一个分布式节点上。这样做虽然无法100%保证系统不会出错，但至少不会碰到由于网络分区带来的负面影响。但同时需要注意的是，放弃P的同时也就意味着放弃了系统的可扩展性 放弃A 一旦系统遇到网络分区或其他故障或为了保证一致性时，放弃可用性，那么受到影响的服务需要等待一定的时间，因此在等待期间系统无法对外提供正常的服务，即不可用 放弃C 这里所说的放弃一致性，实际上指的是放弃数据的强一致性，而保留数据的最终一致性。这样的系统无法保证数据保持实时的一致性，但是能够承诺的是，数据最终会达到一个一致的状态。 需要明确的一点是：对于一个分布式系统而言，分区容错性可以说是一个最基本的要求。因为既然是一个分布式系统，那么分布式系统中的组件必然需要被部署到不同的节点，否则也就无所谓的分布式系统了，因此必然出现子网络。而对于分布式系统而言，网络问题又是一个必定会出现的异常情况，因此分区容错性也就成为了一个分布式系统必然需要面对和解决的问题。因此系统架构师往往需要把精力花在如何根据业务特点在C（一致性）和A（可用性）之间寻求平衡。 BASE理论BASE是Basically Available(基本可用）、Soft state(软状态）和Eventually consistent(最终一致性）三个短语的简写。BASE是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网系统分布式实践的总结，是基于CAP定理逐步演化而来的，其核心思想是即使无法做到强一致性，但每个应用都可以根据自身的业务特点，采用适当的方法来使系统达到最终一致性。接下来，我们着重对BASE中的三要素进行讲解。 基本可用基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性——但请注意，这绝不等价于系统不可用。一下就是两个”基本可用”的例子。 响应时间上的损失：正常情况下，一个在线搜索引擎需要在0.5秒之内返回给用户相应的查询结果，但由于出现故障（比如系统部分机房发生断电或断网故障），查询结果的响应时间增加到了1~2秒。 功能上的损失：正常情况下，在一个电子商务网站（比如淘宝）上购物，消费者几乎能够顺利地完成每一笔订单。但在一些节日大促购物高峰的时候（比如双十一、双十二），由于消费者的购物行为激增，为了保护系统的稳定性（或者保证一致性），部分消费者可能会被引导到一个降级页面，如下： 软状态软状态是指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同的数据副本之间进行数据同步的过程存在延时。 最终一致性最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。 最终一致性是一种特殊的弱一致性：系统能够保证在没有其他新的更新操作的情况下，数据最终一定能够达到一致的状态，因此所有客户端对系统的数据访问都能够获取到最新的值。同时，在没有发生故障的前提下，数据到达一致状态的时间延迟，取决于网络延迟、系统负载和数据复制方案设计等因素。 在实际工程实践中，最终一致性存在一下五类主要变种。 因果一致性(Causal consistency) 读己之所写(Read your writes) 会话一致性(Session consistency) 单调读一致性(Monotonic read consistency) 单调写一致性(Monotonic write consistency) 以上就是最终一致性的五种常见的变种，在实际系统实践中，可以将其中的若干个变种互相结合起来，以构建一个具有最终一致性特性的分布式系统。事实上，最终一致性并不是只有那些大型分布式系统才涉及的特性，许多现代的关系型数据库都采用了最终一致性模型。在现代关系型数据库中（比如MySQL和PostgreSQL），大多都会采用同步或异步方式来实现主备数据复制技术。在同步方式中，数据的复制过程通常是更新事务的一部分，因此在事务完成后，主备数据库的数据就会达到一致。而在异步方式中，备库的更新往往会存在延时，这取决于事务日志在主备数据库之间传输的时间长短。如果传输时间过长或者甚至在日志传输过程中出现异常导致无法及时将事务应用到备库上，那么很显然，从备库中读取的数据将是旧的，因此就出现了数据不一致的情况。当然，无论是采用多次重试还是人为数据订正，关系型数据库还是能够保证最终数据达到一致，这就是系统提供最终一致性保证的经典案例。 参考资料 《从Paxos到ZooKeeper——分布式一致性原理与实践》","raw":null,"content":null,"categories":[{"name":"分布式系统","slug":"分布式系统","permalink":"http://linbingdong.com/categories/分布式系统/"}],"tags":[{"name":"分布式系统","slug":"分布式系统","permalink":"http://linbingdong.com/tags/分布式系统/"}]},{"title":"分布式系列文章——分布式系统的特点及问题","slug":"分布式系列文章——分布式系统的特点及问题","date":"2017-03-11T12:33:42.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2017/03/11/分布式系列文章——分布式系统的特点及问题/","link":"","permalink":"http://linbingdong.com/2017/03/11/分布式系列文章——分布式系统的特点及问题/","excerpt":"随着数据量越来越大，计算机需要处理的业务越来越复杂，单机已经无法满足需求。一个有效的解决方案是把众多廉价的计算机整合起来，共同提供服务，这就是分布式系统。接下来介绍分布式系统的定义、特点，以及分布式环境中存在的问题和挑战。","text":"随着数据量越来越大，计算机需要处理的业务越来越复杂，单机已经无法满足需求。一个有效的解决方案是把众多廉价的计算机整合起来，共同提供服务，这就是分布式系统。接下来介绍分布式系统的定义、特点，以及分布式环境中存在的问题和挑战。 分布式系统定义在《分布式系统概念与设计》一书中，对分布式系统做了 如下定义： 分布式系统是一个硬件或软件组件分布在不同的网络计算机上，彼此之间仅仅通过消息传递进行通信和协调的系统。 也就是说一个分布式系统中的计算机在空间部署上可以是随意分布的，这些计算机可能被放在不同的机柜上，也可能在不同的机房中，甚至分布在不同的城市。这些计算机之间通过网络来通信。 分布式的特点分布式系统有如下体征： 分布性分布式系统中的多台计算机在空间上随意分步。当然，机器的分布情况也会随时变动。 对等性分布式系统中的计算机没有主/从之分，既没有控制整个系统的主机，也没有被控制的从机，组成分布式系统的所有计算机节点都是对等的。 并发性一个分布式系统中的多个节点，可能会并发地操作一些共享的资源，比如数据库或分布式存储等。如何准确并高效地协调分布式并发操作也成为了分布式系统架构与设计中最大的挑战。 缺乏全局时钟在分布式系统中，很难定义两个事件究竟谁先谁后，原因就是分布式系统缺乏一个全局的时钟控制序列。 故障总是会发生组成分布式系统的所有计算机，都有可能发生任何形式的故障。实践表明，在分布式系统中，计算机发生故障是比较常见的。因此，在分布式系统设计时，必须考虑到该问题。 分布式环境的各种问题分布式系统体系结构从其出现之初就伴随着诸多的难题和挑战，本节介绍分布式系统中的一些典型的问题。 通信异常分布式系统中个计算机之间是通过网络进行通信的。由于网络本身的不可靠性，每次网络通信都会伴随着网络不可用的风险。即使分布式系统各节点之间的网络通信能够正常进行，其延时也会远远大于单机操作。在分布式系统中，消息延时和消息丢失非常普遍。 网络分区当网络发生异常情况，可能导致分布式系统中某些节点之间能够正常通信，而某些节点之间无法通信——该现象就是网络分区，就是俗称的『脑裂』。当网络分区出现时，分布式系统就会出现局部小集群，小集群内计算机可以相互通信，小集群之间计算机无法通信。这就对分布式一致性提出了非常大的挑战。 三态因为在分布式系统中，网络可能会出现各式各样的问题，因此分布式系统的每一次请求和响应，存在特有的『三态』概念，即成功、失败与超时。在传统的单机系统中，应用程序在调用一个函数之后，能够得到一个非常明确的相应：成功或失败。而在分布式系统中，由于网络是不可靠的，当网络出现异常的情况下，就可能出现超时现象，发生消息丢失现象。 节点故障节点故障是分布式环境下一个比较常见的问题，指的是组成分布式系统的服务器节点出现宕机或『僵死』现象。通常根据经验来说，每个节点都有可能出现故障，并且每天都在发生。","raw":null,"content":null,"categories":[{"name":"分布式系统","slug":"分布式系统","permalink":"http://linbingdong.com/categories/分布式系统/"}],"tags":[{"name":"分布式系统","slug":"分布式系统","permalink":"http://linbingdong.com/tags/分布式系统/"}]},{"title":"全球分布式数据库：Google Spanner（论文翻译）","slug":"全球分布式数据库：Google Spanner（论文翻译）","date":"2017-03-11T12:33:42.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2017/03/11/全球分布式数据库：Google Spanner（论文翻译）/","link":"","permalink":"http://linbingdong.com/2017/03/11/全球分布式数据库：Google Spanner（论文翻译）/","excerpt":"本文由厦门大学计算机系教师林子雨翻译，翻译质量很高，本人只对极少数翻译得不太恰当的地方进行了修改。\n【摘要】Spanner 是谷歌公司研发的、可扩展的、多版本、全球分布式、同步复制数据库。它是第一个把数据分布在全球范围内的系统，并且支持外部一致性的分布式事务。本文描述了 Spanner 的架构、特性、不同设计决策的背后机理和一个新的时间 API，这个 API 可以暴露时钟的不确定性。这个 API 及其实现，对于支持外部一致性和许多强大特性而言，是非常重要的，这些强大特性包括:非阻塞的读、不采用锁机制的只读事务、原子模式变更。","text":"本文由厦门大学计算机系教师林子雨翻译，翻译质量很高，本人只对极少数翻译得不太恰当的地方进行了修改。 【摘要】Spanner 是谷歌公司研发的、可扩展的、多版本、全球分布式、同步复制数据库。它是第一个把数据分布在全球范围内的系统，并且支持外部一致性的分布式事务。本文描述了 Spanner 的架构、特性、不同设计决策的背后机理和一个新的时间 API，这个 API 可以暴露时钟的不确定性。这个 API 及其实现，对于支持外部一致性和许多强大特性而言，是非常重要的，这些强大特性包括:非阻塞的读、不采用锁机制的只读事务、原子模式变更。 【关键词】Google Spanner, Bigtable, distributed database 1 介绍Spanner 是一个可扩展的、全球分布式的数据库，是在谷歌公司设计、开发和部署的。 在最高抽象层面，Spanner 就是一个数据库，把数据分片存储在许多 Paxos[21]状态机上，这些机器位于遍布全球的数据中心内。复制技术可以用来服务于全球可用性和地理局部性。客户端会自动在副本之间进行失败恢复。随着数据的变化和服务器的变化，Spanner 会自动把数据进行重新分片，从而有效应对负载变化和处理失败。Spanner 被设计成可以扩展到几百万个机器节点，跨越成百上千个数据中心，具备几万亿数据库行的规模。 应用可以借助于 Spanner 来实现高可用性，通过在一个洲的内部和跨越不同的洲之间复制数据，保证即使面对大范围的自然灾害时数据依然可用。我们最初的客户是 F1[35]，一个谷歌广告后台的重新编程实现。F1 使用了跨越美国的 5 个副本。绝大多数其他应用很可能会在属于同一个地理范围内的 3-5 个数据中心内放置数据副本，采用相对独立的失败模式。也就是说，许多应用都会首先选择低延迟，而不是高可用性，只要系统能够从 1-2 个数据中心失败中恢复过来。 Spanner 的主要工作，就是管理跨越多个数据中心的数据副本，但是，在我们的分布式系统体系架构之上设计和实现重要的数据库特性方面，我们也花费了大量的时间。尽管有许多项目可以很好地使用 BigTable[9]，我们也不断收到来自客户的抱怨，客户反映 BigTable 无法应用到一些特定类型的应用上面，比如具备复杂可变的模式，或者对于在大范围内分布的多个副本数据具有较高的一致性要求。其他研究人员也提出了类似的抱怨[37]。谷歌的许多应用已经选择使用 Megastore[5]，主要是因为它的半关系数据模型和对同步复制的支持，尽管 Megastore 具备较差的写操作吞吐量。由于上述多个方面的因素，Spanner 已经从一个类似 BigTable 的单一版本的键值存储，演化成为一个具有时间属性的多版本的数据库。数据被存储到模式化的、半关系的表中，数据被版本化，每个版本都会自动以提交时间作为时间戳，旧版本的数据会更容易被垃圾回收。应用可以读取旧版本的数据。Spanner 支持通用的事务，提供了基于 SQL 的查询语言。 作为一个全球分布式数据库，Spanner 提供了几个有趣的特性:第一，在数据的副本配置方面，应用可以在一个很细的粒度上进行动态控制。应用可以详细规定，哪些数据中心包含哪些数据，数据距离用户有多远(控制用户读取数据的延迟)，不同数据副本之间距离有多远(控制写操作的延迟)，以及需要维护多少个副本(控制可用性和读操作性能)。数据也可以被动态和透明地在数据中心之间进行移动，从而平衡不同数据中心内资源的使用。第二， Spanner 有两个重要的特性，很难在一个分布式数据库上实现，即 Spanner 提供了读和写操作的外部一致性，以及在一个时间戳下面的跨越数据库的全球一致性的读操作。这些特性使得 Spanner 可以支持一致的备份、一致的 MapReduce 执行[12]和原子模式变更，所有都是在全球范围内实现，即使存在正在处理中的事务也可以。 之所以可以支持这些特性，是因为 Spanner 可以为事务分配全球范围内有意义的提交时间戳，即使事务可能是分布式的。这些时间戳反映了事务序列化的顺序。除此以外，这些序列化的顺序满足了外部一致性的要求:如果一个事务 T1 在另一个事务 T2 开始之前就已经提交了，那么，T1 的时间戳就要比 T2 的时间戳小。Spanner 是第一个可以在全球范围内提供这种保证的系统。 实现这种特性的关键技术就是一个新的 TrueTime API 及其实现。这个 API 可以直接暴露时钟不确定性，Spanner 时间戳的保证就是取决于这个 API 实现的界限。如果这个不确定性很大，Spanner 就降低速度来等待这个大的不确定性结束。谷歌的簇管理器软件提供了一个 TrueTime API 的实现。这种实现可以保持较小的不确定性(通常小于 10ms)，主要是借助于现代时钟参考值(比如 GPS 和原子钟)。 第 2 部分描述了 Spanner 实现的结构、特性集和工程方面的决策;第 3 部分介绍我们的新的 TrueTime API，并且描述了它的实现;第 4 部分描述了 Spanner 如何使用 TrueTime 来实现外部一致性的分布式事务、不用锁机制的只读事务和原子模式更新。第 5 部分提供了测试 Spanner 性能和 TrueTime 行为的测试基准，并讨论了 F1 的经验。第 6、7 和 8 部分讨论了相关工作，并给出总结。 2 实现本部分内容描述了 Spanner 的结构和背后的实现机理，然后描述了目录抽象，它被用来管理副本和局部性，并介绍了数据的转移单位。最后，将讨论我们的数据模型，从而说明为什么 Spanner 看起来更加像一个关系数据库，而不是一个键值数据库;还会讨论应用如何可以控制数据的局部性。 一个 Spanner 部署称为一个 universe。假设 Spanner 在全球范围内管理数据，那么，将会只有可数的、运行中的 universe。我们当前正在运行一个测试用的 universe，一个部署/线上用的 universe 和一个只用于线上应用的 universe。 Spanner 被组织成许多个 zone 的集合，每个 zone 都大概像一个 BigTable 服务器的部署。 zone 是管理部署的基本单元。zone 的集合也是数据可以被复制到的位置的集合。当新的数据中心加入服务，或者老的数据中心被关闭时，zone 可以被加入到一个运行的系统中，或者从中移除。zone 也是物理隔离的单元，在一个数据中心中，可能有一个或者多个 zone， 例如，当属于不同应用的数据必须被分区存储到同一个数据中心的不同服务器集合中时，一个数据中心就会有多个 zone 。 图 1 显示了在一个 Spanner 的 universe 中的服务器。一个 zone 包括一个 zonemaster， 和一百至几千个 spanserver。Zonemaster 把数据分配给 spanserver，spanserver 把数据提供给客户端。客户端使用每个 zone 上面的 location proxy 来定位可以为自己提供数据的 spanserver。Universe master 和 placement driver，当前都只有一个。Universe master 主要是一个控制台，它显示了关于 zone 的各种状态信息，可以用于相互之间的调试。Placement driver 会周期性地与 spanserver 进行交互，来发现那些需要被转移的数据，或者是为了满足新的副本约束条件，或者是为了进行负载均衡。 2.1 Spanserver 软件栈本部分内容主要关注 spanserver 实现，来解释复制和分布式事务是如何被架构到我们的基于 BigTable 的实现之上的。图 2 显示了软件栈。在底部，每个 spanserver 负载管理 100-1000 个称为 tablet 的数据结构的实例。一个 tablet 就类似于 BigTable 中的 tablet，也实现了下面的映射: (key:string, timestamp:int64)-&gt;string 与 BigTable 不同的是，Spanner 会把时间戳分配给数据，这种非常重要的方式，使得 Spanner 更像一个多版本数据库，而不是一个键值存储。一个 tablet 的状态是存储在类似于 B-树的文件集合和写前(write-ahead)的日志中，所有这些都会被保存到一个分布式的文件系统中，这个分布式文件系统被称为 Colossus，它继承自 Google File System。 为了支持复制，每个 spanserver 会在每个 tablet 上面实现一个单个的 Paxos 状态机。一个之前实现的Spanner 可以支持在每个 tablet 上面实现多个 Paxos 状态机器，它可以允许更加灵活的复制配置，但是，这种设计过于复杂，被我们舍弃了。每个状态机器都会在相应的 tablet 中保存自己的元数据和日志。我们的 Paxos 实现支持长寿命的领导者（采用基于时间的领导者租约），时间通常在 0 到 10 秒之间。当前的 Spanner 实现中，会对每个 Paxos 写操作进行两次记录:一次是写入到 tablet 日志中，一次是写入到 Paxos 日志中。这种做法只是权宜之计，我们以后会进行完善。我们在 Paxos 实现上采用了管道化的方式，从而可以在存在广域网延迟时改进 Spanner 的吞吐量，但是，Paxos 会把写操作按照顺序的方式执行。 Paxos 状态机是用来实现一系列被一致性复制的映射。每个副本的键值映射状态，都会被保存到相应的 tablet 中。写操作必须在领导者上初始化 Paxos 协议，读操作可以直接从底层的任何副本的 tablet 中访问状态信息，只要这个副本足够新。副本的集合被称为一个 Paxos group。 对于每个是领导者的副本而言，每个 spanserver 会实现一个锁表来实现并发控制。这个锁表包含了两阶段锁机制的状态:它把键的值域映射到锁状态上面。注意，采用一个长寿命的 Paxos 领导者，对于有效管理锁表而言是非常关键的。在 BigTable 和 Spanner 中，我们都专门为长事务做了设计，比如，对于报表操作，可能要持续几分钟，当存在冲突时，采用乐观并发控制机制会表现出很差的性能。对于那些需要同步的操作，比如事务型的读操作，需要获得锁表中的锁，而其他类型的操作则可以不理会锁表。 对于每个扮演领导者角色的副本，每个 spanserver 也会实施一个事务管理器来支持分布式事务。这个事务管理器被用来实现一个 participant leader，该组内的其他副本则是作为 participant slaves。如果一个事务只包含一个 Paxos 组(对于许多事务而言都是如此)，它就可以绕过事务管理器，因为锁表和 Paxos 二者一起可以保证事务性。如果一个事务包含了多 于一个 Paxos 组，那些组的领导者之间会彼此协调合作完成两阶段提交。其中一个参与者组，会被选为协调者，该组的 participant leader 被称为 coordinator leader，该组的 participant slaves 被称为 coordinator slaves。每个事务管理器的状态，会被保存到底层的 Paxos 组。 2.2 目录和放置在一系列键值映射的上层，Spanner 实现支持一个被称为“目录”的桶抽象，也就是包含公共前缀的连续键的集合。(选择“目录”作为名称，主要是由于历史沿袭的考虑，实际 上更好的名称应该是“桶”)。我们会在第 2.3 节解释前缀的源头。对目录的支持，可以让应用通过选择合适的键来控制数据的局部性。 一个目录是数据放置的基本单元。属于一个目录的所有数据，都具有相同的副本配置。 当数据在不同的 Paxos 组之间进行移动时，会一个目录一个目录地转移，如图 3 所示。Spanner 可能会移动一个目录从而减轻一个 Paxos 组的负担，也可能会把那些被频繁地一起访问的目录都放置到同一个组中，或者会把一个目录转移到距离访问者更近的地方。当客户端操作正在进行时，也可以进行目录的转移。我们可以预期在几秒内转移 50MB 的目录。 一个 Paxos 组可以包含多个目录，这意味着一个 Spanner tablet 是不同于一个 BigTable tablet 的。一个 Spanner tablet 没有必要是一个行空间内按照词典顺序连续的分区，相反，它可以是行空间内的多个分区。我们做出这个决定，是因为这样做可以让多个被频繁一起访问的目录被整合到一起。 Movedir 是一个后台任务，用来在不同的 Paxos 组之间转移目录[14]。Movedir 也用来为 Paxos 组增加和删除副本[25]，因为 Spanner 目前还不支持在一个 Paxos 内部进行配置的变更。 Movedir 并不是作为一个事务来实现，这样可以避免在一个块数据转移过程中阻塞正在进行的读操作和写操作。相反，Movedir 会注册一个事实(fact)，表明它要转移数据，然后在后台运行转移数据。当它几乎快要转移完指定数量的数据时，就会启动一个事务来自动转移那部分数据，并且为两个 Paxos 组更新元数据。 一个目录也是一个应用可以指定的地理复制属性(即放置策略)的最小单元。我们的放置规范语言的设计，把管理复制的配置这个任务单独分离出来。管理员需要控制两个维度: 副本的数量和类型，以及这些副本的地理放置属性。他们在这两个维度里面创建了一个命名 选项的菜单。通过为每个数据库或单独的目录增加这些命名选项的组合，一个应用就可以控制数据的复制。例如，一个应用可能会在自己的目录里存储每个终端用户的数据，这就有可能使得用户 A 的数据在欧洲有三个副本，用户 B 的数据在北美有 5 个副本。 为了表达的清晰性，我们已经做了尽量简化。事实上，当一个目录变得太大时，Spanner 会把它分片存储。每个分片可能会被保存到不同的 Paxos 组上(因此就意味着来自不同的服 务器)。Movedir 在不同组之间转移的是分片，而不是转移整个目录。 2.3 数据模型Spanner 会把下面的数据特性集合暴露给应用:基于模式化的半关系表的数据模型，查询语言和通用事务。支持这些特性的动机，是受到许多因素驱动的。需要支持模式化的半关系表是由 Megastore[5]的普及来支持的。在谷歌内部至少有 300 个应用使用 Megastore(尽 管它具有相对低的性能)，因为它的数据模型要比 BigTable 简单，更易于管理，并且支持在跨数据中心层面进行同步复制。BigTable 只可以支持跨数据中心的最终事务一致性。使用 Megastore 的著名的谷歌应用是 Gmail,Picasa,Calendar,Android Market, AppEngine。在 Spanner 中需要支持 SQL 类型的查询语言，也很显然是非常必要的，因为 Dremel[28]作为交互式分析工具已经非常普及。最后，在 BigTable 中跨行事务的缺乏来导致了用户频繁的抱怨; Percolator[32]的开发就是用来部分解决这个问题的。一些作者都在抱怨，通用的两阶段提交的代价过于昂贵，因为它会带来可用性问题和性能问题[9][10][19]。我们认为，最好让应用 程序开发人员来处理由于过度使用事务引起的性能问题，而不是总是围绕着“缺少事务”进 行编程。在 Paxos 上运行两阶段提交弱化了可用性问题。 应用的数据模型是架构在被目录桶装的键值映射层之上。一个应用会在一个 universe 中创建一个或者多个数据库。每个数据库可以包含无限数量的模式化的表。每个表都和关系数据库表类似，具备行、列和版本值。我们不会详细介绍 Spanner 的查询语言，它看起来很像 SQL，只是做了一些扩展。 Spanner 的数据模型不是纯粹关系型的，它的行必须有名称。更准确地说，每个表都需 要有包含一个或多个主键列的排序集合。这种需求，让 Spanner 看起来仍然有点像键值存储: 主键形成了一个行的名称，每个表都定义了从主键列到非主键列的映射。当一个行存在时，必须要求已经给行的一些键定义了一些值(即使是 NULL)。采用这种结构是很有用的，因为这可以让应用通过选择键来控制数据的局部性。 图 4 包含了一个 Spanner 模式的实例，它是以每个用户和每个相册为基础存储图片元数据。这个模式语言和 Megastore 的类似，同时增加了额外的要求，即每个 Spanner 数据库必 须被客户端分割成一个或多个表的层次结构(hierarchy)。客户端应用会使用 INTERLEAVE IN 语句在数据库模式中声明这个层次结构。这个层次结构上面的表，是一个目录表。目录表中的每行都具有键 K，和子孙表中的所有以 K 开始(以字典顺序排序)的行一起，构成了一个目录。ON DELETE CASCADE 意味着，如果删除目录中的一个行，也会级联删除所有相关的子孙行。这个图也解释了这个实例数据库的交织层次(interleaved layout)，例如 Albums(2,1) 代表了来自 Albums 表的、对应于 user_id=2 和 album_id=1 的行。这种表的交织层次形成目录，是非常重要的，因为它允许客户端来描述存在于多个表之间的位置关系，这对于一个分片的分布式数据库的性能而言是很重要的。没有它的话，Spanner 就无法知道最重要的位置关系。 本部分内容描述 TrueTime API，并大概给出它的实现方法。我们把大量细节内容放在另一篇论文中，我们的目标是展示这种 API 的力量。表 1 列出了 API 的方法。TrueTime 会显式地把时间表达成 TTinterval，这是一个时间区间，具有有界限的时间不确定性(不像其他 的标准时间接口，没有为客户端提供―不确定性‖这种概念)。TTinterval 区间的端点是 TTstamp 类型。TT.now()方法会返回一个 TTinterval，它可以保证包含 TT.now()方法在调用时的绝对 时间。这个时间和具备闰秒涂抹(leap-second smearing)的 UNIX 时间一样。把即时误差边 界定义为 ε,平均误差边界为ε。TT.after()和 TT.before()方法是针对 TT.now()的便捷的包装器。 表示一个事件 e 的绝对时间，可以利用函数 tabs(e)。如果用更加形式化的术语，TrueTime 可以保证，对于一个调用 tt=TT.now()，有 tt.earliest≤tabs(enow)≤tt.latest，其中， enow 是调用的事件。 在底层，TrueTime 使用的时间是 GPS 和原子钟。TrueTime 使用两种类型的时间，是因为它们有不同的失败模式。GPS 参考时间的弱点是天线和接收器失效、局部电磁干扰和相关失败(比如设计上的缺陷导致无法正确处理闰秒和电子欺骗)，以及 GPS 系统运行中断。原子钟也会失效，不过失效的方式和 GPS 无关，不同原子钟之间的失效也没有彼此关联。 由于存在频率误差，在经过很长的时间以后，原子钟都会产生明显误差。 TrueTime 是由每个数据中心上面的许多 time master 机器和每台机器上的一个 timeslave daemon 来共同实现的。大多数 master 都有具备专用天线的 GPS 接收器，这些 master 在物理上是相互隔离的，这样可以减少天线失效、电磁干扰和电子欺骗的影响。剩余的 master (我们称为 Armageddon master)则配备了原子钟。一个原子钟并不是很昂贵:一个 Armageddon master 的花费和一个 GPS master 的花费是同一个数量级的。所有 master 的时间 参考值都会进行彼此校对。每个 master 也会交叉检查时间参考值和本地时间的比值，如果二者差别太大，就会把自己驱逐出去。在同步期间，Armageddon master 会表现出一个逐渐增加的时间不确定性，这是由保守应用的最差时钟漂移引起的。GPS master 表现出的时间不确定性几乎接近于 0。 每个 daemon 会从许多 master[29]中收集投票，获得时间参考值，从而减少误差。被选中的 master 中，有些 master 是 GPS master，是从附近的数据中心获得的，剩余的 GPS master 是从远处的数据中心获得的;还有一些是 Armageddon master。Daemon 会使用一个 Marzullo 算法[27]的变种，来探测和拒绝欺骗，并且把本地时钟同步到非撒谎 master 的时间参考值。 为了免受较差的本地时钟的影响，我们会根据组件规范和运行环境确定一个界限，如果机器的本地时钟误差频繁超出这个界限，这个机器就会被驱逐出去。 在同步期间，一个 daemon 会表现出逐渐增加的时间不确定性。ε 是从保守应用的最差 时钟漂移中得到的。ε 也取决于 time master 的不确定性，以及与 time master 之间的通讯延迟。在我们的线上应用环境中，ε 通常是一个关于时间的锯齿形函数。在每个投票间隔中， ε 会在 1 到 7ms 之间变化。因此，在大多数情况下，ε的值是 4ms。Daemon 的投票间隔，在当前是 30 秒，当前使用的时钟漂移比率是 200 微秒/秒，二者一起意味着 0 到 6ms 的锯齿形边界。剩余的 1ms 主要来自到 time master 的通讯延迟。在失败的时候，超过这个锯齿形边界也是有可能的。例如，偶尔的 time master 不确定性，可能会引起整个数据中心范围内的 ε 值的增加。类似的，过载的机器或者网络连接，都会导致 ε 值偶尔地局部增大。 4 并发控制本部分内容描述 TrueTime 如何可以用来保证并发控制的正确性，以及这些属性如何用来实现一些关键特性，比如外部一致性的事务、无锁机制的只读事务、针对历史数据的非阻塞读。这些特性可以保证，在时间戳为 t 的时刻的数据库读操作，一定只能看到在 t 时刻之 前已经提交的事务。 进一步说，把 Spanner 客户端的写操作和 Paxos 看到的写操作这二者进行区分，是非常重要的，我们把 Paxos 看到的写操作称为 Paxos 写操作。例如，两阶段提交会为准备提交阶段生成一个 Paxos 写操作，这时不会有相应的客户端写操作。 4.1 时间戳管理表 2 列出了 Spanner 支持的操作的类型。Spanner 可以支持读写事务、只读事务(预先声明的快照隔离事务)和快照读。独立写操作，会被当成读写事务来执行。非快照独立读操作，会被当成只读事务来执行。二者都是在内部进行 retry，客户端不用进行这种 retry loop。 一个只读事务具备快照隔离的性能优势[6]。一个只读事务必须事先被声明不会包含任何写操作，它并不是一个简单的不包含写操作的读写事务。在一个只读事务中的读操作，在执行时会采用一个系统选择的时间戳，不包含锁机制，因此，后面到达的写操作不会被阻塞。 在一个只读事务中的读操作，可以到任何足够新的副本上去执行(见第 4.1.3 节)。 一个快照读操作，是针对历史数据的读取，执行过程中，不需要锁机制。一个客户端可以为快照读确定一个时间戳，或者提供一个时间范围让 Spanner 来自动选择时间戳。不管是 哪种情况，快照读操作都可以在任何具有足够新的副本上执行。 对于只读事务和快照读而言，一旦已经选定一个时间戳，那么，提交就是不可避免的，除非在那个时间点的数据已经被垃圾回收了。因此，客户端不必在 retry loop 中缓存结果。 当一个服务器失效的时候，客户端就可以使用同样的时间戳和当前的读位置，在另外一个服务器上继续执行读操作。 4.1.1 Paxos 领导者租约Spanner 的 Paxos 实现中使用了时间化的租约，来实现长时间的领导者地位(默认是 10秒)。一个潜在的领导者会发起请求，请求时间化的租约投票，在收到指定数量的投票后，这个领导者就可以确定自己拥有了一个租约。一个副本在成功完成一个写操作后，会隐式地延期自己的租约。对于一个领导者而言，如果它的租约快要到期了，就要显示地请求租约延期。另一个领导者的租约有个时间区间，这个时间区间的起点就是这个领导者获得指定数量的投票那一刻，时间区间的终点就是这个领导者失去指定数量的投票的那一刻(因为有些投 票已经过期了)。Spanner 依赖于下面这些“不连贯性”:对于每个 Paxos 组，每个 Paxos 领 导者的租约时间区间，是和其他领导者的时间区间完全隔离的。附录 A 显示了如何强制实现这些不连贯性。 Spanner 实现允许一个 Paxos 领导者通过把 slave 从租约投票中释放出来这种方式，实现领导者的退位。为了保持这种彼此隔离的不连贯性，Spanner 会对什么时候退位做出限制。把 smax 定义为一个领导者可以使用的最大的时间戳。在退位之前，一个领导者必须等到 TT.after(smax)是真。 4.1.2 为读写事务分配时间戳事务读和写采用两段锁协议。当所有的锁都已经获得以后，在任何锁被释放之前，就可以给事务分配时间戳。对于一个给定的事务，Spanner 会为事务分配时间戳，这个时间戳是 Paxos 分配给 Paxos 写操作的，它代表了事务提交的时间。 Spanner 依赖下面这些单调性:在每个 Paxos 组内，Spanner 会以单调增加的顺序给每个 Paxos 写操作分配时间戳，即使在跨越多个领导者时也是如此。一个单个的领导者副本，可以很容易地以单调增加的方式分配时间戳。在多个领导者之间就会强制实现彼此隔离的不连 贯:一个领导者必须只能分配属于它自己租约时间区间内的时间戳。要注意到，一旦一个时间戳 s 被分配，smax 就会被增加到 s，从而保证彼此隔离性(不连贯性)。 Spanner 也会实现下面的外部一致性:如果一个事务 T2 在事务 T1 提交以后开始执行， 那么，事务 T2 的时间戳一定比事务 T1 的时间戳大。对于一个事务 Ti 而言，定义开始和提交事件eistart和eicommit，事务提交时间为si。对外部一致性的要求就变成了:tabs(e1commit )&lt;tabs(e2start ) s1&lt;s2。执行事务的协议和分配时间戳的协议，遵守两条规则，二者一起保证外部一致性。对于一个写操作 Ti 而言，担任协调者的领导者发出的提交请求的事件为eiserver 。 Start. 为一个事务 Ti 担任协调者的领导者分配一个提交时间戳 si，不会小于 TT.now().latest 的值，TT.now().latest的值是在esierver事件之后计算得到的。要注意，担任参与者的领导者， 在这里不起作用。第 4.2.1 节描述了这些担任参与者的领导者是如何参与下一条规则的实现的。 Commit Wait. 担任协调者的领导者，必须确保客户端不能看到任何被 Ti 提交的数据，直到 TT.after(si)为真。提交等待，就是要确保 si 会比 Ti 的绝对提交时间小。提交等待的实现在 4.2.1 节中描述。证明如下: 4.1.3 在某个时间戳下的读操作第 4.1.2 节中描述的单调性，使得 Spanner 可以正确地确定一个副本是否足够新，从而能够满足一个读操作的要求。每个副本都会跟踪记录一个值，这个值被称为安全时间 tsafe，它是一个副本最近更新后的最大时间戳。如果一个读操作的时间戳是 t，当满足 t&lt;=tsafe 时， 这个副本就可以被这个读操作读取。 。。。 4.1.4 为只读事务分配时间戳一个只读事务分成两个阶段执行:分配一个时间戳 sread[8]，然后当成 sread 时刻的快照读来执行事务读操作。快照读可以在任何足够新的副本上面执行。 在一个事务开始后的任意时刻，可以简单地分配 sread=TT.now().latest，通过第 4.1.2 节中描述过的类似的方式来维护外部一致性。但是，对于时间戳 sread 而言，如果 tsafe 没有增加到足够大，可能需要对 sread 时刻的读操作进行阻塞。除此以外还要注意，选择一个 sread 的值可 能也会增加 smax 的值，从而保证不连贯性。为了减少阻塞的概率，Spanner 应该分配可以保持外部一致性的最老的时间戳。第 4.2.2 节描述了如何选择这种时间戳。 4.2 细节这部分内容介绍一些读写操作和只读操作的实践细节，以及用来实现原子模式变更的特定事务的实现方法。然后，描述一些基本模式的细化。 4.2.1 读写事务就像 Bigtable 一样，发生在一个事务中的写操作会在客户端进行缓存，直到提交。由此导致的结果是，在一个事务中的读操作，不会看到这个事务的写操作的结果。这种设计在 Spanner 中可以很好地工作，因为一个读操作可以返回任何数据读的时间戳，未提交的写操作还没有被分配时间戳。 在读写事务内部的读操作，使用伤停等待(wound-wait)[33]来避免死锁。客户端对位于合适组内的领导者副本发起读操作，需要首先获得读锁，然后读取最新的数据。当一个客户端事务保持活跃的时候，它会发送“保持活跃”信息，防止那些参与的领导者让该事务过时。当一个客户端已经完成了所有的读操作，并且缓冲了所有的写操作，它就开始两阶段提交。客户端选择一个协调者组，并且发送一个提交信息给每个参与的、具有协调者标识的领导者，并发送提交信息给任何缓冲的写操作。让客户端发起两阶段提交操作，可以避免在大范围连接内发送两次数据。 一个参与其中的、扮演非协调者角色的领导者，首先需要获得写锁。然后，它会选择一 个预备时间戳，这个时间戳应该比之前分配给其他事务的任何时间戳都要大(这样可以保持 单调性)，并且通过 Paxos 把准备提交记录写入日志。然后，每个参与者就把自己的准备时 间戳通知给协调者。 扮演协调者的领导者，也会首先获得写锁，但是，会跳过准备阶段。在从所有其他的、扮演参与者的领导者那里获得信息后，它就会为整个事务选择一个时间戳。这个提交时间戳 s 必须大于或等于所有的准备时间戳(这是为了满足第 4.1.3 节讨论的限制条件)，在协调者收到它的提交信息时，s 应该大于 TT.now().latest，并且 s 应该大于这个领导者为之前的其他 所有事务分配的时间戳(再次指出，这样做是为了满足单调性)。这个扮演协调者的领导者，就会通过 Paxos 在日志中写入一个提交记录(或者当等待其他参与者发生超时就在日志中写 入终止记录)。 在允许任何协调者副本去提交记录之前，扮演协调者的领导者会一直等待到 TT.after(s)， 从而可以保证遵循第 4.1.2 节中描述的提交等待规则。因为，扮演协调者的领导者会根据 TT.now().latest 来选择 s，而且必须等待直到那个时间戳可以确保成为过去，预期的等待时间 至少是 2*ε。这种等待时间通常会和 Paxos 通信时间发生重叠。在提交等待之后，协调者就会发送一个提交时间戳给客户端和所有其他参与的领导者。每个参与的领导者会通过 Paxos 把事务结果写入日志。所有的参与者会在同一个时间戳进行提交，然后释放锁。 4.2.2 只读事务分配一个时间戳需要一个协商阶段，这个协商发生在所有参与到该读操作中的 Paxos 组之间。由此导致的结果是，Spanner 需要为每个只读事务提供一个 scope 表达式，它可以指出整个事务需要读取哪些键。对于单独的查询，Spanner 可以自动计算出 scope。 如果 scope 的值是由单个 Paxos 组来提供的，那么，客户端就会给那个组的领导者发起一个只读事务(当前的 Spanner 实现中，只会为 Paxos leader 中的只读事务选择一个时间戳)， 为那个领导者分配 sread 并且执行读操作。对于一个单个位置的读操作，Spanner 通常会比 TT.now().latest 做得更好。我们把 LastTS()定义为在 Paxos 组中最后提交的写操作的时间戳。如果没有准备提交的事务，这个分配到的时间戳 sread=LastTS()就很容易满足外部一致性要求: 这个事务将可以看见最后一个写操作的结果，然后排队排在它之后。 如果 scope 的值是由多个 Paxos 组来提供的，就会有几种选择。最复杂的选择就是，和所有组的领导者进行一轮沟通，大家根据 LastTS()进行协商得到 sread。Spanner 当前实现了一个更加简单的选择。这个选择可以避免一轮协商，让读操作在 sread=TT.now().latest 时刻去 执行(这可能会等待安全时间的增加)。这个事务中的所有读操作，可以被发送到任何足够 新的副本上执行。 4.2.3 模式变更事务TrueTime 允许 Spanner 支持原子模式变更。使用一个标准的事务是不可行的，因为参与者的数量(即数据库中组的数量)可能达到几百万个。Bigtable 可以支持在一个数据中心内进行原子模式变更，但是，这个操作会阻塞所有其他操作。 一个 Spanner 模式变更事务通常是一个标准事务的、非阻塞的变种。首先，它会显式地分配一个未来的时间戳，这个时间戳会在准备阶段进行注册。由此，跨越几千个服务器的模式变更，可以在不打扰其他并发活动的前提下完成。其次，读操作和写操作，它们都是隐式地依赖于模式，它们都会和任何注册的模式变更时间戳t保持同步:当它们的时间戳小于 t 时， 读写操作就执行到时刻 t;当它们的时间戳大于时刻 t 时，读写操作就必须阻塞，在模式变更事务后面进行等待。如果没有 TrueTime，那么定义模式变更发生在 t 时刻，就变得毫无意义。 5. 实验分析我们对 Spanner 性能进行了测试，包括复制、事务和可用性。然后，我们提供了一些关于 TrueTime 的实验数据，并且提供了我们的第一个用例——F1。 5.1 微测试基准表 3 给出了一用于 Spanner 的微测试基准(microbenchmark)。这些测试是在分时机器上实现的:每个 spanserver 采用 4GB 内存和四核 CPU(AMD Barcelona 2200MHz)。客户端运行在单独的机器上。每个 zone 都包含一个 spanserver。客户端和 zone 都放在一个数据中心集合内，它们之间的网络距离不会超过 1ms。这种布局是很普通的，许多数据并不需要把数 据分散存储到全球各地)。测试数据库具有 50 个 Paxos 组和 2500 个目录。操作都是独立的 4KB 大小的读和写。All reads were served out of memory after a compaction，从而使得我们只需要衡量 Spanner 调用栈的开销。此外，还会进行一轮读操作，来预热任何位置的缓存。 对于延迟实验而言，客户端会发起足够少量的操作，从而避免在服务器中发生排队。从 1 个副本的实验中，提交等待大约是 5ms，Paxos 延迟大约是 9ms。随着副本数量的增加， 延迟大约保持不变，只具有很少的标准差，因为在一个组的副本内，Paxos 会并行执行。随着副本数量的增加，获得指定投票数量的延迟对一个 slave 副本的慢速度不会很敏感。 对于吞吐量的实验而言，客户端发起足够数量的操作，从而使得 CPU 处理能力达到饱和。快照读操作可以在任何足够新的副本上进行，因此，快照读的吞吐量会随着副本的数量增加而线性增加。单个读的只读事务，只会在领导者上执行，因为，时间戳分配必须发生在领导者上。只读事务吞吐量会随着副本数量的增加而增加，因为有效的 spanserver 的数量会增加:在这个实验的设置中，spanserver 的数量和副本的数量相同，领导者会被随机分配到不同的 zone。写操作的吞吐量也会从这种实验设置中获得收益(副本从 3 变到 5 时写操作吞吐量增加了，就能够说明这点)，但是，随着副本数量的增加，每个写操作执行时需要完 成的工作量也会线性增加，这就会抵消前面的收益。 表 4 显示了两阶段提交可以扩展到合理数量的参与者:它是对一系列实验的总结，这些实验运行在 3 个 zone 上，每个 zone 具有 25 个 spanserver。扩展到 50 个参与者，无论在平均值还是第 99 个百分位方面，都是合理的。在 100 个参与者的情形下，延迟开发明显增加。 5.2 可用性图 5 显示了在多个数据中心运行 Spanner 时的可用性方面的收益。它显示了三个吞吐量实验的结果，并且存在数据中心失败的情形，所有三个实验结果都被重叠放置到一个时间轴 上。测试用的 universe 包含 5 个 zone Zi，每个 zone 都拥有 25 个 spanserver。测试数据库被 分片成 1250 个 Paxos 组，100 个客户端不断地发送非快照读操作，累积速率是每秒 50K 个读操作。所有领导者都会被显式地放置到 Z1。每个测试进行 5 秒钟以后，一个 zone 中的所有服务器都会被“杀死”:non-leader 杀掉 Z2，leader-hard 杀掉 Z1，leader-soft 杀掉 Z1，但是，它会首先通知所有服务器它们将要交出领导权。 杀掉 Z2 对于读操作吞吐量没有影响。杀掉 Z1，给领导者一些时间来把领导权交给另一个 zone 时，会产生一个小的影响:吞吐量会下降，不是很明显，大概下降 3-4%。另一方面，没有预警就杀掉 Z1 有一个明显的影响:完成率几乎下降到 0。随着领导者被重新选择，系统的吞吐量会增加到大约每秒 100K 个读操作，主要是由于我们的实验设置:系统中有额外的能力，当找不到领导者时操作会排队。由此导致的结果是，系统的吞吐量会增加直到到达 系统恒定的速率。 我们可以看看把 Paxos 领导者租约设置为 10ms 的效果。当我们杀掉这个 zone，对于这 个组的领导者租约的过期时间，会均匀地分布到接下来的 10 秒钟内。来自一个死亡的领导者的每个租约一旦过期，就会选择一个新的领导者。大约在杀死时间过去 10 秒钟以后，所有的组都会有领导者，吞吐量就恢复了。短的租约时间会降低服务器死亡对于可用性的影响， 但是，需要更多的更新租约的网络通讯开销。我们正在设计和实现一种机制，它可以在领导者失效的时候，让 slave 释放 Paxos 领导者租约。 5.3 TrueTime关于 TrueTime，必须回答两个问题: ε 是否就是时钟不确定性的边界? ε 会变得多糟糕? 对于第一个问题，最严峻的问题就是，如果一个局部的时钟漂移大于 200us/sec，那就会破坏 TrueTime 的假设。我们的机器统计数据显示，坏的 CPU 的出现概率要比坏的时钟出现概率大 6 倍。也就是说，与更加严峻的硬件问题相比，时钟问题是很少见的。由此，我们也相信，TrueTime 的实现和 Spanner 其他软件组件一样，具有很好的可靠性，值得信任。 图 6 显示了 TrueTime 数据，是从几千个 spanserver 中收集的，这些 spanserver 跨越了多 个数据中心，距离 2200 公里以上。图中描述了 ε 的第 90 个、99 个和 99.9 个百分位的情况， 是在对 timemaster 进行投票后立即对 timeslave daemon 进行样本抽样的。这些抽样数据没有考虑由于时钟不确定性带来的 ε 值的锯齿，因此测量的是 timemaster 不确定性(通常是 0) 再加上通讯延迟。 图 6 中的数据显示了，在决定 ε 的基本值方面的上述两个问题，通常都不会是个问题。 但是，可能会存在明显的拖尾延迟问题，那会引起更高的 ε 值。图中，3 月 30 日拖尾延迟的降低，是因为网络的改进，减少了瞬间网络连接的拥堵。在 4 月 13 日 ε 的值增加了，持续了大约 1 个小时，主要是因为例行维护时关闭了两个 time master。我们会继续调研并且消除引起 TrueTime 突变的因素。 5.4 F1Spanner 在 2011 年早期开始进行在线负载测试，它是作为谷歌广告后台 F1[35]的重新实现的一部分。这个后台最开始是基于 MySQL 数据库，在许多方面都采用手工数据分区。未 经压缩的数据可以达到几十 TB，虽然这对于许多 NoSQL 实例而言数据量是很小的，但是， 对于采用数据分区的 MySQL 而言，数据量是非常大的。MySQL 的数据分片机制，会把每个客户和所有相关的数据分配给一个固定的分区。这种布局方式，可以支持针对单个客户的 索引构建和复杂查询处理，但是，需要了解一些商业知识来设计分区。随着客户数量的增长， 对数据进行重新分区，代价是很大的。最近一次的重新分区，花费了两年的时间，为了降低风险，在多个团队之间进行了大量的合作和测试。这种操作太复杂了，无法常常执行，由此导致的结果是，团队必须限制 MySQL 数据库的增长，方法是，把一些数据存储在外部的 Bigtable 中，这就会牺牲事务和查询所有数据的能力。 F1 团队选择使用 Spanner 有几个方面的原因。首先，Spanner 不需要手工分区。其次， Spanner 提供了同步复制和自动失败恢复。在采用 MySQL 的 master-slave 复制方法时，很难进行失败恢复，会有数据丢失和当机的风险。再次，F1 需要强壮的事务语义，这使得使用 其他 NoSQL 系统是不实际的。应用语义需要跨越任意数据的事务和一致性读。F1 团队也需要在他们的数据上构建二级索引(因为 Spanner 没有提供对二级索引的自动支持)，也有能力使用 Spanner 事务来实现他们自己的一致性全球索引。 所有应用写操作，现在都是默认从 F1 发送到 Spanner。而不是发送到基于 MySQL 的应 用栈。F1 在美国的西岸有两个副本，在东岸有三个副本。这种副本位置的选择，是为了避免发生自然灾害时出现服务停止问题，也是出于前端应用的位置的考虑。实际上，Spanner 的失败自动恢复，几乎是不可见的。在过去的几个月中，尽管有不在计划内的机群失效，但是，F1 团队最需要做的工作仍然是更新他们的数据库模式，来告诉 Spanner 在哪里放置 Paxos 领导者，从而使得它们尽量靠近应用前端。 Spanner 时间戳语义，使得它对于 F1 而言，可以高效地维护从数据库状态计算得到的、放在内存中的数据结构。F1 会为所有变更都维护一个逻辑历史日志，它会作为每个事务的 一部分写入到 Spanner。F1 会得到某个时间戳下的数据的完整快照，来初始化它的数据结构， 然后根据数据的增量变化来更新这个数据结构。 表 5 显示了 F1 中每个目录的分片数量的分布情况。每个目录通常对应于 F1 上的应用栈中的一个用户。绝大多数目录(同时意味着绝大多数用户)都只会包含一个分片，这就意味着，对于这些用户数据的读和写操作只会发生在一个服务器上。多于 100 个分片的目录，是那些包含 F1 二级索引的表:对这些表的多个分片进行写操作，是极其不寻常的。F1 团队也只是在以事务的方式进行未经优化的批量数据加载时，才会碰到这种情形。 表 6 显示了从 F1 服务器来测量的 Spanner 操作的延迟。在东海岸数据中心的副本，在 选择 Paxos 领导者方面会获得更高的优先级。表 6 中的数据是从这些数据中心的 F1 服务器 上测量得到的。写操作延迟分布上存在较大的标准差，是由于锁冲突引起的肥尾效应(fat tail)。在读操作延迟分布上存在更大的标准差，部分是因为 Paxos 领导者跨越了两个数据中心，只有其中的一个是采用了固态盘的机器。此外，测试内容还包括系统中的每个针对两个 数据中心的读操作:字节读操作的平均值和标准差分别是 1.6KB 和 119KB。 6. 相关工作Megastore[5]和 DynamoDB[3]已经提供了跨越多个数据中心的一致性复制。DynamoDB 提供了键值存储接口，只能在一个 region 内部进行复制。Spanner 和 Megastore 一样，都提供了半关系数据模型，甚至采用了类似的模式语言。Megastore 无法活动高性能。Megastore 是架构在 Bigtable 之上，这带来了很高的通讯代价。Megastore 也不支持长寿命的领导者， 多个副本可能会发起写操作。来自不同副本的写操作，在 Paxos 协议下一定会发生冲突，即使他们不会发生逻辑冲突:会严重影响吞吐量，在一个 Paxos 组内每秒钟只能执行几个写操作。Spanner 提供了更高的性能，通用的事务和外部一致性。 Pavlo 等人[31]对数据库和 MapReduce[12]的性能进行了比较。他们指出了几个努力的方向，可以在分布式键值存储之上充分利用数据库的功能[1][4][7][41]，二者可以实现充分的融合。我们比较赞同这个结论，并且认为集成多个层是具有优势的:把复制和并发控制集成起来，可以减少 Spanner 中的提交等待代价。 在一个采用了复制的存储上面实现事务，可以至少追述到 Gifford 的论文[16]。Scatter[17] 是一个最近的基于 DHT 的键值存储，可以在一致性复制上面实现事务。Spanner 则要比 Scatter 在更高的层次上提供接口。Gray 和 Lamport[18]描述了一个基于 Paxos 的非阻塞的提交协议，他们的协议会比两阶段提交协议带来更多的代价，而两阶段提交协议在大范围分布 式的组中的代价会进一步恶化。Walter[36]提供了一个快照隔离的变种，但是无法跨越数据中心。相反，我们的只读事务提供了一个更加自然的语义，因为我们对于所有的操作都支持外部语义。 最近，在减少或者消除锁开销方面已经有大量的研究工作。Calvin[40]消除了并发控制: 它会重新分配时间戳，然后以时间戳的顺序执行事务。HStore[39]和 Granola[11]都支持自己的事务类型划分方法，有些事务类型可以避免锁机制。但是，这些系统都无法提供外部一致性。Spanner 通过提供快照隔离，解决了冲突问题。 VoltDB[42]是一个分片的内存数据库，可以支持在大范围区域内进行主从复制，支持灾难恢复，但是没有提供通用的复制配置方法。它是一个被称为 NewSQL 的实例，这是实现 可扩展的 SQL[38]的强大的市场推动力。许多商业化的数据库都可以支持历史数据读取，比如 Marklogic[26]和 Oracle’ Total Recall[30]。Lomet 和 Li[24]对于这种时间数据库描述了一种 实现策略。 Faresite 给出了与一个受信任的时钟参考值相关的时钟不确定性的边界13:Farsite 中的服务器租约的方式，和 Spanner 中维护 Paxos 租约的方式 相同。在之前的工作中[2][23]，宽松同步时钟已经被用来进行并发控制。我们已经展示了 TrueTime 可以从 Paxos 状态机集合中推导出全球时间。 7. 未来的工作在过去一年的大部分时间里，我们都是 F1 团队一起工作，把谷歌的广告后台从 MySQL 迁移到 Spanner。我们正在积极改进它的监控和支撑工具，同时在优化性能。此外，我们已经开展了大量工作来改进备份恢复系统的功能和性能。我们当前正在实现 Spanner 模式语言，自动维护二级索引和自动基于负载的分区。在未来，我们会调研更多的特性。以最优化的方式并行执行读操作，是我们追求的有价值的策略，但是，初级阶段的实验表明，实现这个目标比较艰难。此外，我们计划最终可以支持直接变更 Paxos 配置[22]34]。 我们希望许多应用都可以跨越数据中心进行复制，并且这些数据中心彼此靠近。 TrueTime ε 可能会明显影响性能。把 ε 值降低到 1ms 以内，并不存在不可克服的障碍。 Time-master-query 间隔可以继续减少，Time-master-query 延迟应该随着网络的改进而减少， 或者通过采用分时技术来避免延迟。 最后，还有许多有待改进的方面。尽管 Spanner 在节点数量上是可扩展的，但是与节点相关的数据结构在复杂的 SQL 查询上的性能相对较差，因为，它们是被设计成服务于简单的键值访问的。来自数据库文献的算法和数据结构，可以极大改进单个节点的性能。另外，根据客户端负载的变化，在数据中心之间自动转移数据，已经成为我们的一个目标，但是，为了有效实现这个目标，我们必须具备在数据中心之间自动、协调地转移客户端应用进程的能力。转移进程会带来更加困难的问题——如何在数据中心之间管理和分配资源。 8. 总结总的来说，Spanner 对来自两个研究群体的概念进行了结合和扩充:一个是数据库研究群体，包括熟悉易用的半关系接口，事务和基于 SQL 的查询语言;另一个是系统研究群体，包括可扩展性，自动分区，容错，一致性复制，外部一致性和大范围分布。自从 Spanner 概念成形，我们花费了 5 年以上的时间来完成当前版本的设计和实现。花费这么长的时间，一部分原因在于我们慢慢意识到，Spanner 不应该仅仅解决全球复制的命名空间问题，而且也应该关注 Bigtable 中所丢失的数据库特性。 我们的设计中一个亮点特性就是 TrueTime。我们已经表明，在时间 API 中明确给出时钟不确定性，可以以更加强壮的时间语义来构建分布式系统。此外，因为底层的系统在时钟不确定性上采用更加严格的边界，实现更强壮的时间语义的代价就会减少。作为一个研究群体，我们在设计分布式算法时，不再依赖于弱同步的时钟和较弱的时间 API。 致谢许多人帮助改进了这篇论文:Jon Howell，Atul Adya, Fay Chang, Frank Dabek, Sean Dorward, Bob Gruber, David Held, Nick Kline, Alex Thomson, and Joel Wein. 我们的管理层对于我们的工作和论文发表都非常支持:Aristotle Balogh, Bill Coughran, Urs H ̈olzle, Doron Meyer, Cos Nicolaou, Kathy Polizzi, Sridhar Ramaswany, and Shivakumar Venkataraman. 我们的工作是在Bigtable和Megastore团队的工作基础之上开展的。F1团队，尤其是Jeff Shute ，和我们一起工作，开发了数据模型，跟踪性能和纠正漏洞。Platforms团队，尤其是Luiz Barroso 和Bob Felderman，帮助我们一起实现了TrueTime。最后，许多谷歌员工都曾经在我们的团队工作过，包括Ken Ashcraft, Paul Cychosz, Krzysztof Ostrowski, Amir Voskoboynik, Matthew Weaver, Theo Vassilakis, and Eric Veach; or have joined our team recently: Nathan Bales, Adam Beberg, Vadim Borisov, Ken Chen, Brian Cooper, Cian Cullinan, Robert-Jan Huijsman, Milind Joshi, Andrey Khorlin, Dawid Kuroczko, Laramie Leavitt, Eric Li, Mike Mammarella, Sunil Mushran, Simon Nielsen, Ovidiu Platon, Ananth Shrinivas, Vadim Suvorov, and Marcel van der Holst. 参考文献[1] Azza Abouzeid et al. ―HadoopDB: an architectural hybrid of MapReduce and DBMS technologies for analytical workloads‖. Proc. of VLDB. 2009, pp. 922–933. [2] A. Adya et al. ―Efficient optimistic concurrency control using loosely synchronized clocks‖. Proc. of SIGMOD. 1995, pp. 23–34. [3] Amazon. Amazon DynamoDB. 2012. [4] Michael Armbrust et al. ―PIQL: Success-Tolerant Query Processing in the Cloud‖. Proc. of VLDB. 2011, pp. 181–192. [5] Jason Baker et al. ―Megastore: Providing Scalable, Highly Available Storage for Interactive Services‖. Proc. of CIDR. 2011, pp. 223–234. [6] Hal Berenson et al. ―A critique of ANSI SQL isolation levels‖. Proc. of SIGMOD. 1995, pp. 1–10. [7] Matthias Brantner et al. ―Building a database on S3‖. Proc. of SIGMOD. 2008, pp. 251–264. [7] Matthias Brantner et al. ―Building a database on S3‖. Proc. of SIGMOD. 2008, pp. 251–264. [8] A. Chan and R. Gray. ―Implementing Distributed Read-Only Transactions‖. IEEE TOSE SE-11.2 (Feb. 1985), pp. 205–212. [9] Fay Chang et al. ―Bigtable: A Distributed Storage System for Structured Data‖. ACM TOCS 26.2 (June 2008), 4:1–4:26. [10] Brian F. Cooper et al. ―PNUTS: Yahoo!’s hosted data serving platform‖. Proc. of VLDB. 2008, pp. 1277–1288. [11] James Cowling and Barbara Liskov. ―Granola: Low-Overhead Distributed Transaction Coordination‖. Proc. of USENIX ATC. 2012, pp. 223–236. [12] Jeffrey Dean and Sanjay Ghemawat. ―MapReduce: a flexible data processing tool‖. CACM 53.1 (Jan. 2010), pp. 72–77. [13] John Douceur and Jon Howell. Scalable Byzantine-Fault-Quantifying Clock Synchronization. Tech. rep. MSR-TR-2003-67. MS Research, 2003. [14] John R. Douceur and Jon Howell. ―Distributed directory service in the Farsite file system‖. Proc. of OSDI. 2006, pp. 321–334. [15] Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung. ―The Google file system‖. Proc. of SOSP. Dec. 2003, pp. 29–43. [16] David K. Gifford. Information Storage in a Decentralized Computer System. Tech. rep. CSL-81-8. PhD dissertation. Xerox PARC, July 1982. [17] Lisa Glendenning et al. ―Scalable consistency in Scatter‖. Proc. of SOSP. 2011. [18] Jim Gray and Leslie Lamport. ―Consensus on transaction commit‖. ACM TODS 31.1 (Mar. 2006), pp. 133–160. [19] Pat Helland. ―Life beyond Distributed Transactions: an Apostate’s Opinion‖. Proc. of CIDR. 2007, pp. 132–141. [20] Maurice P. Herlihy and Jeannette M. Wing. ―Linearizability: a correctness condition forconcurrent objects‖. ACM TOPLAS 12.3 (July 1990), pp. 463–492. [21] Leslie Lamport. ―The part-time parliament‖. ACM TOCS 16.2 (May 1998), pp. 133–169. [22] Leslie Lamport, Dahlia Malkhi, and Lidong Zhou. ―Reconfiguring a state machine‖. SIGACT News 41.1 (Mar. 2010), pp. 63–73. [23] Barbara Liskov. ―Practical uses of synchronized clocks in distributed systems‖. Distrib. Comput. 6.4 (July 1993), pp. 211–219. [24] David B. Lomet and Feifei Li. ―Improving Transaction-Time DBMS Performance and Functionality‖. Proc. of ICDE (2009), pp. 581–591. [25] Jacob R. Lorch et al. ―The SMART way to migrate replicated stateful services‖. Proc. of EuroSys. 2006, pp. 103–115. [26] MarkLogic. MarkLogic 5 Product Documentation. 2012. [27] Keith Marzullo and Susan Owicki. ―Maintaining the time in a distributed system‖. Proc. of PODC. 1983, pp. 295–305. [28] Sergey Melnik et al. ―Dremel: Interactive Analysis of Web-Scale Datasets‖. Proc. of VLDB. 2010, pp. 330–339. [29] D.L. Mills. Time synchronization in DCNET hosts. Internet Project Report IEN–173. COMSAT Laboratories, Feb. 1981. [30] Oracle. Oracle Total Recall. 2012. [31] Andrew Pavlo et al. ―A comparison of approaches to large-scale data analysis‖. Proc. of SIGMOD. 2009, pp. 165–178. [32] Daniel Peng and Frank Dabek. ―Large-scale incremental processing using distributed transactions and notifications‖. Proc. of OSDI. 2010, pp. 1–15. [33] Daniel J. Rosenkrantz, Richard E. Stearns, and Philip M. Lewis II. ―System level concurrency control for distributed database systems‖. ACM TODS 3.2 (June 1978), pp. 178–198. [34] Alexander Shraer et al. ―Dynamic Reconfiguration of Primary/Backup Clusters‖. Proc. ofSENIX ATC. 2012, pp. 425–438. [35] Jeff Shute et al. ―F1—The Fault-Tolerant Distributed RDBMS Supporting Google’s Ad Business‖. Proc. of SIGMOD. May 2012, pp. 777–778. [36] Yair Sovran et al. ―Transactional storage for geo-replicated systems‖. Proc. of SOSP. 2011, pp. 385–400. [37] Michael Stonebraker. Why Enterprises Are Uninterested in NoSQL. 2010. [38] Michael Stonebraker. Six SQL Urban Myths. 2010. [39] Michael Stonebraker et al. ―The end of an architectural era: (it’s time for a complete rewrite)‖. Proc. of VLDB. 2007, pp. 1150–1160. [40] Alexander Thomson et al. ―Calvin: Fast Distributed Transactions for Partitioned Database Systems‖. Proc. of SIGMOD.2012, pp. 1–12. [41] Ashish Thusoo et al. ―Hive — A Petabyte Scale Data Warehouse Using Hadoop‖. Proc. of ICDE. 2010, pp. 996–1005. [42] VoltDB. VoltDB Resources. 2012.","raw":null,"content":null,"categories":[{"name":"分布式数据库","slug":"分布式数据库","permalink":"http://linbingdong.com/categories/分布式数据库/"},{"name":"Google Spanner","slug":"分布式数据库/Google-Spanner","permalink":"http://linbingdong.com/categories/分布式数据库/Google-Spanner/"}],"tags":[{"name":"分布式数据库","slug":"分布式数据库","permalink":"http://linbingdong.com/tags/分布式数据库/"},{"name":"Spanner","slug":"Spanner","permalink":"http://linbingdong.com/tags/Spanner/"},{"name":"论文翻译","slug":"论文翻译","permalink":"http://linbingdong.com/tags/论文翻译/"}]},{"title":"Java中Comparable与Comparator的区别","slug":"Java中Comparable与Comparator的区别","date":"2017-03-11T12:33:42.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2017/03/11/Java中Comparable与Comparator的区别/","link":"","permalink":"http://linbingdong.com/2017/03/11/Java中Comparable与Comparator的区别/","excerpt":"相同\nComparable和Comparator都是用来实现对象的比较、排序\n要想对象比较、排序，都需要实现Comparable或Comparator接口\nComparable和Comparator都是Java的接口\n","text":"相同 Comparable和Comparator都是用来实现对象的比较、排序 要想对象比较、排序，都需要实现Comparable或Comparator接口 Comparable和Comparator都是Java的接口 区别 Comparator位于java.util包下，而Comparable位于java.lang包下 Comparable接口的实现是在类的内部（如 String、Integer已经实现了Comparable接口，自己就可以完成比较大小操作），Comparator接口的实现是在类的外部（可以理解为一个是自已完成比较，一个是外部程序实现比较） 实现Comparable接口要重写compareTo方法, 在compareTo方法里面实现比较 public class Student implements Comparable &#123; String name; int age public int compareTo(Student another) &#123; int i = 0; i = name.compareTo(another.name); if(i == 0) &#123; return age - another.age; &#125; else &#123; return i; &#125; &#125;&#125; 这时我们可以直接用 Collections.sort( StudentList ) 对其排序了.( **只需传入要排序的列表**） 实现Comparator需要重写 compare 方法 public class Student&#123; String name; int age&#125;class StudentComparator implements Comparator &#123; public int compare(Student one, Student another) &#123; int i = 0; i = one.name.compareTo(another.name); if(i == 0) &#123; return one.age - another.age; &#125; else &#123; return i; &#125; &#125;&#125; Collections.sort( StudentList , new StudentComparator()) 可以对其排序（ **不仅要传入待排序的列表，还要传入实现了Comparator的类的对象**） 总结 如果比较的方法只要用在一个类中，用该类实现Comparable接口就可以。 如果比较的方法在很多类中需要用到，就自己写个类实现Comparator接口，这样当要比较的时候把实现了Comparator接口的类传过去就可以，省得重复造轮子。这也是为什么Comparator会在java.util包下的原因。使用Comparator的优点是：1.与实体类分离 2.方便应对多变的排序规则","raw":null,"content":null,"categories":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/tags/Java/"}]},{"title":"PostgreSQL综合","slug":"学习PostgreSQL不可错过的一篇文章","date":"2017-03-11T12:33:42.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2017/03/11/学习PostgreSQL不可错过的一篇文章/","link":"","permalink":"http://linbingdong.com/2017/03/11/学习PostgreSQL不可错过的一篇文章/","excerpt":"PostgreSQL是以加州大学伯克利分校计算机系开发的 POSTGRES, Version 4.2为基础的对象关系型数据库管理系统(ORDBMS)。POSTGRES开创的许多概念在很久以后才出现在商业数据库中。","text":"PostgreSQL是以加州大学伯克利分校计算机系开发的 POSTGRES, Version 4.2为基础的对象关系型数据库管理系统(ORDBMS)。POSTGRES开创的许多概念在很久以后才出现在商业数据库中。 1. 简介PostgreSQL是以加州大学伯克利分校计算机系开发的 POSTGRES, Version 4.2为基础的对象关系型数据库管理系统(ORDBMS)。POSTGRES开创的许多概念在很久以后才出现在商业数据库中。 PostgreSQL支持大部分SQL标准并且提供了许多其它现代特性： 复杂查询 外键 触发器 可更新的视图 事务完整性 多版本并发控制 另外，PostgreSQL可以用许多方法进行扩展，比如通过增加新的： 数据类型 函数 操作符 聚合函数 索引方法 过程语言 经过二十几年的发展，PostgreSQL 是目前世界上最先进的开源数据库系统。 2. 体系基本概念PostgreSQL使用常见的客户端/服务器 的模式。一次PostgreSQL会话由下列相关的进程(程序)组成： 服务器进程 它管理数据库文件，接受来自客户端应用与数据库的连接，并且代表客 户端在数据库上执行操作。数据库服务器程序叫postgres。 客户端应用 客户端应用可能本身就是多种多样的：它们可以是一个字符界面的工具，也可以是一个图形界面的应用， 或者是一个通过访问数据库来显示网页的 web 服务器，或者是一个特殊的数据库管理工具。一些客户端应用是和PostgreSQL发布一起提供的，但绝大部分是用户开发的。 PostgreSQL服务器可以处理来自客户端的多个并发连接。 因此，它为每个连接启动(“forks”)一个新的进程。从这个时候开始，客户端和新服务器进程就不再经过最初的postgres进程进行通讯。因此，主服务器总是在运行，等待客户端连接，而客户端及其相关联的服务器进程则是起起停停。 3. 基本使用3.1 常用命令\\l 列出所有数据库 或者： SELECT datname FROM pg_database;\\du 列出所有角色/用户 或者： SELECT rolname FROM pg_roles;\\q 退出数据库\\d 列出当前数据库里的所有表\\dt 列出当前数据库里的所有资料表\\c dbname 切换数据库\\dx 显示安装的插件\\x 切换横向竖向显示show &lt;参数名&gt; 查看该参数的值 3.2 建表CREATE TABLE weather ( city varchar(80), temp_lo int, -- low temperature temp_hi int, -- high temperature prcp real, -- precipitation date date); 3.3 从文本加载数据先在/tmp下创建mydb.txt,内容如下： &apos;shanghai&apos; 20 30 0.07 &apos;2011-11-11&apos;&apos;chengdu&apos; 2 45 0.9 &apos;2008-09-08&apos;&apos;shanghai&apos; 20 30 0.07 &apos;2011-11-11&apos;&apos;chengdu&apos; 2 45 0.9 &apos;2008-09-08&apos; 将mydb.txt里的内容导入weather表中： COPY weather FROM &apos;/tmp/mydb.txt&apos; delimiter &apos; &apos;; --delimiter指定分隔符，txt文件默认分隔符是&apos;\\t&apos;,CSV文件默认分隔符是&apos;,&apos; 此时查看weather表中的数据： select * from weather; 结果： mydb=# select * from weather; city | tmp_lo | tmp_hi | prcp | date------------+--------+--------+------+------------ &apos;shanghai&apos; | 20 | 30 | 0.07 | 2011-11-11 &apos;chengdu&apos; | 2 | 45 | 0.9 | 2008-09-08 &apos;shanghai&apos; | 20 | 30 | 0.07 | 2011-11-11 &apos;chengdu&apos; | 2 | 45 | 0.9 | 2008-09-08(4 行记录) 4. 单机部署在192.168.20.93和192.168.20.94上分别部署了单机版的PostgreSQL。 4.1 安装PostgreSQL源rpm -Uvh http://yum.postgresql.org/9.4/redhat/rhel-7-x86_64/pgdg-centos94-9.4-1.noarch.rpm 4.2 执行安装命令yum updateyum install postgresql94-server postgresql94-contrib 4.3 验证是否安装成功执行： rpm -qa | grep postgres 结果： postgresql94-9.4.10-1PGDG.rhel7.x86_64postgresql94-server-9.4.10-1PGDG.rhel7.x86_64postgresql94-libs-9.4.10-1PGDG.rhel7.x86_64postgresql94-contrib-9.4.10-1PGDG.rhel7.x86_64 说明安装成功 4.4 初始化数据库先创建数据存放目录： mkdir -p /opt/pgsql/data 赋予postgres用户该目录的权限： chown postgres /opt/pgsql/data 切换到postgres用户： su postgres 执行初始化： initdb -D /opt/pgsql/data 注： -D 后面是数据库文件存放的目录，如果不指定则默认在/var/lib/pgsql/9.4/data下 初始化的日志如下： 属于此数据库系统的文件宿主为用户 &quot;postgres&quot;.此用户也必须为服务器进程的宿主.数据库簇将使用本地化语言 &quot;zh_CN.UTF-8&quot;进行初始化.默认的数据库编码已经相应的设置为 &quot;UTF8&quot;.initdb: 无法为本地化语言环境&quot;zh_CN.UTF-8&quot;找到合适的文本搜索配置缺省的文本搜索配置将会被设置到&quot;simple&quot;禁止为数据页生成校验和.修复已存在目录 /opt/pgsql/data 的权限 ... 成功正在创建子目录 ... 成功选择默认最大联接数 (max_connections) ... 100选择默认共享缓冲区大小 (shared_buffers) ... 128MB选择动态共享内存实现 ......posix创建配置文件 ... 成功在 /opt/pgsql/data/base/1 中创建 template1 数据库 ... 成功初始化 pg_authid ... 成功初始化dependencies ... 成功创建系统视图 ... 成功正在加载系统对象描述 ...成功创建(字符集)校对规则 ... 成功创建字符集转换 ... 成功正在创建字典 ... 成功对内建对象设置权限 ... 成功创建信息模式 ... 成功正在装载PL/pgSQL服务器端编程语言...成功清理数据库 template1 ... 成功拷贝 template1 到 template0 ... 成功拷贝 template1 到 template0 ... 成功同步数据到磁盘...成功成功. 您现在可以用下面的命令运行数据库服务器: /usr/pgsql-9.4/bin/postmaster -D /opt/pgsql/data/或者 /usr/pgsql-9.4/bin/pg_ctl -D /opt/pgsql/data/ -l logfile start 4.5 启动服务1.切换到postgres用户 su postgres 因为启动服务同样必须以PostgreSQL用户帐户登录来做。 2.启动服务 没有-D选项，服务器将使用环境变量PGDATA命名的目录； 如果这个环境变量也没有，将导致失败。通常，最好在后台启动postgres，使用下面的 Unix shell 语法： pg_ctl -D /opt/pgsql/data/ -l logfile start 3.设置开机自动启动 在Linux系统里，要么往/etc/rc.d/rc.local或 /etc/rc.local文件里加上下面几行： /usr/local/pgsql/bin/pg_ctl start -l logfile -D /usr/local/pgsql/data 4.6 创建用户PostgreSQL使用角色的概念管理数据库访问权限。 根据角色自身的设置不同，一个角色可以看做是一个数据库用户，或者一组数据库用户。 角色可以拥有数据库对象(比如表)以及可以把这些对象上的权限赋予其它角色， 以控制谁拥有访问哪些对象的权限。另外，我们也可以把一个角色的成员 权限赋予其它角色，这样就允许成员角色使用分配给另一个角色的权限。角色的概念替换了”用户”和”组”。在PostgreSQL 版本 8.1 之前，用户和组是独立类型的记录，但现在它们只是角色。 任何角色都可以是一个用户、一个组、或者两者。 数据库角色从概念上与操作系统用户是完全无关的。在实际使用中把它们对应起来可能比较方便， 但这不是必须的。数据库角色在整个数据库集群中是全局的(而不是每个库不同)。 要创建一个角色，使用 SQL 命令CREATE ROLE执行： CREATE ROLE name; name遵循 SQL 标识的规则：要么完全没有特殊字符， 要么用双引号包围(实际上你通常会给命令增加额外的选项，比如LOGIN。 下面显示更多细节)。要删除一个现有角色，使用类似的DROP ROLE命令： DROP ROLE name; 为了方便，程序createuser和dropuser 提供了对了这些 SQL 命令的封装。我们可以在 shell 命令上直接调用它们： 直接在shell里输入： createuser lbd; 这样就创建了lbd这个角色。 dropuser lbd; 这样就创建了lbd这个角色。 要检查现有角色的集合，可以检查pg_roles系统表，比如： SELECT rolname FROM pg_roles; 结果如下： postgres=# SELECT rolname FROM pg_roles; rolname---------- postgres lbd(2 行记录) psql的元命令\\du也可以用于列出现有角色。 结果如下： postgres=# \\du 角色列表 角色名称 | 属性 | 成员属于---------+----------------------------------+---------- lbd | | &#123;&#125; postgres| 超级用户, 建立角色, 建立 DB, 复制 | &#123;&#125; 5. 主从流复制部署192.168.20.93上部署主服务器，192.168.20.94上部署从服务器。 5.1 简介postgres在9.0之后引入了主从的流复制机制，所谓流复制，就是从服务器通过tcp流从主服务器中同步相应的数据。这样当主服务器数据丢失时从服务器中仍有备份。 与基于文件日志传送相比，流复制允许保持从服务器更新。 从服务器连接主服务器，其产生的流WAL记录到从服务器， 而不需要等待主服务器写完WAL文件。 PostgreSQL流复制默认是异步的。在主服务器上提交事务和从服务器上变化可见之间有一个小的延迟，这个延迟远小于基于文件日志传送，通常1秒能完成。如果主服务器突然崩溃，可能会有少量数据丢失。 同步复制必须等主服务器和从服务器都写完WAL后才能提交事务。这样在一定程度上会增加事务的响应时间。 配置同步复制仅需要一个额外的配置步骤： synchronous_standby_names必须设置为一个非空值。synchronous_commit也必须设置为on。 这里部署的是异步的流复制。 注:主从服务器所在节点的系统、环境\u0001等最好一致。PostgreSQL版本也最好一致，否则可能会有问题。 5.2 安装部署先在192.168.20.93和192.168.20.94均安装PostgreSQL。 具体安装部署步骤见上一节：单机部署 5.2.1 主服务器主服务器为192.168.20.93 先创建一个新目录： mkdir /opt/pgsql/pg_archive 1.首先需要创建一个数据库用户进行主从同步。创建用户replica，并赋予登录和复制的权限。 postgres# CREATE ROLE replica login replication encrypted password &apos;replica&apos; 2.修改pg_hba.conf，允许replica用户来同步。 在pg_hba.conf里增加两行： host all all 192.168.20.94/32 trust #允许94连接到主服务器host replication replica 192.168.20.94/32 md5 #允许94使用replica用户来复制 这样，就设置了replica这个用户可以从192.168.20.93进行流复制请求。 注：第二个字段必须要填replication 4.修改postgresql.conf listen_addresses = &apos;*&apos; # 监听所有IParchive_mode = on # 允许归档archive_command = &apos;cp %p /opt/pgsql/pg_archive/%f&apos; # 用该命令来归档logfile segmentwal_level = hot_standby max_wal_senders = 32 # 这个设置了可以最多有几个流复制连接，差不多有几个从，就设置几个wal_keep_segments = 256 ＃ 设置流复制保留的最多的xlog数目wal_sender_timeout = 60s ＃ 设置流复制主机发送数据的超时时间max_connections = 100 # 这个设置要注意下，从库的max_connections必须要大于主库的 配置完两个文件后重启服务器。 pg_ctl stop -D /opt/pgsql/datapg_ctl start -D /opt/pgsql/data 3.测试94能否连接93数据库。在94上运行如下命令： psql -h 192.168.20.93 -U postgres 看看是否能进入数据库。若可以，则正常。 5.2.2 从服务器1.从主节点拷贝数据到从节点 su - postgresrm -rf /opt/pgsql/data/* #先将data目录下的数据都清空pg_basebackup -h 192.168.20.93 -U replica -D /opt/pgsql/data -X stream -P # 从93拷贝数据到94（基础备份）mkdir /opt/pgsql/pg_archive 2.配置recovery.conf 复制/usr/pgsql-9.4/share/recovery.conf.sample 到 /opt/pgsql/data/recovery.conf cp /usr/pgsql-9.4/share/recovery.conf.sample /opt/pgsql/data/recovery.conf 修改recovery.conf standby_mode = on # 说明该节点是从服务器primary_conninfo = &apos;host=192.168.20.93 port=5432 user=replica password=replica&apos; # 主服务器的信息以及连接的用户recovery_target_timeline = &apos;latest&apos; 3.配置postgresql.conf wal_level = hot_standbymax_connections = 1000 ＃ 一般查多于写的应用从库的最大连接数要比较大hot_standby = on ＃ 说明这台机器不仅仅是用于数据归档，也用于数据查询max_standby_streaming_delay = 30s # 数据流备份的最大延迟时间wal_receiver_status_interval = 10s # 多久向主报告一次从的状态，当然从每次数据复制都会向主报告状态，这里只是设置最长的间隔时间hot_standby_feedback = on # 如果有错误的数据复制，是否向主进行反馈 配置完后重启从服务器 pg_ctl stop -D /opt/pgsql/datapg_ctl start -D /opt/pgsql/data 5.3 验证是否部署成功在主节点上执行： select client_addr,sync_state from pg_stat_replication; 结果如下： postgres=# select client_addr,sync_state from pg_stat_replication; client_addr | sync_state---------------+------------ 192.168.20.94 | async(1 行记录) 说明94是从服务器，在接收流，而且是异步流复制。 此外，还可以分别在主、从节点上运行 ps aux | grep postgres 来查看进程： 主服务器（93）上： postgres 262270 0.0 0.0 337844 2832 ? Ss 10:14 0:00 postgres: wal sender process replica 192.168.20.94(13059) streaming 0/A002A88 可以看到有一个 wal sender 进程。 从服务器（94）上： postgres 569868 0.0 0.0 384604 2960 ? Ss 10:14 0:02 postgres: wal receiver process streaming 0/A002B60 可以看到有一个 wal receiver 进程。 至此，PostgreSQL主从流复制安装部署完成。 在主服务器上插入数据或删除数据，在从服务器上能看到相应的变化。从服务器上只能查询，不能插入或删除。 6. 主要配置1. 连接相关listen_addresses = &apos;*&apos; 数据库用来监听客户端连接的IP地址，*表示监听所有IP。port = 5432 数据库监听户端连接的TCP端口。默认值是5432。max_connections = 100 允许客户端的最大连接数，默认是100，足够了。superuser_reserved_connections = 3 为超级用户保留的连接数，默认为3。2. 资源使用shared_buffers = 128MB 可以被PostgreSQL用于缓存数据的内存大小。大的shared_buffers需要大的checkpoint_segments,同时需要申请更多的System V共享内存资源.这个值不需要设的太大, 因为PostgreSQL还依赖操作系统的cache来提高读性能。另外, 写操作频繁的数据库这个设太大反而会增加checkpoint压力(除非你使用了SSD或者IOPS能力很好的存储).work_mem = 4MB 内部排序和哈希操作可使用的工作内存大小。maintenance_work_mem = 64MB 这里定义的内存只是在CREATE INDEX, VACUUM等时用到。这个值越大, VACUUM, CREATE INDEX的操作越快, 当然大到一定程度瓶颈就不在内存了, 可能是CPU例如创建索引.这个值是一个操作的内存使用上限, 而不是一次性分配出去的. 并且需要注意如果开启了autovacuum, 最大可能有autovacuum_max_workers*maintenance_work_mem的内存被系统消耗掉.3. WALwal_level = hot_standby 如果需要做数据库WAL日志备份的话至少需要设置成archive级别, 如果需要做hot_standby那么需要设置成hot_standby。hot_standby意味着WAL记录得更详细, 如果没有打算做hot_standby设置得越低性能越好。fsync = on 强制把数据同步更新到磁盘wal_buffers = -1 默认是-1 根据shared_buffers的设置自动调整shared_buffers*3% .最大限制是XLOG的segment_size.checkpoint_segments = 3 多少个xlog file产生后开始checkpoint操作。建议设置为shared_buffers除以单个XLOG文件的大小。checkpoint_timeout = 5min 这个和checkpoint_segments的效果是一样的, 只是触发的条件是时间条件。archive_mode = on 允许归档。archive_command = &apos;cp %p /opt/pgsql/pg_archive/%f&apos; 归档调用的命令。4. 主从复制## postgresql.confmax_wal_senders = 32 最大的wal sender进程数。hot_standby = on 在从服务器上设置为 on ，则该服务器也可用作查询。max_standby_streaming_delay = 30s 数据流备份的最大延迟时间。wal_receiver_status_interval = 10s 多久向主报告一次从的状态，当然从每次数据复制都会向主报告状态，这里只是设置最长的间隔时间。hot_standby_feedback = on 如果有错误的数据复制，是否向主进行反馈。## recovery.conf（只有从服务器需要配置）standby_mode = on 说明该节点是从服务器primary_conninfo = &apos;host=192.168.20.93 port=5432 user=replica password=replica&apos; 主服务器的信息以及连接的用户recovery_target_timeline = &apos;latest&apos;5. 内核资源max_files_per_process = 1000 设定每个数据库进程能够打开的文件的数目。默认值是1000。shared_preload_libraries = &apos;&apos; 设置数据库在启动时要加载的操作系统共享库文件。如果有多个库文件，名字用逗号分开。如果数据库在启动时未找到shared_preload_libraries指定的某个库文件，数据库将无法启动。默认值为空串。6. AUTOVACUUM参数autovacuum = on 是否打开数据库的自动垃圾收集功能。默认值是on。如果autovacuum被设为on，参数track_counts也要被设为on，自动垃圾收集才能正常工作。注意，即使这个参数被设为off，如果事务ID回绕即将发生，数据库会自动启动一个垃圾收集操作。这个参数只能在文件postgresql.conf中被设置。log_autovacuum_min_duration = -1 单位是毫秒。如果它的值为0，所有的垃圾搜集操作都会被记录在数据库运行日志中，如果它的值是-1，所有的垃圾收集操作都不会被记录在数据库运行日志中。如果把它的值设为250毫秒，只要自动垃圾搜集发出的VACUUM和ANALYZE命令的执行时间超过250毫秒，VACUUM和ANALYZE命令的相关信息就会被记录在数据库运行日志中。默认值是-1。autovacuum_max_workers = 3 设置能同时运行的最大的自动垃圾收集工作进程的数目。默认值是3。autovacuum_naptime = 1min 设置自动垃圾收集控制进程的睡眠时间。autovacuum_vacuum_threshold = 50 设置触发垃圾收集操作的阈值。默认值是50。只有一个表上被删除或更新的记录的数目超过了autovacuum_vacuum_threshold的值，才会对这个表执行垃圾收集操作。7. 文件位置data_directory = &apos;/opt/pgsql/data&apos; 数据存放位置，初始化时可以指定，也可以在这里修改。hba_file = &apos;/opt/pgsql/data/pg_hba.conf&apos; 主从复制配置文件pg_hba.conf的路径ident_file = /opt/pgsql/data/pg_ident.conf&apos; 配置文件pg_ident.conf的路径8. 编码lc_messages = &apos;zh_CN.UTF-8&apos; 系统错误信息的语言环境lc_monetary = &apos;zh_CN.UTF-8&apos; 货币格式的语言环境lc_numeric = &apos;zh_CN.UTF-8&apos; 数字的语言环境lc_time = &apos;zh_CN.UTF-8&apos; 时间的语言环境 7. 插件（扩展）PostgreSQL的contrib/目录和extension/目录附带包含若干插件的源代码。 在附录 F中被描述。其它插件是独立开发的， 比如PostGIS。 甚至PostgreSQL的复制方案也是在外部开发的。 比如 Slony-I 是一个流行的主/从复制方案，它就是独立在核心项目之外开发的。 PostgreSQL的插件主要用来提供新的用户自定义函数，操作符，或类型。 若要使用插件，需要在数据库系统中注册新的SQL对象。（如果该插件没有在contrib或extension目录下，需要先自己安装，或者在编译源码的时候指定。） 在PostgreSQL 9.1和以后版本，这是通过执行 CREATE EXTENSION命令来实现。 CREATE EXTENSION module_name 此命令必须由数据库管理员运行。如想在某个数据库中使用该插件，则必须在该数据库中运行如上命令。另外， 在数据库template1中运行它，这样在随后创建的数据库中也可使用该插件。 具体插件的安装使用请参考下一节：PostGIS插件安装与使用 8. PostGIS插件安装与使用8.1 简介PostGIS是对象关系型数据库PostgreSQL的一个插件，PostGIS提供如下空间信息服务功能：空间对象、空间索引、空间操作函数和空间操作符。同时，PostGIS遵循OpenGIS的规范。 PostGIS支持所有的空间数据类型，这些类型包括：点（POINT）、线（LINESTRING）、多边形（POLYGON）、多点 （MULTIPOINT）、多线（MULTILINESTRING）、多多边形（MULTIPOLYGON）和集合对象集 （GEOMETRYCOLLECTION）等。PostGIS支持所有的对象表达方法，比如WKT和WKB。 PostGIS支持所有的数据存取和构造方法，如GeomFromText()、AsBinary()，以及GeometryN()等。 PostGIS提供简单的空间分析函数（如Area和Length）同时也提供其他一些具有复杂分析功能的函数，比如Distance。 PostGIS提供了对于元数据的支持，如GEOMETRY_COLUMNS和SPATIAL_REF_SYS，同时，PostGIS也提供了相应的支持函数，如AddGeometryColumn和DropGeometryColumn。 PostGIS提供了一系列的二元谓词（如Contains、Within、Overlaps和Touches）用于检测空间对象之间的空间关系，同时返回布尔值来表征对象之间符合这个关系。 PostGIS提供了空间操作符（如Union和Difference）用于空间数据操作。比如，Union操作符融合多边形之间的边界。两个交迭的多边形通过Union运算就会形成一个新的多边形，这个新的多边形的边界为两个多边形中最大边界。 PostGIS还提供以下功能： 数据库坐标变换 数据库中的几何类型可以通过Transform函数从一种投影系变换到另一种投影系中。在OpenGIS中的几何类型都将SRID作为自身结构的一部分，但不知什么原因，在OpenGIS的SFSQL规范中，并没有引入Transform。 球体长度运算 存储在普通地理坐标系中的集合类型如果不进行坐标变换是无法进行程度运算的，OpenGIS所提供的坐标变换使得积累类型的程度计算变成可能。 三维的几何类型 SFSQL规范只是针对二维集合类型。OpenGIS提供了对三维集合类型的支持，具体是利用输入的集合类型维数来决定输出的表现方式。例如，即便 所有几何对象内部都以三维形式存储，纯粹的二维交叉点通常还是以二维的形式返回。此外，还提供几何对象在不同维度间转换的功能。 空间聚集函数 在数据库中，聚集函数是一个执行某一属性列所有数据操作的函数。比如Sum和Average，Sum是求某一关系属性列的数据总和，Average 则是求取某一关系属性列的数据平均值。与此对应，空间聚集函数也是执行相同的操作，不过操作的对象是空间数据。例如聚集函数Extent返回一系列要素中 的最大的包裹矩形框，如“SELECT EXTENT(GEOM) FROM ROADS”这条SQL语句的执行结果是返回ROADS这个数据表中所有的包裹矩形框。 栅格数据类型 PostGIS通过一种新的数据类型片，提供对于大的栅格数据对象的存储。片由以下几个部分组成：包裹矩形框、SRID、类型和一个字节序列。通过 将片的大小控制在数据库页值（32×32）以下，使得快速的随即访问变成可能。一般大的图片也是通过将其切成32×32像素的片然后再存储在数据库中的。 8.2 部署8.2.1 安装PostGIS yum install postgis2_94 # 因为安装的PostgreSQL版本为9.4，所以是postgis2_94 注： 需要PostgreSQL9.1以上版本才支持PostGIS. 8.2.2 使PostGIS可用想要在PostgreSQL中使用PostGIS插件，安装只是第一步。每个数据库想要使用PostGIS必须先在该数据库中使PostGIS可用。假设我们想在gisdb这个数据库中使用PostGIS,先进入gisdb数据库，执行以下步骤： gisdb=# CREATE EXTENSION postgis;gisdb=# CREATE EXTENSION postgis_topology; 8.2.3 查看是否安装成功 在gisdb数据库中输入\\du，查看已安装的插件 gisdb=# \\dx 已安装扩展列表 名称 | 版本 | 架构模式 | 描述------------------+-------+------------+--------------------------------------------------------------------- plpgsql | 1.0 | pg_catalog | PL/pgSQL procedural language postgis | 2.1.8 | public | PostGIS geometry, geography, and raster spatial types and functions postgis_topology | 2.1.8 | topology | PostGIS topology spatial types and functions(3 行记录) 可以看到已经安装了postgis和postgis_topology。 8.3 使用8.3.1 创建空间数据表首先建立一个常规的表格存储有关城市（cities）的信息。这个表格有两栏，一个是 ID 编号，一个是城市名： gisdb=# CREATE TABLE cities (id int4, name varchar(50)); 现在添加一个空间列用于存储城市的位置。习惯上这个列叫做 the_geom。它记录了数据为什么类型（点、线、面）、有几维（这里是二维）以及空间坐标系统。此处使用 EPSG:4326 坐标系统： gisdb=# SELECT AddGeometryColumn (&apos;cities&apos;, &apos;the_geom&apos;, 4326, &apos;POINT&apos;, 2); 完成后，查询 cities 表单应当显示这个新栏目。同时页面将显示当前表达没有记录（0 rows）。 gisdb=# select * from cities; id | name | the_geom----+-----------------+----------------------------------------------------（0行记录） 为添加记录，需要使用 SQL 命令。对于空间列，使用 PostGIS 的 ST_GeomFromText可以将文本转化为坐标与参考系号的记录： gisdb=# INSERT INTO cities (id, the_geom, name) VALUES (1,ST_GeomFromText(&apos;POINT(-0.1257 51.508)&apos;,4326),&apos;London, England&apos;);gisdb=# INSERT INTO cities (id, the_geom, name) VALUES (2,ST_GeomFromText(&apos;POINT(-81.233 42.983)&apos;,4326),&apos;London, Ontario&apos;);gisdb=# INSERT INTO cities (id, the_geom, name) VALUES (3,ST_GeomFromText(&apos;POINT(27.91162491 -33.01529)&apos;,4326),&apos;East London,SA&apos;); 当然，这样的输入方式难以操作。其它方式可以更快的输入数据。就目前来说，表格内已经有了一些城市数据，可以先进行查询等操作。 8.3.2 简单查询标准的 SQL 操作都可以用于 PostGIS 表： gisdb=# SELECT * FROM cities; id | name | the_geom----+-----------------+---------------------------------------------------- 1 | London, England | 0101000020E6100000BBB88D06F016C0BF1B2FDD2406C14940 2 | London, Ontario | 0101000020E6100000F4FDD478E94E54C0E7FBA9F1D27D4540 3 | East London,SA | 0101000020E610000040AB064060E93B4059FAD005F58140C0(3 行记录) 这里的坐标是无法阅读的 16 进制格式。要以 WKT 文本显示，使用 ST_AsText(the_geom) 或ST_AsEwkt(the_geom) 函数。也可以使用 ST_X(the_geom) 和 ST_Y(the_geom) 显示一个维度的坐标： gisdb=# SELECT id, ST_AsText(the_geom), ST_AsEwkt(the_geom), ST_X(the_geom), ST_Y(the_geom) FROM cities; id | st_astext | st_asewkt | st_x | st_y----+------------------------------+----------------------------------------+-------------+----------- 1 | POINT(-0.1257 51.508) | SRID=4326;POINT(-0.1257 51.508) | -0.1257 | 51.508 2 | POINT(-81.233 42.983) | SRID=4326;POINT(-81.233 42.983) | -81.233 | 42.983 3 | POINT(27.91162491 -33.01529) | SRID=4326;POINT(27.91162491 -33.01529) | 27.91162491 | -33.01529(3 行记录) 8.3.3 空间查询PostGIS 为 PostgreSQL 扩展了许多空间操作功能。以上已经涉及了转换空间坐标格式的 ST_GeomFromText 。多数空间操作以 ST（spatial type）开头，在 PostGIS 文档相应章节有罗列。这里回答一个具体的问题：上面三个城市相互的距离是多少？查询语句怎么写？ gisdb=# SELECT p1.name,p2.name,ST_Distance_Sphere(p1.the_geom,p2.the_geom) FROM cities AS p1, cities AS p2 WHERE p1.id &gt; p2.id; name | name | st_distance_sphere-----------------+-----------------+-------------------- London, Ontario | London, England | 5875787.03777356 East London,SA | London, England | 9789680.59961472 East London,SA | London, Ontario | 13892208.6782928(3 行记录) 输出显示了距离数据。注意 ‘WHERE’ 部分防止了输出城市到自身的距离（0）或者两个城市不同排列的距离数据（London, England 到 London, Ontario 和 London, Ontario 到 London, England 的距离是一样的）。 9. 参考资料 https://www.postgresql.org/ http://www.postgres.cn/docs/9.4/index.html http://www.cnblogs.com/yjf512/p/4499547.html https://yq.aliyun.com/articles/63183?spm=5176.100240.searchblog.173.aJC9Fo https://yq.aliyun.com/articles/214?spm=5176.100240.searchblog.66.iduww3 http://postgis.net/install/ http://www.cnblogs.com/zhaowenzhong/p/5667434.html http://wiki.clusterlabs.org/wiki/PgSQL_Replicated_Cluster?spm=5176.100239.blogcont64841.22.H7WZ3U","raw":null,"content":null,"categories":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"http://linbingdong.com/categories/PostgreSQL/"}],"tags":[{"name":"关系数据库","slug":"关系数据库","permalink":"http://linbingdong.com/tags/关系数据库/"},{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"http://linbingdong.com/tags/PostgreSQL/"}]},{"title":"Java中System.arraycopy()和Arrays.copyOf()的区别","slug":"Java中System.arraycopy()和Arrays.copyOf()的区别","date":"2017-03-11T12:33:42.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2017/03/11/Java中System.arraycopy()和Arrays.copyOf()的区别/","link":"","permalink":"http://linbingdong.com/2017/03/11/Java中System.arraycopy()和Arrays.copyOf()的区别/","excerpt":"先看看System.arraycopy()的声明：\npublic static native void arraycopy(Object src,int srcPos, Object dest, int destPos,int length);`\n","text":"先看看System.arraycopy()的声明： public static native void arraycopy(Object src,int srcPos, Object dest, int destPos,int length);` src - 源数组。srcPos - 源数组中的起始位置。dest - 目标数组。destPos - 目标数据中的起始位置。length - 要复制的数组元素的数量。 该方法用了native关键字，说明调用的是其他语言写的底层函数。 再看Arrays.copyOf() public static &lt;T,U&gt; T[] copyOf(U[] original, int newLength, Class&lt;? extends T[]&gt; newType) { @SuppressWarnings(&quot;unchecked&quot;) T[] copy = ((Object)newType == (Object)Object[].class)?(T[]) new Object[newLength]:(T[]) Array.newInstance(newType.getComponentType(), newLength);System.arraycopy(original,0, copy,0, Math.min(original.length, newLength)); return copy; } 该方法对应不同的数据类型都有各自的重载方法original - 要复制的数组newLength - 要返回的副本的长度newType - 要返回的副本的类型仔细观察发现，copyOf()内部调用了System.arraycopy()方法 区别在于： arraycopy()需要目标数组，将原数组拷贝到你自己定义的数组里，而且可以选择拷贝的起点和长度以及放入新数组中的位置 copyOf()是系统自动在内部新建一个数组，调用arraycopy()将original内容复制到copy中去，并且长度为newLength。返回copy; 即将原数组拷贝到一个长度为newLength的新数组中，并返回该数组。 总结Array.copyOf()可以看作是受限的System.arraycopy(),它主要是用来将原数组全部拷贝到一个新长度的数组，适用于数组扩容。","raw":null,"content":null,"categories":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/tags/Java/"}]},{"title":"Java中wait、sleep和yield的区别","slug":"Java中wait、sleep和yield的区别","date":"2017-03-11T12:33:42.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2017/03/11/Java中wait、sleep和yield的区别/","link":"","permalink":"http://linbingdong.com/2017/03/11/Java中wait、sleep和yield的区别/","excerpt":"","text":"Java中wait、sleep的区别或者Java中sleep、yield的区别是Java面试或者多线程面试中最常问的问题之一。首先，一个最明显的区别是：wait是Object类的方法，sleep和yield是Thread类的静态方法。 本质上，wait方法是用来让线程等待某个条件，进入该条件的等待集中。而sleep和yield方法是用来让线程让出CPU时间，把CPU交给线程调度器，使得其他线程能获得CPU时间。 接下来详细比较三个方法。 wait常用的wait方法有wait( )和wait(long timeout) wait( )方法导致当前线程进入等待状态直到它被通知（其他线程调用notify或notifyAll方法。notify/notifyAll方法解除等待线程的阻塞状态）。 wait(long timeout) 方法导致当前线程进入等待状态直到它被通知或者经过指定的时间。 wait( )后，线程会释放掉它所占有的对象的锁，从而使线程所在对象中的其它synchronized数据可被别的线程使用。 wait方法只能在一个同步方法中调用。如果当前线程不是对象锁的持有者，该方法抛出一个IllegalMonitorStateException异常。 sleepThread.sleep(long millis),必须带有一个时间参数。 sleep(long millis)使当前线程进入停滞状态，所以执行sleep()的线程在指定的时间内肯定不会被执行。 sleep(long millis)可能使任意优先级的其他线程得到执行机会。 sleep(long millis)不会释放锁。 调用sleep方法的线程在唤醒之后不保证能获取到CPU，它会先进入就绪态，与其他线程竞争CPU。 yieldyield方法没有参数。 yield方法使当前线程让出CPU，但让出的时间是不可设定的。 yield方法也不会释放锁。 yield只能把CPU让给相同优先级的其他线程，而不会把CPU给更高或更低优先级的其他线程。若此时没有其他线程跟它在有一个优先级，则该线程继续获得CPU时间，因此可能某线程刚调用yield方法又马上被执行。","raw":null,"content":null,"categories":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/tags/Java/"}]},{"title":"程序员资料大全","slug":"程序员资料大全","date":"2017-03-11T12:33:42.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2017/03/11/程序员资料大全/","link":"","permalink":"http://linbingdong.com/2017/03/11/程序员资料大全/","excerpt":"活到老，学到老。","text":"活到老，学到老。 本文由stanzhai整理。 目录 资料篇 技术站点 必看书籍 大牛博客 GitHub篇 工具篇 平台工具 常用工具 第三方服务 爬虫相关(好玩的工具) 安全相关 Web服务器性能/压力测试工具/负载均衡器 大数据处理/数据分析/分布式工具 Web前端 语言篇 Scala Java Python Swift .NET C &amp; C++ 其他 游戏开发相关 日志聚合，分布式日志收集 RTP,实时传输协议与音视频 资料篇技术站点 在线学习：Coursera、edX、Udacity、MIT公开课、MOOC学院、慕课网 Hacker News：非常棒的针对编程的链接聚合网站 Techmeme：美国知名科技新闻和博客聚集网站，类似的还有（Panda, Hacker &amp; Designer News） Reddit - Programming板块：同上 Java牛人必备：Program Creek Stack Overflow：IT技术问答网站 SegmentFault：中文的技术问答社区 GitHub：全球最大的源代码管理平台，很多知名开源项目都在上面，如Linux内核，OpenStack等 码云：支持中文可免费创建私有项目的代码托管平台，可作为备选 LeetCode：来做做这些题吧，看看自己的算法水平如何？这可比什么面试宝典强多了。 LintCode：支持中文的编程题在线训练平台，可作为备选 Kaggle,Topcoder: 机器学习、大数据竞赛 掘金：高质量的技术社区 开发者头条 InfoQ：企业级应用，关注软件开发领域 V2EX: way to explore 国内老牌技术社区：OSChina、博客园、CSDN、51CTO 免费的it电子书：http://it-ebooks.info/ 在线学习：http://www.udemy.com/ 优质学习资源：http://plus.mojiax.com/ 代码练习：http://exercism.io/ and https://www.codingame.com DevStore:开发者服务商店 MSDN：微软相关的官方技术集中地，主要是文档类 谷歌开发者 码库 - 收录了实用的开源项目及资源 必看书籍 SICP(Structure and Interpretation of Computer Programs) 深入理解计算机系统 代码大全2 人件 人月神话 软件随想录 算法导论（麻省理工学院出版社） 离散数学及其应用 设计模式 编程之美 黑客与画家 编程珠玑 The Little Schemer Simply Scheme_Introducing_Computer_Science C++ Prime Effective C++ TCP/IP详解 Unix 编程艺术 技术的本质 软件随想录 计算机程序设计艺术 职业篇：程序员的自我修养,程序员修炼之道,高效能程序员的修炼 《精神分析引论》弗洛伊德 《失控》《科技想要什么》《技术元素》凯文凯利 程序开发心理学 天地一沙鸥 搞定：无压力工作的艺术 大牛博客 云风（游戏界大牛）: http://blog.codingnow.com/ 王垠（不少文章喷到蛮有道理）：http://www.yinwang.org/ 冰河-伞哥(Lisp大牛)：http://tianchunbinghe.blog.163.com/ R大【干货满满】RednaxelaFX写的文章/回答的导航帖 陈皓-左耳朵耗子：http://coolshell.cn/ Jeff Atwood（国外知名博主）: https://blog.codinghorror.com/ 阮一峰（黑客与画家译者，Web）：http://www.ruanyifeng.com/ 廖雪峰（他的Python、Git教程不少人都看过）：http://www.liaoxuefeng.com/ 道哥的黑板报（安全）：https://zhuanlan.zhihu.com/taosay 国内GitHub上关注度较高的开发者 GitHub篇 Awesome Awesome: 这是个Awesome合集，常见的资料这里面都能找到 Awesome2: 类似第一个Awesome 杂七杂八、有用没用的Awesome合集 非常不错的语言类学习资料集合：Awesomeness awesome-ios-ui awesome-android-ui Awesome-MaterialDesign awesome-public-datasets awesome-AppSec(系统安全) awesome-datascience awesome-dataviz - 数据可视化库及资料 书籍资料 free-programming-books 中文版 免费的编程中文书籍索引 《程序员编程艺术 — 面试和算法心得》 GoBooks Papers we love 自然语言处理NLP推荐学习路线及参考资料 超级棒的机器学习资料（框架，库，软件）, 中文翻译版 机器学习(Machine Learning)&amp;深入学习(Deep Learning)资料 Docker资料合集 学习使用Strom Hadoop Internals Spark Internals 大数据时代的数据分析与数据挖掘 – 基于Hadoop实现 如何制作操作系统 借助开源项目学习软件开发 几个不错的开源游戏引擎 一起写Python文章，一起看Python文章 R的极客理想系列文章 HTTP接口设计指南 分享自己长期关注的前端开发相关的优秀网站、博客、以及活跃开发者 Readings in Databases Data Science blogs 日志：每个软件工程师都应该知道的有关实时数据的统一概念 Android Code Path Android Learn Notes PHP 类库框架，资料集合 优秀项目 最值得关注的10个C语言开源项目 15款值得学习的小型开源项目 iOS-100个开源组件 十大Material Design开源项目 Android开源项目分类汇总 前端 &amp; Node.js 前端资源 前端开发指南 前端技能汇总 前端资源大导航 收集前端方面的书籍 2014年最新前端开发面试题 简单清晰的JavaScript语言教程，代码示例 JavaScript编程规范 JavaScript必看视频 JavaScript标准参考教程（阮一峰的，整理的不错） JS必看 AngularJS Guide的中文分支 Angular2学习资料 AngularJS应用的最佳实践和风格指南 React-Native学习指南 七天学会NodeJS node.js中文资料导航 Nodejs学习路线图 如何学习nodejs 工作，工具 系统管理员工具集合 Pro Git Nginx开发从入门到精通 Google 全球 IP 地址库 收集整理远程工作相关的资料 Color schemes for hackers 游戏开发工具集，MagicTools 开发者工具箱， free-for-dev GitHub秘籍 Git风格指南 Bast-App 工具篇平台工具 Phabricator: 软件开发平台，Facebook出品，现已开源，CodeReview神器（从这个往下一直到GitLab之间的工具统统可以忽略了） Redmine/Trac：项目管理平台 Jenkins/Jira(非开源)：持续集成系统（Apache Continuum，这个是Apache下的CI系统，还没来得及研究） git，svn：源代码版本控制系统 GitLab/Gitorious：构建自己的GitHub服务器 Postman:RESTful，api测试工具，HTTP接口开发必备神器 Lottie: AE动画变原生代码，设计师必备 Sonar：代码质量管理平台 Nessus: 系统漏洞扫描器 gitbook：https://www.gitbook.io/写书的好东西，当然用来写文档也很不错的（发现不少产品的文档就是用的它） Travis-ci：开源项目持续集成必备，和GitHub相结合，https://travis-ci.org/ Trello：简单高效的项目管理平台，注重看板管理 日志聚合：graylog、ELK（推荐新一代的graylog，基本上算作是开源的Splunk了） 开源测试工具、社区（Selenium、OpenQA.org） Puppet:一个自动管理引擎，可以适用于Linux、Unix以及Windows平台。所谓配置管理系统，就是管理机器里面诸如文件、用户、进程、软件包这些资源。无论是管理1台，还是上万台机器Puppet都能轻松搞定。其他类似工具：CFEngine、SaltStack、Ansible Nagios：系统状态监控报警，还有个Icinga(完全兼容nagios所有的插件,工作原理,配置文件以及方法,几乎一模一样。配置简单,功能强大) Ganglia：分布式监控系统 fleet：分布式init系统 Ansible：能够大大简化Unix管理员的自动化配置管理与流程控制方式。 GeoLite免费数据库 jsHint:js代码验证工具 haproxy: 高可用负载均衡（此外类似的系统还有nginx，lvs） linux OS性能分析工具：dstat，iostat，iotop，nmon kimono：将网页信息转换为api接口的工具 集群管理工具：pdsh，ClusterSSH，mussh（可以用它快速管理Hadoop集群）ipa-server做统一的认证管理 influxdb: 分布式时序数据库，结合Grafana可以进行实时数据分析 dot: 程序员绘图利器（是种语言，也是个工具） Graph::Easy: （Ascii Art工具）字符流程图绘制，实乃程序员装逼神器。其他类似的工具Asciiflow, vi插件：drawit! spf13-vim: 让你的vim飞起来！ Kubernetes: 容器集群管理系统 Gatling: 服务器性能压力测试工具 systemtap: Linux内核探测工具、内核调试神器 Cygwin：Windows下的类UNIX模拟环境 MinGW：Windows下的GNU工具集 常用工具 Mac下的神兵利器 asciinema: 终端录屏神器 Fiddler：非常好用的Web前端调试工具，当然是针对底层http协议的，一般情况使用Chrome等自带的调试工具也足够了，特殊情况还得用它去处理 Charles: Mac上的Web代理调试工具，类似Fiddler fir.im免费的移动App内测托管平台 wireshark：知名的网络数据包分析工具 PowerCmd:替代Windows Cmd的利器 RegexBuddy:强大的正则表达式测试工具 Soure Insight：源代码阅读神器 SublimeText：程序员最爱的编辑器 Database.NET：一个通用的关系型数据库客户端，基于.NET 4.0开发的，做简单的处理还是蛮方便的 Navicat Premium：支持MySql、PostgreSQL、Oracle、Sqlite和SQL Server的客户端，通用性上不如Database.NET，但性能方面比Database.NET好很多，自带备份功能也用于数据库定时备份。 Synergy : 局域网内一套键盘鼠标控制多台电脑 DameWare：远程协助工具集（我在公司主要控制大屏幕用） Radmin: 远程控制工具，用了一段时间的DameWare，还要破解，对Win7支持的不好，还是发现这个好用 Listary：能极大幅度提高你 Windows 文件浏览与搜索速度效率的「超级神器」 Clover：给资源管理器加上多标签，我平时工作的时候就用它，像Chrome一样使用资源管理器，甚是方便啊（这是Windows平台的） WinLaunch：模拟Mac OS的Launch工具 Fritzing：绘制电路图 LICEcap：gif教程制作 git，svn：版本控制系统 Enigma Virtual Box（将exe，dll等封装成一个可执行程序） Open DBDiff(针对SqlServer)数据库同步 SymmetricDS：数据库同步 BIEE,Infomatica，SPSS，weka，R语言：数据分析 CodeSmith，LightSwitch：代码生成 Pandoc：Markdown转换工具，出书用的。以前玩过docbook，不过现在还是Markdown盛行啊。 Window Magnet[Mac]：增强Mac窗口管理功能，想Win7一样具有窗口拖放到屏幕边缘自动调整的功能 log explorer：查看SqlServer日志 dependency walker：查询Windows应用程序dll依赖项 Shairport4w：将iPhone，iPad，iPod上的音频通过AirPlay协议传输到PC上 ngrok：内网穿透工具 Axure:快速原型制作工具，还有个在线作图的工具国内的一个创业团队做的，用着很不错http://www.processon.com/ Origami: 次世代交互设计神器 百度脑图：http://naotu.baidu.com/ tinyproxy:（Linux）小型的代理服务器支持http和https协议 EaseUS Partition Master：超级简单的分区调整工具，速度还是蛮快的，C盘不够用了就用它从D盘划点空间吧，不用重装系统这么折腾哦。 CheatEngine：玩游戏修改内存值必备神器（记得我在玩轩辕剑6的时候就用的它，超级方便呢） ApkIDE:Android反编译神器 翻、墙工具（自|由|门、天行浏览器，免费的VPN：http://www.mangovpn.com/）,发现最方便还属Lantern，免费用起来超级方便（更新于2015-08-22） 设计工具：Sketch、OmniGraffle MindManger：思维导图 MagicDraw:Uml图工具 innotop：MySql状态监测工具 墨刀：比Axure更为简单的原型工具，可以快速制作原型 Karabiner: Mac专用，修改键盘键位的神器，机械键盘必备 Timing：Mac专用，统计你的时间都花在哪了 f.lux: 护眼神器，过滤蓝光，程序员护眼必备良品 LaTeX: 基于ΤΕΧ的排版系统, 让写论文更方便 Antlr：开源的语法分析器，可以让你毫无压力的写个小parser 第三方服务 DnsPod：一个不错的智能DNS服务解析提供商 DigitalOcean：海外的云主机提供商，价格便宜，磁盘是SSD的，用过一段时间整体上还可以，不过毕竟是海外的，网速比较慢。国内的就是阿里云了。还有个比较知名的是：Linode，据说速度上比DigitalOcean好很多 移动端推送服务：个推、JPush、云巴 LeanCloud：移动应用开发服务，包括:数据存储、用户管理、消息推送、应用统计、社交分享、实时聊天等服务 Color Hunt: 漂亮炫酷的配色网站，程序员的福音 Heroku: PaaS平台 爬虫相关(好玩的工具) Phantomjs(Web自动化测试，服务端渲染等) berserkJS(基于Phantomjs的改进版本) SlimerJS CasperJS selenium HtmlUnit（开源的java 页面分析工具，也是个Headless的浏览器） 安全相关 sql注入检测：sqlmap、haviji 端口扫描：nmap 渗透测试：BurpLoader sqltools: sql漏洞利用工具 snort: 入侵检测 Web服务器性能/压力测试工具/负载均衡器 ab: ab是apache自带的一款功能强大的测试工具 curl-loader: 真实模拟、测试Web负载 http_load: 程序非常小，解压后也不到100K webbench: 是Linux下的一个网站压力测试工具，最多可以模拟3万个并发连接去测试网站的负载能力。 Siege: 一款开源的压力测试工具，可以根据配置对一个WEB站点进行多用户的并发访问，记录每个用户所有请求过程的相应时间，并在一定数量的并发访问下重复进行。 squid（前端缓存），nginx（负载），nodejs（没错它也可以，自己写点代码就能实现高性能的负载均衡器）：常用的负载均衡器 Piwik：开源网站访问量统计系统 ClickHeat：开源的网站点击情况热力图 HAProxy：高性能TCP /HTTP负载均衡器 ElasticSearch：搜索引擎基于Lucene Page Speed SDK和YSLOW HAR Viewer: HAR分析工具 protractor：E2E（end to end）自动化测试工具 大数据处理/数据分析/分布式工具 Hadoop：分布式的文件系统，结合其MapReduce编程模型可以用来做海量数据的批处理（Hive，Pig，HBase啥的就不说了），值得介绍的是Cloudera的Hadoop分支CDH5，基于YARN MRv2集成了Spark可直接用于生产环境的Hadoop，对于企业快速构建数据仓库非常有用。 Spark：大规模数据处理框架（可以应付企业中常见的三种数据处理场景：复杂的批量数据处理（batch data processing）；基于历史数据的交互式查询（interactive query）；基于实时数据流的数据处理（streaming data processing）），CSND有篇文章介绍的不错 除了Spark，其他几个不错的计算框架还有：Kylin，Flink，Drill Ceph:Linux分布式文件系统（特点：无中心） Storm：实时流数据处理，可以看下IBM的一篇介绍 （还有个Yahoo的S4，也是做流数据处理的） Druid: 实时数据分析存储系统 Ambari: 大数据平台搭建、监控利器；类似的还有CDH Tachyon：分布式内存文件系统 Mesos：计算框架一个集群管理器，提供了有效的、跨分布式应用或框架的资源隔离和共享 Impala：新一代开源大数据分析引擎，提供Sql语义，比Hive强在速度上 presto: facebook的开源工具，大数据分布式sql查询引擎 SNAPPY：快速的数据压缩系统，适用于Hadoop生态系统中 Kafka:高吞吐量的分布式消息队列系统 ActiveMQ:是Apache出品，最流行的，能力强劲的开源消息总线 MQTT:Message Queuing Telemetry Transport，消息队列遥测传输）是IBM开发的一个即时通讯协议，有可能成为物联网的重要组成部分 RabbitMQ：记得OpenStack就是用的这个东西吧 ZeroMQ：宣称是将分布式计算变得更简单，是个分布式消息队列，可以看下云风的一篇文章的介绍 开源的日志收集系统：scribe、chukwa、kafka、flume。这有一篇对比文章 Zookeeper：可靠的分布式协调的开源项目 Databus：LinkedIn 实时低延迟数据抓取系统 数据源获取：Flume、Google Refine、Needlebase、ScraperWiki、BloomReach 序列化技术：JSON、BSON、Thrift、Avro、Google Protocol Buffers NoSql：ScyllaDB（宣称是世界上最快的NoSql）、Apache Casandra、MongoDB、Apache CouchDB、Redis、BigTable、HBase、Hypertable、Voldemort、Neo4j MapReduce相关：Hive、Pig、Cascading、Cascalog、mrjob、Caffeine、S4、MapR、Acunu、Flume、Kafka、Azkaban、Oozie、Greenplum 数据处理：R、Yahoo! Pipes、Mechanical Turk、Solr/ Lucene、ElasticSearch、Datameer、Bigsheets、Tinkerpop NLP自然语言处理：Natural Language Toolkit、Apache OpenNLP、Boilerpipe、OpenCalais 机器学习：TensorFlow（Google出品），WEKA、Mahout、scikits.learn、SkyTree 可视化技术：GraphViz、Processing、Protovis、Google Fusion Tables、Tableau、Highcharts、EChats（百度的还不错）、Raphaël.js Kettle：开源的ETL工具 Pentaho：以工作流为核心的开源BI系统 Mondrian：开源的Rolap服务器 Oozie：开源hadoop的工作流调度引擎，类似的还有：Azkaban 开源的数据分析可视化工具：Weka、Orange、KNIME Cobar：阿里巴巴的MySql分布式中间件 数据清洗：data wrangler， Google Refine Web前端 Material Design: 谷歌出品，必属精品 Vue.js: 借鉴了Angular及React的JS框架，设计理念较为先进 GRUNT: js task runner Sea.js: js模块化 knockout.js：MVVM开发前台，绑定技术 Angular.js: 使用超动感HTML &amp; JS开发WEB应用！ Highcharts.js，Flot:常用的Web图表插件 NVD3: 基于d3.js的图表库 Raw：非常不错的一款高级数据可视化工具 Rickshaw:时序图标库，可用于构建实时图表 JavaScript InfoVis Toolkit：另一款Web数据可视化插件 Pdf.js，在html中展现pdf ACE，CodeMirror：Html代码编辑器（ACE甚好啊） NProcess：绚丽的加载进度条 impress.js：让你制作出令人眩目的内容展示效果(类似的还有reveal) Threejs：3DWeb库 Hightopo：基于Html5的2D、3D可视化UI库 jQuery.dataTables.js:高度灵活的表格插件 Raphaël：js，canvas绘图库，后来发现百度指数的图形就是用它绘出来的 director.js：js路由模块，前端路由，Nodejs后端路由等，适合构造单页应用 pace.js：页面加载进度条 bower：Web包管理器 jsnice：有趣的js反编译工具，猜压缩后的变量名，http://www.jsnice.org/ D3.js: 是一个基于JavaScript数据展示库（类似的还有P5.js） Zepto.js：移动端替代jQuery的东东，当然也可以使用jquery-mobile. UI框架：Foundation，Boostrap，Pure，EasyUI，Polymer 前段UI设计师必去的几个网站：Dribbble，awwwards，unmatchedstyle，UIMaker Mozilla 开发者中心：https://developer.mozilla.org/en-US/ 图标资源：IcoMoon（我的最爱），Font Awesome, Themify Icons，FreePik，Glyphicons artDialog:非常漂亮的对话框 AdminLTE：github上的一个开源项目，基于Boostrap3的后台管理页面框架 Respond.js：让不懂爱的IE6-8支持响应式设计 require.js: js模块加载库 select2：比chosen具有更多特性的选择框替代库 AngularUI：集成angular.js的UI库 normalize.css: 采用了现代化标准让各浏览器渲染出的html保持一致的库 CreateJS：Html5游戏引擎 Less,Compass:简化CSS开发 emojify.js:用于自动识别网页上的Emoji文字并将其显示为图像 simditor:一个不错的开源的html编辑器，简洁高效 Sencha: 基于html5的移动端开发框架 SuperScrollorama+TweenMax+skrollr:打造超酷的视差滚动效果网页动画 jquery-smooth-scroll:同上，平滑滚动插件 Animate.css:实现了各种动画效果的css库 Emmet:前端工程师必备，前身为 Zen Coding React: facebook出品的js UI库 highlight.js：专门用来做语法高亮的库 GoJS: Html5交互式图表库，看demo更适合层次结构的图表。 10 Pure CSS (Mostly) Flat Mobile Devices: http://marvelapp.github.io/devices.css/ CodePen: http://codepen.io/ jsfiddle: http://jsfiddle.net/ 前端js，html，css测试利器 语言篇折腾中：Scala、Python、Lua、JavaScript、Go 待折腾： Racket OCaml Rust Julia Scala Scala Standard Library API Scala School!: A Scala tutorial by Twitter A Tour of Scala: Tutorial introducing the main concepts of Scala Scala Overview on StackOverflow: A list of useful questions sorted by topic Programming in Scala，最新的第3版，还没有电子版，电子版是第一版 《Scala for the Impatient》 《Scala in Depth》 《Programming Scala》Dean Wampler and Alex Payne. O’Reilly 2009 Scala By Example Scala Cheatsheet学习模式匹配的好资料 Glossary of Scala and FP terms Metascala: A JVM written in Scala LMS: Program Generation and Embedded Compilers in Scala Java 常用的IDE：IntelliJ IDEA(强烈推荐)，Eclipse，Netbeans fastutil: 性能更好的Java集合框架 Guava: 谷歌的Java工具包，应用广泛 Curator：Netflix公司开源的一个Zookeeper client library，用于简化Zookeeper客户端编程，现在已经是apache下的一个独立项目了。Spark的HA也用的这货。 Rx(Reactive Extensions)框架：Vert.x, RxJava(Android中用的比较多), Quasar FindBugs: 代码静态分析工具，找出代码缺陷 Java反编译工具：Luyten，JD-Gui Drools: 规则引擎 Jersey: Java RESTful 框架 canal: 阿里巴巴出品，binlog增量订阅&amp;消费组件 Web开发相关：Tomcat、Resin、Jetty、WebLogic等，常用的组件Struts，Spring，Hibernate Netty: 异步事件驱动网络应用编程框架，用于高并发网络编程比较好（NIO框架，spark 1.2.0就用netty替代了nio） MINA：简单地开发高性能和高可靠性的网络应用程序（也是个NIO框架），不少手游服务端是用它开发的 jOOQ：java Orm框架 Janino: 超级小又快的Java编译器，Spark的Tungsten引起用的它 Activiti:工作流引擎，类似的还有jBPM、Snaker Perfuse:是一个用户界面包用来把有结构与无结构数据以具有交互性的可视化图形展示出来. Gephi:复杂网络分析软件, 其主要用于各种网络和复杂系统，动态和分层图的交互可视化与探测开源工具 Nutch:知名的爬虫项目，hadoop就是从这个项目中发展出来的 web-harvest：Web数据提取工具 POM工具：Maven+Artifactory Akka:一款基于actor模型实现的 并发处理框架 EclEmma：覆盖测试工具 Shiro:安全框架 joda-time:简化时间处理 parboiled:表达式解析 dozer: 深拷贝神器 dubbo: 阿里巴巴出品的分布式服务框架 jackson databind: json序列化工具(fastjson,simplejson) Atomikos: 分布式事务管理 BoneCP：性能很赞的数据库连接池组件，据说比c3p0快好多 ProGuard: obconfuscation tool, 强大的混淆工具 S-99：Scala相关的99个问题 Python PyCharm：最佳Python IDE Eric,Eclipse+pydev,比较不错的Python IDE PyWin:Win32 api编程包 numpy:科学计算包，主要用来处理大型矩阵计算等，此外还有SciPy，Matplotlib GUI相关：PyQt，PyQwt supervisor:进程监控工具 PyGame: 基于Python的多媒体开发和游戏软件开发模块 Web框架: Django 开源web开发框架，它鼓励快速开发,并遵循MVC设计 Swift Swift精选资料 43个优秀的开源项目 客户端 糗事百科 Swift HackerNews Swift 知乎日报app Framework Twitter框架 Mac下简单HTTP Server Swifter 小工具 Swift Alarm Swift Note Swift RSS Reader Swift-PM2.5查询app 游戏 Flappy Swift FanFan Swift .NET Xilium.CefGlue:基于CEF框架的.NET封装，基于.NET开发Chrome内核浏览器 CefSharp：同上，有一款WebKit的封装，C#和Js交互会更简单 netz:免费的 .NET 可执行文件压缩工具 SmartAssembly:变态的.net代码优化混淆工具 NETDeob0：.net反混淆工具，真是魔高一尺道高一丈啊(还有个de4dot，在GitHub上，都是开源的) ILMerge：将所有引用的DLL和exe文件打成一个exe文件 ILSpy:开源.net程序反编译工具 Javascript.NET：很不错的js执行引擎，对v8做了封装 NPOI: Excel操作 DotRAS:远程访问服务的模块 WinHtmlEditor: Winform下的html编辑器 SmartThreadPool:使用C#实现的，带高级特性的线程池 Snoop: WPF Spy Utility Autofac: 轻量级IoC框架 HtmlAgilityPack：Html解析利器 Quartz.NET：Job调度 HttpLib：@CodePlex，简化http请求 SuperSocket：简化Socket操作，基于他的还有个SuperWebSocket，可以开发独立的WebSocket服务器了 DocX：未安装Office的情况下操作Word文件 Dapper：轻量级的ORM类，性能不错 HubbleDotNet：支持接入数据库的全文搜索系统 fastJSON：@CodeProject，高性能的json序列化类 ZXing.NET：@CodePlex，QR，条形码相关 Nancy：轻量级Http服务器，做个小型的Web应用可以摆脱IIS喽(Nancy.Viewengines.Razor,可以加入Razor引擎) AntiXSS：微软的XSS防御库Microsoft Web Protection Library Jint：JavaScript解释器 CS-Script：将C#代码文件作为脚本执行 Jexus：Linux下 高性能、易用、免费的ASP.NET服务器 Clay：将dynamic发挥的更加灵活，像写js一样写C# DynamicJSON：不必定义数据模型获取json数据 SharpPcap：C#版的WinPcap调用端，牛逼的网络包分析库（自带PacketNotNet用于包协议分析） Roslyn：C#，VB编译器 ImageResizer: 服务端自由控制图片大小，真乃神器也，对手机端传小图，PC端传大图，CMS用它很方便 UI相关：DevExpress, Fluent(Office 07风格), mui（Modern UI for WPF） NetSparkle：应用自动更新组件 ConfuserEx: 开源.net混淆工具 ServiceStack: 开源高性能Web服务框架，可用于构建高性能的REST服务 Expression Evaluator：Eval for C#,处理字符串表达式 http://nugetmusthaves.com/ Reactive Extensions (Rx):异步，事件驱动编程包， Rx = Observables + LINQ + Schedulers C &amp; C++ Thrift:用来进行可扩展且跨语言的服务的开发(类似的还有个Avro，Google protobuf)。 libevent:是一个事件触发的网络库，适用于windows、linux、bsd等多种平台，内部使用select、epoll、kqueue等系统调用管理事件机制。（对了还有个libev呢） Boost:不多说了，准C++标准库 Valgrind\\Ptmalloc\\Purify: 调试工具 NetworkServer架构：acceptor-&gt;dispatcher-&gt;worker(这个不算工具哦) POCO - 开源的C++类库及应用程序框架的集合,它主要提供简单的、快速的网络和可移植应用程序 breakpad:崩溃转储和分析模块，很多crashreport会用到 UI界面相关：MFC、BCG和QT这类的就不说了，高端一点的还有Html和DirectUI技术：libcef（基于chrome内核的，想想使用html5开发页面，还真有点小激动呢）、HtmlLayout、Duilib、Bolt，非C++的，还有node-webkit也不错，集成了node和webkit内核。 其他游戏开发相关 MINA：使用Java开发手游和页游服务器(对了还有Netty，也很猛的，都是基于NIO的) HP-Socket：见有有些页游服务器使用这个构建的 Unreal: 虚幻引擎，C++，基于这个引擎的游戏很多 OGRE：大名鼎鼎的3D图形渲染引擎，天龙八部OL、火炬之光等不少游戏都用了这个引擎 OpenVDB：梦工厂C++的特效库，开源的 cocos2d：跨平台2D游戏引擎 unity3d：跨平台3D游戏引擎，很火的哦 Nodejs：也有不少使用它来开发手游和也有服务器（网易的Pomelo） 日志聚合，分布式日志收集 Scribe：Facebook的（nodejs + scribe + inotify 同步日志） logstash:强大的日志收集系统，可以基于logstash+kibana+elasticsearch+redis开发强大的日志分析平台 log.io: nodejs开发的实时日志收集系统 RTP,实时传输协议与音视频 RTP，RTCP，RTSP-&gt; librtp，JRTPLIB(遵循了RFC1889标准) 环形缓冲区，实时数据传输用 SDL,ffmpeg,live555,Speex Red5:用Java开发开源的Flash流媒体服务器。它支持：把音频（MP3）和视频（FLV）转换成播放流； 录制客户端播放流（只支持FLV）；共享对象；现场直播流发布；远程调用。","raw":null,"content":null,"categories":[{"name":"学习资料","slug":"学习资料","permalink":"http://linbingdong.com/categories/学习资料/"}],"tags":[{"name":"好文转载","slug":"好文转载","permalink":"http://linbingdong.com/tags/好文转载/"},{"name":"学习资料","slug":"学习资料","permalink":"http://linbingdong.com/tags/学习资料/"}]},{"title":"纠删码（Erasure Code）浅析","slug":"纠删码（Erasure Code）浅析","date":"2017-03-11T12:33:42.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2017/03/11/纠删码（Erasure Code）浅析/","link":"","permalink":"http://linbingdong.com/2017/03/11/纠删码（Erasure Code）浅析/","excerpt":"【摘要】：副本策略和纠删码是存储领域常见的两种数据冗余技术。相比于副本策略，纠删码具有更高的磁盘利用率。 Reed-Solomon码是一种常见的纠删码。","text":"【摘要】：副本策略和纠删码是存储领域常见的两种数据冗余技术。相比于副本策略，纠删码具有更高的磁盘利用率。 Reed-Solomon码是一种常见的纠删码。 多副本策略即将数据存储多个副本（一般是三副本，比如HDFS），当某个副本丢失时，可以通过其他副本复制回来。三副本的磁盘利用率为1/3。 纠删码技术主要是通过纠删码算法将原始的数据进行编码得到冗余，并将数据和冗余一并存储起来，以达到容错的目的。其基本思想是将n块原始的数据元素通过一定的计算，得到m块冗余元素（校验块）。对于这n+m块的元素，当其中任意的m块元素出错（包括原始数据和冗余数据）时，均可以通过对应的重构算法恢复出原来的n块数据。生成校验的过程被成为编码（encoding），恢复丢失数据块的过程被称为解码（decoding）。磁盘利用率为n/(n+m)。基于纠删码的方法与多副本方法相比具有冗余度低、磁盘利用率高等优点。 两种冗余技术对比如下表： 两种技术 磁盘利用率 计算开销 网络消耗 恢复效率 多副本(3副本) 1/3 几乎没有 较低 较高 纠删码(n+m) n/(n+m) 高 较高 较低 Reed-Solomon(RS)码Reed-Solomon（RS）码是存储系统较为常用的一种纠删码，它有两个参数n和m，记为RS(n,m)。n代表原始数据块个数。m代表校验块个数。接下来介绍RS码的原理。 RS码原理以n=5，m=3为例。即5个原始数据块，乘上一个(n+m)*n的矩阵，然后得出一个(n+m)*1的矩阵。根据矩阵特点可以得知结果矩阵中前面5个值与原来的5个数据块的值相等，而最后3个则是计算出来的校验块。 以上过程为编码过程。D是原始数据块，得到的C为校验块。 假设丢失了m块数据。如下： 那我们如何从剩余的n个数据块（注意，这里剩余的n块可能包含几个原始数据块+几个校验块）恢复出来原始的n个数据块呢，就需要通过下面的decoding（解码）过程来实现。 第一步：从编码矩阵中删去丢失数据块和丢失编码块对应行。 将删掉m个块的(n+m)*1个矩阵变形为n*1矩阵，同时B矩阵也需要删掉对应的m个行得出一个B’的变形矩阵，这个B’就是n*n矩阵。如下：假设D1、D4、C2丢失，我们得到如下B’矩阵及等式。 第二步：求出B’的逆矩阵。 第三步：等式两边分别乘上B’的逆矩阵。 B’和它的逆矩阵相乘得到单位矩阵I，如下： 左边只剩下原始数据矩阵D： 至此完成解码过程。 注：图中黄色部分为范德蒙矩阵。至于如何生成B矩阵，以及如何求B’的逆矩阵，请查看其他相关文献，这里不再赘述。 小结RS的特点： 低冗余度，高磁盘利用率。 数据恢复代价高。 丢失数据块或者编码块时， RS需要读取n个数据块和校验块才能恢复数据， 数据恢复效率也在一定程度上制约了RS的可靠性。 数据更新代价高。 数据更新相当于重新编码， 代价很高， 因此常常针对只读数据，或者冷数据。 工程实践中，一般对于热数据还是会使用多副本策略来冗余，冷数据使用纠删码。 值得期待的是，纠删码技术也即将在Hadoop 3.0中发布。 参考资料 论文《Erasure Codes for Storage Applications》 论文《存储系统中纠删码研究综述》","raw":null,"content":null,"categories":[{"name":"纠删码","slug":"纠删码","permalink":"http://linbingdong.com/categories/纠删码/"}],"tags":[{"name":"存储","slug":"存储","permalink":"http://linbingdong.com/tags/存储/"},{"name":"数据冗余","slug":"数据冗余","permalink":"http://linbingdong.com/tags/数据冗余/"},{"name":"纠删码","slug":"纠删码","permalink":"http://linbingdong.com/tags/纠删码/"}]},{"title":"经典编程书籍","slug":"经典编程书籍","date":"2017-03-11T12:33:42.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2017/03/11/经典编程书籍/","link":"","permalink":"http://linbingdong.com/2017/03/11/经典编程书籍/","excerpt":"经典技术书籍，涵盖：计算机系统与网络、系统架构、算法与数据结构、前端开发、后端开发、移动开发、数据库、测试、项目与团队、程序员职业修炼、求职面试和编程相关的经典书籍。\n原文","text":"经典技术书籍，涵盖：计算机系统与网络、系统架构、算法与数据结构、前端开发、后端开发、移动开发、数据库、测试、项目与团队、程序员职业修炼、求职面试和编程相关的经典书籍。 原文 这个列表综合了伯乐在线网站以往推荐经典书籍文章中的列表，以及在微信和微博中被广泛推荐的好书。虽然已经包括了100多本，覆盖的面也比较全。仍然有很多方面需要补充，而且相信还有很多没有被收录的好书。欢迎大家在 issues 中推荐或自荐。 计算机系统与网络 《图灵的秘密:他的生平、思想及论文解读》 《计算机系统概论》 《深入理解Linux内核》 《深入Linux内核架构》 《TCP/IP详解 卷1：协议》 《Linux系统编程（第2版）》 《Linux内核设计与实现（第3版）》 《深入理解计算机系统（原书第2版）》 《计算机程序的构造和解释（原书第2版）》 《编码：隐匿在计算机软硬件背后的语言》 《性能之颠：洞悉系统、企业与云计算》 《UNIX网络编程 卷1：套接字联网API（第3版）》 《UNIX网络编程 卷2：进程间通信》 《Windows核心编程(第5版)》 《WireShark网络分析就这么简单》 《WireShark网络分析的艺术》 编程通用 《编程原本》 《代码大全》 《UNIX编程艺术》 《代码整洁之道》 《编程珠玑（第2版）》 《编程珠玑（续）》 《软件调试的艺术》 《修改代码的艺术》 《编程语言实现模式》 《编写可读代码的艺术》 《解析极限编程：拥抱变化》 《精通正则表达式（第3版）》 《编译原理（第2版）》龙书 《重构：改善既有代码的设计》 《七周七语言：理解多种编程范型》 《调试九法：软硬件错误的排查之道》 《程序设计语言：实践之路（第3版）》 《计算的本质：深入剖析程序和计算机》 《设计模式 : 可复用面向对象软件的基础》 算法与数据结构 《算法（第4版）》 《算法导论（原书第2版）》 《Python算法教程》 《算法设计与分析基础（第3版）》 《学习 JavaScript 数据结构与算法,编程题》 《数据结构与算法,编程题分析 : C++描述（第4版）》 《数据结构与算法,编程题分析 : C语言描述（第2版）》 《数据结构与算法,编程题分析 : Java语言描述（第2版）》 职业修炼与规划 《大教堂与集市》 《卓有成效的程序员》 《程序员的职业素养》 《程序员修炼之道：从小工到专家》 《软件开发者路线图：从学徒到高手》 《我编程，我快乐: 程序员职业规划之道》 《程序员的思维修炼：开发认知潜能的九堂课》 《高效程序员的45个习惯：敏捷开发修炼之道(修订版)》 大师访谈 《编程大师智慧》 《编程大师访谈录》 《编程人生 : 15位软件先驱访谈录》 《奇思妙想 : 15位计算机天才及其重大发现》 《图灵和ACM图灵奖》 架构/性能 《微服务设计》 《大数据日知录》 《企业应用架构模式》 《Web性能权威指南》 《SRE：Google运维解密》 《发布！软件的设计与部署》 《高扩展性网站的 50 条原则》 《大型网站技术架构:核心原理与案例分析》 《恰如其分的软件架构：风险驱动的设计方法》 《软件系统架构：使用视点和视角与利益相关者合作（第2版）》 Web前端 《高性能 JavaScript》 《锋利的 jQuery（第2版）》 《JavaScript 忍者秘籍》（感谢@joker-danta 补充推荐） 《编写可维护的 JavaScript》 《你不知道的 JavaScript（上）》 《JavaScript 权威指南（第6版）》 《JavaScript 语言精粹（修订版）》 《JavaScript DOM编程艺术 （第2版）》 《JavaScript 高级程序设计（第3版）》 《JavaScript 异步编程：设计快速响应的网络应用》 《Effective JavaScript：编写高质量JavaScript代码的68个有效方法》 《HTML5 权威指南》 《HTML5 秘籍（第2版）》 《HTML5 与 CSS3 基础教程（第八版）》 《CSS 揭秘》 《CSS 设计指南（第3版）》 《CSS 权威指南（第3版）》 《深入浅出 HTML 与 CSS》 Java开发 《Java8 实战》 《Java并发编程实战》 《Java性能权威指南》 《Java程序员修炼之道》 《实战Java高并发程序设计》 《Java编程思想 （第4版）》 《深入理解Java虚拟机（第2版）》 《Effective java 中文版（第2版）》 《Java核心技术·卷1：基础知识（原书第9版）》 《Java核心技术·卷2：高级特性（原书第9版）》 .NET 《精通C#（第6版）》 《深入理解C#（第3版）》 《CLR via C#（第4版）》 Python 《集体智慧编程》 《笨办法学Python》 《Python基础教程》 《Python源码剖析》 《Head First Python》 《与孩子一起学编程》 《Python学习手册（第4版）》 《Python Cookbook（第3版）》 《Python参考手册（第4版）》 《Python核心编程（第3版）》 《Python科学计算（第2版）》 《利用 Python 进行数据分析》 《Think Python：像计算机科学家一样思考Python（第2版）》 《Python编程实战:运用设计模式、并发和程序库创建高质量程序》 《Python绝技：运用Python成为顶级黑客》 《Flask Web开发:基于Python的Web应用开发实战》 Android 《Android编程权威指南（第2版）》 《移动应用UI设计模式（第2版）》 iOS 《iOS编程实战》 《iOS编程（第4版）》 《Objective-C高级编程》 《Effective Objective-C 2.0：编写高质量iOS与OS X代码的52个有效方法》 PHP 《Head First PHP &amp; MySQL（中文版）》 《深入PHP：面向对象、模式与实践（第3版）》 C语言 《C标准库》 《C和指针》 《C专家编程》 《C陷阱与缺陷》 《C语言接口与实现》 《C程序设计语言（第2版）》 《C语言参考手册（第5版）》 C++ 《C++标准库》 《C++编程思想》 《C++语言的设计与演化》 《C++程序设计原理与实践》 《C++ Primer （中文第5版）》 《C++ Primer习题集(第5版) 》 《C++程序设计语言(第1-3部分)(原书第4版) 》 《Effective C++:改善程序与设计的55个具体做法(第3版)(中文版) 》 《More Effective C++:35个改善编程与设计的有效方法(中文版) 》&nbsp; 机器学习和数据挖掘 《数据之巅》 《矩阵分析》 《机器学习》 《统计学习方法》 《机器学习导论》 《推荐系统实践》 《机器学习实战》 《Web数据挖掘》 《深入浅出统计学》 《模式分类（第2版）》 《概率论与数理统计》 《统计学习基础(第2版)(英文) 》 《数据挖掘：概念与技术（第3版）》 《数据挖掘：实用机器学习工具与技术（原书第3版）》 《大数据：互联网大规模数据挖掘与分布式处理（第2版）》 数据库 《SQL应用重构》 《SQL Cookbook》 《高性能MySQL （第3版）》 《深入浅出SQL（中文版）》 《MySQL技术内幕 : InnoDB存储引擎（第2版）》 《深入浅出MySQL : 数据库开发、优化与管理维护》 测试 《探索式软件测试》 《有效的单元测试》 《Google软件测试之道》 项目与团队 《人月神话》 《快速软件开发》 《人件（原书第3版）》 《门后的秘密：卓越管理的故事》 《极客与团队：软件工程师的团队生存秘笈》 求职面试 《程序员面试金典（第5版）》 《编程之美 : 微软技术面试心得》 《金领简历：敲开苹果、微软、谷歌的大门》 《剑指Offer：名企面试官精讲典型编程题（纪念版）》 编程之外 《暗时间》 《数学之美》 《赢得朋友》 《精益创业》 《批判性思维》 《世界是数字的》 《程序员的数学》 《程序员健康指南》 《禅与摩托车维修艺术》 《关键对话：如何高效能沟通》 《写作法宝：非虚构写作指南》 《黑客与画家 : 来自计算机时代的高见》 《软件随想录（卷1）》《软件随想录（卷2）》 《如何把事情做到最好：改变全球9800万人的人生指导书》","raw":null,"content":null,"categories":[{"name":"学习资料","slug":"学习资料","permalink":"http://linbingdong.com/categories/学习资料/"}],"tags":[{"name":"学习资料","slug":"学习资料","permalink":"http://linbingdong.com/tags/学习资料/"},{"name":"经典编程书籍","slug":"经典编程书籍","permalink":"http://linbingdong.com/tags/经典编程书籍/"}]},{"title":"设计模式-观察者模式","slug":"设计模式-观察者模式","date":"2017-03-11T12:33:42.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2017/03/11/设计模式-观察者模式/","link":"","permalink":"http://linbingdong.com/2017/03/11/设计模式-观察者模式/","excerpt":"观察者模式又叫发布/订阅模式。","text":"观察者模式又叫发布/订阅模式。 定义观察者模式定义了对象之间的一对多依赖，使得当一个对象改变状态时，它的所有依赖者都会收到通知并自动更新。观察者模式又叫发布/订阅模式。 角色 抽象主题（Subject）：它把所有观察者对象的引用保存到一个列表里，每个主题都可以有任何数量的观察者。抽象主题提供一个接口，可以增加和删除观察者对象。 具体主题（ConcreteSubject）：将有关状态存入具体观察者对象；在具体主题内部状态改变时，给所有登记过的观察者发出通知。 抽象观察者（Observer）：为所有的具体观察者定义一个接口，在得到主题通知时更新自己。 具体观察者（ConcreteObserver）：实现抽象观察者角色所要求的更新接口，以便使本身的状态与主题状态协调。 类图 示例以猎头-求职者为例。猎头是主题，求职者是观察者。 Subject 接口 public interface Subject &#123; public void registerObserver(Observer o); public void removeObserver(Observer o); public void notifyAllObservers();&#125; Observer 接口 public interface Observer &#123; public void update(Subject s);&#125; HeadHunter 类实现 Subject 接口 import java.util.LinkedList;public class HeadHunter implements Subject &#123; private LinkedList&lt;Observer&gt; userList; private LinkedList&lt;String&gt; jobs; public HeadHunter() &#123; userList = new LinkedList&lt;Observer&gt;(); jobs = new LinkedList&lt;String&gt;(); &#125; @Override public void registerObserver(Observer o) &#123; userList.add(o); &#125; @Override public void removeObserver(Observer o) &#123; userList.remove(o); &#125; @Override public void notifyAllObservers() &#123; for (Observer o: userList) &#123; o.update(this); &#125; &#125; public void addJob(String job) &#123; jobs.add(job); notifyAllObservers(); &#125; public LinkedList&lt;String&gt; getJobs() &#123; return jobs; &#125; public String toString() &#123; return jobs.toString(); &#125;&#125; JobSeeker 类实现 Observer 接口 public class JobSeeker implements Observer &#123; private String name; public JobSeeker(String name) &#123; this.name = name; &#125; @Override public void update(Subject s) &#123; System.out.println(this.name + \" got notified!\"); System.out.println(s); &#125;&#125; 测试 public class Main &#123; public static void main(String[] args) &#123; HeadHunter hh = new HeadHunter(); JobSeeker lbd = new JobSeeker(\"lbd\"); JobSeeker lbx = new JobSeeker(\"lbx\"); JobSeeker lbn = new JobSeeker(\"lbn\"); JobSeeker lbb = new JobSeeker(\"lbb\"); hh.registerObserver(lbd); hh.registerObserver(lbx); hh.registerObserver(lbn); hh.registerObserver(lbb); hh.removeObserver(lbb); hh.addJob(\"looking for Java engineers\"); hh.addJob(\"looking for Python engineers\"); &#125;&#125; 输出 lbd got notified![looking for Java engineers]lbx got notified![looking for Java engineers]lbn got notified![looking for Java engineers]lbd got notified![looking for Java engineers, looking for Python engineers]lbx got notified![looking for Java engineers, looking for Python engineers]lbn got notified![looking for Java engineers, looking for Python engineers] 总结观察者模式使主题和观察者之间松耦合，松耦合的设计能够让我们建立有弹性的OO系统，能够应对变化，因为对象之间的相互依赖降到了最低。 其他 上面的例子是观察者模式的“推”模式，还有一种“拉”模式。 Java 的 java.util 库里面，提供了一个 Observable 类以及一个 Observer 接口，构成 Java 语言对观察者模式的支持。","raw":null,"content":null,"categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://linbingdong.com/categories/设计模式/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/tags/Java/"},{"name":"设计模式","slug":"设计模式","permalink":"http://linbingdong.com/tags/设计模式/"}]},{"title":"Java中四种引用","slug":"Java中四种引用","date":"2017-03-11T12:33:42.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2017/03/11/Java中四种引用/","link":"","permalink":"http://linbingdong.com/2017/03/11/Java中四种引用/","excerpt":"\n为了使能更加灵活地控制对象的生命周期。从JDK 1.2版本开始，把对象的引用分为4种级别。这4种级别由高到低依次为：强引用、软引用、弱引用和虚引用。\n","text":"为了使能更加灵活地控制对象的生命周期。从JDK 1.2版本开始，把对象的引用分为4种级别。这4种级别由高到低依次为：强引用、软引用、弱引用和虚引用。 强引用（StrongReference）强引用是级别最高，也是最常用的引用。拥有强引用的对象绝不会被垃圾回收器回收。 软引用（SoftReference）如果一个对象只具有软引用，则内存空间足够，垃圾回收器就不会回收它；如果内存空间不足了，就会回收这些对象的内存。 弱引用（WeakReference）只具有弱引用的对象拥有更短暂的生命周期。在执行gc的时候会被回收。 虚引用（PhantomReference）“虚引用”顾名思义，就是形同虚设，虚引用并不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收器回收。仅用于在发生gc时接收一个系统通知。","raw":null,"content":null,"categories":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://linbingdong.com/tags/Java/"}]},{"title":"分布式一致性算法：Raft 算法（Raft 论文翻译）","slug":"分布式一致性算法：Raft 算法（Raft 论文翻译）","date":"2017-03-11T12:33:42.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2017/03/11/分布式一致性算法：Raft 算法（Raft 论文翻译）/","link":"","permalink":"http://linbingdong.com/2017/03/11/分布式一致性算法：Raft 算法（Raft 论文翻译）/","excerpt":"Raft 算法是可以用来替代 Paxos 算法的分布式一致性算法，而且 raft 算法比 Paxos 算法更易懂且更容易实现。本文对 raft 论文进行翻译，希望能有助于读者更方便地理解 raft 的思想。如果对 Paxos 算法感兴趣，可以看我的另一篇文章：分布式系列文章——Paxos算法原理与推导","text":"Raft 算法是可以用来替代 Paxos 算法的分布式一致性算法，而且 raft 算法比 Paxos 算法更易懂且更容易实现。本文对 raft 论文进行翻译，希望能有助于读者更方便地理解 raft 的思想。如果对 Paxos 算法感兴趣，可以看我的另一篇文章：分布式系列文章——Paxos算法原理与推导 摘要Raft 是用来管理复制日志（replicated log）的一致性协议。它跟 multi-Paxos 作用相同，效率也相当，但是它的组织结构跟 Paxos 不同。这使得 Raft 比 Paxos 更容易理解并且更容易在工程实践中实现。为了使 Raft 协议更易懂，Raft 将一致性的关键元素分开，如 leader 选举、日志复制和安全性，并且它实施更强的一致性以减少必须考虑的状态的数量。用户研究的结果表明，Raft 比 Paxos 更容易学习。 Raft 还包括一个用于变更集群成员的新机制，它使用重叠的大多数（overlapping majorities）来保证安全性。 1 介绍一致性算法允许多台机器作为一个集群协同工作，并且在其中的某几台机器出故障时集群仍然能正常工作。 正因为如此，一致性算法在建立可靠的大规模软件系统方面发挥了关键作用。 在过去十年中，Paxos [15,16] 主导了关于一致性算法的讨论：大多数一致性的实现都是基于 Paxos 或受其影响，Paxos 已成为用于教授学生一致性相关知识的主要工具。 不幸的是，Paxos 实在是太难以理解，尽管许多人一直在努力尝试使其更易懂。 此外，其架构需要复杂的改变来支持实际系统。 结果是，系统开发者和学生都在与 Paxos 斗争。 在我们自己与 Paxos 斗争之后，我们开始着手寻找一个新的一致性算法，可以为系统开发和教学提供更好的基础。 我们的方法是不寻常的，因为我们的主要目标是可理解性：我们可以为实际系统定义一个一致性算法，并以比 Paxos 更容易学习的方式描述它吗？在该算法的设计过程中，重要的不仅是如何让该算法起作用，还有清晰地知道该算法为什么会起作用。 这项工作的结果是一个称为 Raft 的一致性算法。 在设计 Raft 时，我们使用了特定的技术来提高可理解性，包括分解（Raft 分离 leader 选举，日志复制和安全）和状态空间减少（相对于 Paxos ，Raft 减少了不确定性程度和服务器之间彼此不一致的方式 ）。 一项针对两个大学的 43 名学生的用户研究表明，Raft 比 Paxos 更容易理解：在学习两种算法后，其中 33 名学生能够更好地回答关于 Raft 的问题。 Raft 在许多方面类似于现有的一致性算法（尤其是 Oki 和 Liskov 的 Viewstamped Replication [29,22]），但它有几个新特性： Strong leader：在 Raft 中，日志条目（log entries）只从 leader 流向其他服务器。 这简化了复制日志的管理，使得 raft 更容易理解。 Leader 选举：Raft 使用随机计时器进行 leader 选举。 这只需在任何一致性算法都需要的心跳（heartbeats）上增加少量机制，同时能够简单快速地解决冲突。 成员变更：Raft 使用了一种新的联合一致性方法，其中两个不同配置的大多数在过渡期间重叠。 这允许集群在配置更改期间继续正常运行。 我们认为，Raft 优于 Paxos 和其他一致性算法，不仅在教学方面，在工程实现方面也是。 它比其他算法更简单且更易于理解; 它被描述得十分详细足以满足实际系统的需要; 它有多个开源实现，并被多家公司使用; 它的安全性已被正式规定和验证; 它的效率与其他算法相当。 本文的剩余部分介绍了复制状态机问题（第 2 节），讨论了 Paxos 的优点和缺点（第3节），描述了我们实现易理解性的方法（第 4 节），提出了 Raft 一致性算法（第 5-8 节），评估 Raft（第 9 节），并讨论了相关工作（第 10 节）。 2 复制状态机一致性算法是在复制状态机[37]的背景下产生的。 在这种方法中，一组服务器上的状态机计算相同状态的相同副本，并且即使某些服务器宕机，也可以继续运行。 复制状态机用于解决分布式系统中的各种容错问题。 例如，具有单个 leader 的大规模系统，如 GFS [8]，HDFS [38] 和 RAMCloud [33] ，通常使用单独的复制状态机来进行 leader 选举和存储 leader 崩溃后重新选举需要的配置信息。Chubby [2] 和 ZooKeeper [11] 都是复制状态机。 复制状态机通常使用复制日志实现，如图 1 所示。每个服务器存储一个包含一系列命令的日志，其状态机按顺序执行日志中的命令。 每个日志中命令都相同并且顺序也一样，因此每个状态机处理相同的命令序列。 这样就能得到相同的状态和相同的输出序列。 一致性算法的工作就是保证复制日志的一致性。 每台服务器上的一致性模块接收来自客户端的命令，并将它们添加到其日志中。 它与其他服务器上的一致性模块通信，以确保每个日志最终以相同的顺序包含相同的命令，即使有一些服务器失败。 一旦命令被正确复制，每个服务器上的状态机按日志顺序处理它们，并将输出返回给客户端。 这样就形成了高可用的复制状态机。 实际系统中的一致性算法通常具有以下属性： 它们确保在所有非拜占庭条件下（包括网络延迟，分区和数据包丢失，重复和乱序）的安全性（不会返回不正确的结果）。 只要任何大多数（过半）服务器都可以运行，并且可以相互通信和与客户通信，一致性算法就可用。 因此，五台服务器的典型集群可以容忍任何两台服务器的故障。 假设服务器突然宕机，它们可以稍后从状态恢复并重新加入群集。 它们不依赖于时序来确保日志的一致性：错误的时钟和极端消息延迟在最坏的情况下会导致可用性问题（译者注：言外之意是可以保证一致性）。 在通常情况下，只要集群的大部分（过半服务器）已经响应了单轮远程过程调用，命令就可以完成; 少数（一半以下）慢服务器不会影响整个系统性能。 3 Paxos 存在的问题在过去十年里，Leslie Lamport 的 Paxos 协议[15]几乎成为一致性的同义词：它是课堂上教授最多的一致性协议，并且大多数一致性的实现也以它为起点。 Paxos 首先定义了能够在单个决策（例如单个复制日志条目）上达成一致的协议。 我们将这个子集称为 single-decree Paxos。 然后 Paxos 组合该协议的多个实例以促进一系列决策，例如日志（multi-Paxos）。 Paxos能够确保安全性和活性，并且支持集群成员的变更。它的正确性已被证明，并且在正常情况下是高效的。 不幸的是，Paxos 有两个显著的缺点。 第一个缺点是 Paxos 非常难以理解。 Paxos 的描述晦涩难懂，臭名昭著（译者注：《The Part-time Parliament》比较晦涩难懂，但是《Paxos Made Simple》就比较容易理解）; 很少有人成功地理解它，即使能理解也必须付出巨大的努力。 因此，已有几个尝试用更简单的方式来描述 Paxos [16,20,21] 。 这些描述集中在 single-degree Paxos ，但它们仍然具有挑战性。 在对 NSDI 2012 参会者的非正式调查中，我们发现很少有人喜欢 Paxos ，即使是经验丰富的研究人员。 我们自己也跟 Paxos 进行了艰苦的斗争; 我们也无法完全理解整个协议，直到阅读了几个更简单的描述和自己设计替代 Paxos 的协议，整个过程花了将近一年。 Paxos 晦涩难懂的原因是作者选择了single-degree Paxos作为基础。Single-decree Paxos 分成两个阶段，这两个阶段没有简单直观的说明，并且不能被单独理解。因此，很难理解为什么该算法能起作用。Multi-Paxos 的合成规则又增加了许多复杂性。我们相信，对多个决定（日志而不是单个日志条目）达成一致的总体问题可以用其他更直接和更明显的方式进行分解。 Paxos的第二个问题是它不能为构建实际的实现提供良好的基础。 一个原因是没有针对 multi-Paxos 的广泛同意的算法。 Lamport的描述主要是关于 single-decree Paxos; 他描述了 multi-Paxos 的可能方法，但缺少许多细节。 已经有几个尝试来具体化和优化 Paxos ，例如[26]，[39]和[13]，但这些彼此各不相同并且跟 Lamport 描述的也不同。 像Chubby [4] 这样的系统已经实现了类 Paxos（Paxos-like）算法，但大多数情况下，它们的细节并没有公布。 此外，Paxos 的架构对于构建实际系统来说是一个糟糕的设计，这是 single-decree 分解的另一个结果。 例如，独立地选择日志条目集合，然后再将它们合并到顺序日志中几乎没有任何好处，这只会增加复杂性。 围绕日志设计系统是更简单和有效的方法，新日志条目按照约束顺序地添加到日志中。 Paxos 的做法适用于只需要做一次决策的情况，如果需要做一系列决策，更简单和快速的方法是先选择一个 leader ，然后让该 leader 协调这些决策。 因此，实际的系统跟 Paxos 相差很大。几乎所有的实现都是从 Paxos 开始，然后发现很多实现上的难题，接着就开发了一种和 Paxos 完全不一样的架构。这样既费时又容易出错，而且 Paxos 本身晦涩难懂使得该问题更加严重。Paxos 的公式可能可以很好地证明它的正确性，但是现实的系统和 Paxos 差别是如此之大，以至于这些证明并没有什么太大的价值。下面来自 Chubby 作者的评论非常典型： 在Paxos算法描述和实现现实系统之间有着巨大的鸿沟。最终的系统往往建立在一个还未被证明的协议之上。 由于以上问题，我们得出的结论是 Paxos 算法没有为系统实践和教学提供一个良好的基础。考虑到一致性问题在大规模软件系统中的重要性，我们决定尝试设计一个能够替代 Paxos 并且具有更好特性的一致性算法。Raft算法就是这次实验的结果。 4 为可理解性而设计在设计 Raft 算法过程中我们有几个目标：它必须提供一个完整的实际的系统实现基础，这样才能大大减少开发者的工作；它必须在任何情况下都是安全的并且在典型的应用条件下是可用的；并且在正常情况下是高效的。但是我们最重要的目标也是最大的挑战是可理解性。它必须保证能够被大多数人容易地理解。另外，它必须能够让人形成直观的认识，这样系统的构建者才能够在现实中进行扩展。 在设计 Raft 算法的时候，很多情况下我们需要在多个备选方案中进行选择。在这种情况下，我们基于可理解性来评估备选方案：解释各个备选方案的难道有多大（例如，Raft 的状态空间有多复杂，是否有微妙的含义）？对于一个读者而言，完全理解这个方案和含义是否容易？ 我们意识到这样的分析具有高度的主观性；但是我们使用了两种通用的技术来解决这个问题。第一个技术就是众所周知的问题分解：只要有可能，我们就将问题分解成几个相对独立的，可被解决的、可解释的和可理解的子问题。例如，Raft 算法被我们分成 leader 选举，日志复制，安全性和成员变更几个部分。 我们使用的第二个方法是通过减少状态的数量来简化状态空间，使得系统更加连贯并且尽可能消除不确定性。特别的，所有的日志是不允许有空洞的，并且 Raft 限制了使日志之间不一致的方式。尽管在大多数情况下我们都试图去消除不确定性，但是在某些情况下不确定性可以提高可理解性。特别是，随机化方法虽然引入了不确定性，但是他们往往能够通过使用相近的方法处理可能的选择来减少状态空间。我们使用随机化来简化 Raft 中的 leader 选举算法。 5 Raft 一致性算法Raft 是一种用来管理第 2 节中描述的复制日志的算法。图 2 是该算法的浓缩，可用作参考，图 3 列举了该算法的一些关键特性。图中的这些内容将在剩下的章节中逐一介绍。 Raft 通过首先选举一个 distinguished leader，然后让它全权负责管理复制日志来实现一致性。Leader 从客户端接收日志条目，把日志条目复制到其他服务器上，并且在保证安全性的时候通知其他服务器将日志条目应用到他们的状态机中。拥有一个 leader 大大简化了对复制日志的管理。例如，leader 可以决定新的日志条目需要放在日志中的什么位置而不需要和其他服务器商议，并且数据都是从 leader 流向其他服务器。leader 可能宕机，也可能和其他服务器断开连接，这时一个新的 leader 会被选举出来。 通过选举一个 leader 的方式，Raft 将一致性问题分解成了三个相对独立的子问题，这些问题将会在接下来的子章节中进行讨论： Leader 选举：当前的 leader 宕机时，一个新的 leader 必须被选举出来。（5.2 节） 日志复制：Leader 必须从客户端接收日志条目然后复制到集群中的其他节点，并且强制要求其他节点的日志和自己的保持一致。 安全性：Raft 中安全性的关键是图 3 中状态机的安全性：如果有任何的服务器节点已经应用了一个特定的日志条目到它的状态机中，那么其他服务器节点不能在同一个日志索引位置应用一条不同的指令。章节 5.4 阐述了 Raft 算法是如何保证这个特性的；该解决方案在选举机制（5.2 节）上增加了额外的限制。 在展示一致性算法之后，本章节将讨论可用性的一些问题以及时序在系统中的作用。 5.1 Raft 基础一个 Raft 集群包含若干个服务器节点；通常是 5 个，这样的系统可以容忍 2 个节点的失效。在任何时刻，每一个服务器节点都处于这三个状态之一：leader、follower 或者 candidate 。在正常情况下，集群中只有一个 leader 并且其他的节点全部都是 follower 。Follower 都是被动的：他们不会发送任何请求，只是简单的响应来自 leader 和 candidate 的请求。Leader 处理所有的客户端请求（如果一个客户端和 follower 通信，follower 会将请求重定向给 leader）。第三种状态，candidate ，是用来选举一个新的 leader（章节 5.2）。图 4 展示了这些状态和他们之间的转换关系；这些转换关系在接下来会进行讨论。 Raft 把时间分割成任意长度的任期（term），如图 5 所示。任期用连续的整数标记。每一段任期从一次选举开始，一个或者多个 candidate 尝试成为 leader 。如果一个 candidate 赢得选举，然后他就在该任期剩下的时间里充当 leader 。在某些情况下，一次选举无法选出 leader 。在这种情况下，这一任期会以没有 leader 结束；一个新的任期（包含一次新的选举）会很快重新开始。Raft 保证了在任意一个任期内，最多只有一个 leader 。 不同的服务器节点观察到的任期转换的次数可能不同，在某些情况下，一个服务器节点可能没有看到 leader 选举过程或者甚至整个任期全程。任期在 Raft 算法中充当逻辑时钟的作用，这使得服务器节点可以发现一些过期的信息比如过时的 leader 。每一个服务器节点存储一个当前任期号，该编号随着时间单调递增。服务器之间通信的时候会交换当前任期号；如果一个服务器的当前任期号比其他的小，该服务器会将自己的任期号更新为较大的那个值。如果一个 candidate 或者 leader 发现自己的任期号过期了，它会立即回到 follower 状态。如果一个节点接收到一个包含过期的任期号的请求，它会直接拒绝这个请求。 Raft 算法中服务器节点之间使用 RPC 进行通信，并且基本的一致性算法只需要两种类型的 RPC。请求投票（RequestVote） RPC 由 candidate 在选举期间发起（章节 5.2），追加条目（AppendEntries）RPC 由 leader 发起，用来复制日志和提供一种心跳机制（章节 5.3）。第 7 节为了在服务器之间传输快照增加了第三种 RPC。当服务器没有及时的收到 RPC 的响应时，会进行重试， 并且他们能够并行的发起 RPC 来获得最佳的性能。 5.2 Leader 选举Raft 使用一种心跳机制来触发 leader 选举。当服务器程序启动时，他们都是 follower 。一个服务器节点只要能从 leader 或 candidate 处接收到有效的 RPC 就一直保持 follower 状态。Leader 周期性地向所有 follower 发送心跳（不包含日志条目的 AppendEntries RPC）来维持自己的地位。如果一个 follower 在一段选举超时时间内没有接收到任何消息，它就假设系统中没有可用的 leader ，然后开始进行选举以选出新的 leader 。 要开始一次选举过程，follower 先增加自己的当前任期号并且转换到 candidate 状态。然后投票给自己并且并行地向集群中的其他服务器节点发送 RequestVote RPC（让其他服务器节点投票给它）。Candidate 会一直保持当前状态直到以下三件事情之一发生：(a) 它自己赢得了这次的选举（收到过半的投票），(b) 其他的服务器节点成为 leader ，(c) 一段时间之后没有任何获胜者。这些结果会在下面的章节里分别讨论。 当一个 candidate 获得集群中过半服务器节点针对同一个任期的投票，它就赢得了这次选举并成为 leader 。对于同一个任期，每个服务器节点只会投给一个 candidate ，按照先来先服务（first-come-first-served）的原则（注意：5.4 节在投票上增加了额外的限制）。要求获得过半投票的规则确保了最多只有一个 candidate 赢得此次选举（图 3 中的选举安全性）。一旦 candidate 赢得选举，就立即成为 leader 。然后它会向其他的服务器节点发送心跳消息来确定自己的地位并阻止新的选举。 在等待投票期间，candidate 可能会收到另一个声称自己是 leader 的服务器节点发来的 AppendEntries RPC 。如果这个 leader 的任期号（包含在RPC中）不小于 candidate 当前的任期号，那么 candidate 会承认该 leader 的合法地位并回到 follower 状态。 如果 RPC 中的任期号比自己的小，那么 candidate 就会拒绝这次的 RPC 并且继续保持 candidate 状态。 第三种可能的结果是 candidate 既没有赢得选举也没有输：如果有多个 follower 同时成为 candidate ，那么选票可能会被瓜分以至于没有 candidate 赢得过半的投票。当这种情况发生时，每一个候选人都会超时，然后通过增加当前任期号来开始一轮新的选举。然而，如果没有其他机制的话，该情况可能会无限重复。 Raft 算法使用随机选举超时时间的方法来确保很少发生选票瓜分的情况，就算发生也能很快地解决。为了阻止选票一开始就被瓜分，选举超时时间是从一个固定的区间（例如 150-300 毫秒）随机选择。这样可以把服务器都分散开以至于在大多数情况下只有一个服务器会选举超时；然后该服务器赢得选举并在其他服务器超时之前发送心跳。同样的机制被用来解决选票被瓜分的情况。每个 candidate 在开始一次选举的时候会重置一个随机的选举超时时间，然后一直等待直到选举超时；这样减小了在新的选举中再次发生选票瓜分情况的可能性。9.3 节展示了该方案能够快速地选出一个 leader 。 选举的例子可以很好地展示可理解性是如何指导我们选择设计方案的。起初我们打算使用一种等级系统（ranking system）：每一个 candidate 都被赋予一个唯一的等级（rank），等级用来在竞争的 candidate 之间进行选择。如果一个 candidate 发现另一个 candidate 拥有更高的等级，它就会回到 follower 状态，这样高等级的 candidate 能够更加容易地赢得下一次选举。但是我们发现这种方法在可用性方面会有一下小问题。我们对该算法进行了多次调整，但是每次调整之后都会有新的小问题。最终我们认为随机重试的方法更加显然且易于理解。 5.3 日志复制Leader 一旦被选举出来，就开始为客户端请求提供服务。客户端的每一个请求都包含一条将被复制状态机执行的指令。Leader 把该指令作为一个新的条目追加到日志中去，然后并行的发起 AppendEntries RPC 给其他的服务器，让它们复制该条目。当该条目被安全地复制（下面会介绍），leader 会应用该条目到它的状态机中（状态机执行该指令）然后把执行的结果返回给客户端。如果 follower 崩溃或者运行缓慢，或者网络丢包，leader 会不断地重试 AppendEntries RPC（即使已经回复了客户端）直到所有的 follower 最终都存储了所有的日志条目。 日志以图 6 展示的方式组织。每个日志条目存储一条状态机指令和 leader 收到该指令时的任期号。任期号用来检测多个日志副本之间的不一致情况，同时也用来保证图 3 中的某些性质。每个日志条目都有一个整数索引值来表明它在日志中的位置。 Leader 决定什么时候把日志条目应用到状态机中是安全的；这种日志条目被称为已提交的。Raft 算法保证所有已提交的日志条目都是持久化的并且最终会被所有可用的状态机执行。一旦创建该日志条目的 leader 将它复制到过半的服务器上，该日志条目就会被提交（例如在图 6 中的条目 7）。同时，leader 日志中该日志条目之前的所有日志条目也都会被提交，包括由其他 leader 创建的条目。5.4 节讨论在 leader 变更之后应用该规则的一些细节，并且证明了这种提交的规则是安全的。Leader 追踪将会被提交的日志条目的最大索引，未来的所有 AppendEntries RPC 都会包含该索引，这样其他的服务器才能最终知道哪些日志条目需要被提交。Follower 一旦知道某个日志条目已经被提交就会将该日志条目应用到自己的本地状态机中（按照日志的顺序）。 我们设计了 Raft 日志机制来维持不同服务器之间日志高层次的一致性。这么做不仅简化了系统的行为也使得系统行为更加可预测，同时该机制也是保证安全性的重要组成部分。Raft 维护着以下特性，这些同时也构成了图 3 中的日志匹配特性： 如果不同日志中的两个条目拥有相同的索引和任期号，那么他们存储了相同的指令。 如果不同日志中的两个条目拥有相同的索引和任期号，那么他们之前的所有日志条目也都相同。 Leader 在特定的任期号内的一个日志索引处最多创建一个日志条目，同时日志条目在日志中的位置也从来不会改变。该点保证了上面的第一条特性。第二个特性是由 AppendEntries RPC 执行一个简单的一致性检查所保证的。在发送 AppendEntries RPC 的时候，leader 会将前一个日志条目的索引位置和任期号包含在里面。如果 follower 在它的日志中找不到包含相同索引位置和任期号的条目，那么他就会拒绝该新的日志条目。一致性检查就像一个归纳步骤：一开始空的日志状态肯定是满足 Log Matching Property（日志匹配特性） 的，然后一致性检查保证了日志扩展时的日志匹配特性。因此，每当 AppendEntries RPC 返回成功时，leader 就知道 follower 的日志一定和自己相同（从第一个日志条目到最新条目）。 正常操作期间，leader 和 follower 的日志保持一致，所以 AppendEntries RPC 的一致性检查从来不会失败。然而，leader 崩溃的情况会使日志处于不一致的状态（老的 leader 可能还没有完全复制它日志里的所有条目）。这种不一致会在一系列的 leader 和 follower 崩溃的情况下加剧。图 7 展示了在什么情况下 follower 的日志可能和新的 leader 的日志不同。Follower 可能缺少一些在新 leader 中有的日志条目，也可能拥有一些新 leader 没有的日志条目，或者同时发生。缺失或多出日志条目的情况可能会涉及到多个任期。 图 7：当一个 leader 成功当选时（最上面那条日志），follower 可能是（a-f）中的任何情况。每一个盒子表示一个日志条目；里面的数字表示任期号。Follower 可能会缺少一些日志条目（a-b），可能会有一些未被提交的日志条目（c-d），或者两种情况都存在（e-f）。例如，场景 f 可能这样发生，f 对应的服务器在任期 2 的时候是 leader ，追加了一些日志条目到自己的日志中，一条都还没提交（commit）就崩溃了；该服务器很快重启，在任期 3 重新被选为 leader，又追加了一些日志条目到自己的日志中；在这些任期 2 和任期 3 中的日志都还没被提交之前，该服务器又宕机了，并且在接下来的几个任期里一直处于宕机状态。 在 Raft 算法中，leader 通过强制 follower 复制它的日志来解决不一致的问题。这意味着 follower 中跟 leader 冲突的日志条目会被 leader 的日志条目覆盖。5.4 节会证明通过增加一个限制可以保证安全性。 要使得 follower 的日志跟自己一致，leader 必须找到两者达成一致的最大的日志条目（索引最大），删除 follower 日志中从那个点之后的所有日志条目，并且将自己从那个点之后的所有日志条目发送给 follower 。所有的这些操作都发生在对 AppendEntries RPCs 中一致性检查的回复中。Leader 针对每一个 follower 都维护了一个 nextIndex ，表示 leader 要发送给 follower 的下一个日志条目的索引。当选出一个新 leader 时，该 leader 将所有 nextIndex 的值都初始化为自己最后一个日志条目的 index 加1（图 7 中的 11）。如果 follower 的日志和 leader 的不一致，那么下一次 AppendEntries RPC 中的一致性检查就会失败。在被 follower 拒绝之后，leaer 就会减小 nextIndex 值并重试 AppendEntries RPC 。最终 nextIndex 会在某个位置使得 leader 和 follower 的日志达成一致。此时，AppendEntries RPC 就会成功，将 follower 中跟 leader 冲突的日志条目全部删除然后追加 leader 中的日志条目（如果有需要追加的日志条目的话）。一旦 AppendEntries RPC 成功，follower 的日志就和 leader 一致，并且在该任期接下来的时间里保持一致。 如果想要的话，该协议可以被优化来减少被拒绝的 AppendEntries RPC 的个数。例如，当拒绝一个 AppendEntries RPC 的请求的时候，follower 可以包含冲突条目的任期号和自己存储的那个任期的第一个 index 。借助这些信息，leader 可以跳过那个任期内所有冲突的日志条目来减小 nextIndex；这样就变成每个有冲突日志条目的任期需要一个 AppendEntries RPC 而不是每个条目一次。在实践中，我们认为这种优化是没有必要的，因为失败不经常发生并且也不可能有很多不一致的日志条目。 通过这种机制，leader 在当权之后就不需要任何特殊的操作来使日志恢复到一致状态。Leader 只需要进行正常的操作，然后日志就能在回复 AppendEntries 一致性检查失败的时候自动趋于一致。Leader 从来不会覆盖或者删除自己的日志条目（图 3 的 Leader Append-Only 属性）。 这样的日志复制机制展示了第 2 节中描述的一致性特性：只要过半的服务器能正常运行，Raft 就能够接受，复制并应用新的日志条目；在正常情况下，新的日志条目可以在一个 RPC 来回中被复制给集群中的过半机器；并且单个运行慢的 follower 不会影响整体的性能。 5.4 安全性前面的章节里描述了 Raft 算法是如何进行 leader 选举和日志复制的。然而，到目前为止描述的机制并不能充分地保证每一个状态机会按照相同的顺序执行相同的指令。例如，一个 follower 可能会进入不可用状态，在此期间，leader 可能提交了若干的日志条目，然后这个 follower 可能会被选举为 leader 并且用新的日志条目覆盖这些日志条目；结果，不同的状态机可能会执行不同的指令序列。 这节通过对 leader 选举增加一个限制来完善 Raft 算法。这一限制保证了对于给定的任意任期号， leader 都包含了之前各个任期所有被提交的日志条目（图 3 中的 Leader Completeness 性质）。有了这一 leader 选举的限制，我们也使得提交规则更加清晰。最后，我们展示了对于 Leader Completeness 性质的简要证明并且说明该性质是如何领导复制状态机执行正确的行为的。 5.4.1 选举限制在任何基于 leader 的一致性算法中，leader 最终都必须存储所有已经提交的日志条目。在某些一致性算法中，例如 Viewstamped Replication[22]，一开始并没有包含所有已经提交的日志条目的服务器也可能被选为 leader 。这种算法包含一些额外的机制来识别丢失的日志条目并将它们传送给新的 leader ，要么是在选举阶段要么在之后很快进行。不幸的是，这种方法会导致相当大的额外的机制和复杂性。Raft 使用了一种更加简单的方法，它可以保证新 leader 在当选时就包含了之前所有任期号中已经提交的日志条目，不需要再传送这些日志条目给新 leader 。这意味着日志条目的传送是单向的，只从 leader 到 follower，并且 leader 从不会覆盖本地日志中已经存在的条目。 Raft 使用投票的方式来阻止 candidate 赢得选举除非该 candidate 包含了所有已经提交的日志条目。候选人为了赢得选举必须与集群中的过半节点通信，这意味着至少其中一个服务器节点包含了所有已提交的日志条目。如果 candidate 的日志至少和过半的服务器节点一样新（接下来会精确地定义“新”），那么他一定包含了所有已经提交的日志条目。RequestVote RPC 执行了这样的限制： RPC 中包含了 candidate 的日志信息，如果投票者自己的日志比 candidate 的还新，它会拒绝掉该投票请求。 Raft 通过比较两份日志中最后一条日志条目的索引值和任期号来定义谁的日志比较新。如果两份日志最后条目的任期号不同，那么任期号大的日志更新。如果两份日志最后条目的任期号相同，那么日志较长的那个更新。 5.4.2 提交之前任期内的日志条目如同 5.3 节描述的那样，一旦当前任期内的某个日志条目已经存储到过半的服务器节点上，leader 就知道该日志条目已经被提交了。如果某个 leader 在提交某个日志条目之前崩溃了，以后的 leader 会试图完成该日志条目的复制。然而，如果是之前任期内的某个日志条目已经存储到过半的服务器节点上，leader 也无法立即断定该日志条目已经被提交了。图 8 展示了一种情况，一个已经被存储到过半节点上的老日志条目，仍然有可能会被未来的 leader 覆盖掉。 图 8：如图的时间序列展示了为什么 leader 无法判断老的任期号内的日志是否已经被提交。在 (a) 中，S1 是 leader ，部分地复制了索引位置 2 的日志条目。在 (b) 中，S1 崩溃了，然后 S5 在任期 3 中通过 S3、S4 和自己的选票赢得选举，然后从客户端接收了一条不一样的日志条目放在了索引 2 处。然后到 (c)，S5 又崩溃了；S1 重新启动，选举成功，继续复制日志。此时，来自任期 2 的那条日志已经被复制到了集群中的大多数机器上，但是还没有被提交。如果 S1 在 (d) 中又崩溃了，S5 可以重新被选举成功（通过来自 S2，S3 和 S4 的选票），然后覆盖了他们在索引 2 处的日志。但是，在崩溃之前，如果 S1 在自己的任期里复制了日志条目到大多数机器上，如 (e) 中，然后这个条目就会被提交（S5 就不可能选举成功）。 在这种情况下，之前的所有日志也被提交了。 为了消除图 8 中描述的问题，Raft 永远不会通过计算副本数目的方式来提交之前任期内的日志条目。只有 leader 当前任期内的日志条目才通过计算副本数目的方式来提交；一旦当前任期的某个日志条目以这种方式被提交，那么由于日志匹配特性，之前的所有日志条目也都会被间接地提交。在某些情况下，领导人可以安全地断定一个老的日志条目已经被提交（例如，如果该条目已经存储到所有服务器上），但是 Raft 为了简化问题使用了一种更加保守的方法。 Raft 会在提交规则上增加额外的复杂性是因为当 leader 复制之前任期内的日志条目时，这些日志条目都保留原来的任期号。在其他的一致性算法中，如果一个新的 leader 要重新复制之前的任期里的日志时，它必须使用当前新的任期号。Raft 的做法使得更加容易推导出（reason about）日志条目，因为他们自始至终都使用同一个任期号。另外，和其他的算法相比，Raft 中的新 leader 只需要发送更少的日志条目（其他算法中必须在它们被提交之前发送更多的冗余日志条目来给它们重新编号）。 5.4.3 安全性论证在给出了完整的 Raft 算法之后，我们现在可以更加精确的讨论 leader 完整性特性（Leader Completeness Prop-erty）（这一讨论基于 9.2 节的安全性证明）。我们假设 leader 完整性特性是不满足的，然后我们推出矛盾来。假设任期 T 的 leader（leader T）在任期内提交了一个日志条目，但是该日志条目没有被存储到未来某些任期的 leader 中。假设 U 是大于 T 的没有存储该日志条目的最小任期号。 图 9：如果 S1 （任期 T 的 leader）在它的任期里提交了一个新的日志条目，然后 S5 在之后的任期 U 里被选举为 leader ，那么肯定至少会有一个节点，如 S3，既接收了来自 S1 的日志条目，也给 S5 投票了。 U 一定在刚成为 leader 的时候就没有那条被提交的日志条目了（leader 从不会删除或者覆盖任何条目）。 Leader T 复制该日志条目给集群中的过半节点，同时，leader U 从集群中的过半节点赢得了选票。因此，至少有一个节点（投票者）同时接受了来自 leader T 的日志条目和给 leader U 投票了，如图 9。该投票者是产生矛盾的关键。 该投票者必须在给 leader U 投票之前先接受了从 leader T 发来的已经被提交的日志条目；否则它就会拒绝来自 leader T 的 AppendEntries 请求（因为此时它的任期号会比 T 大）。 该投票者在给 leader U 投票时依然保有这该日志条目，因为任何 U 、T 之间的 leader 都包含该日志条目（根据上述的假设），leader 从不会删除条目，并且 follower 只有跟 leader 冲突的时候才会删除条目。 该投票者把自己选票投给 leader U 时，leader U 的日志必须至少和投票者的一样新。这就导致了以下两个矛盾之一。 首先，如果该投票者和 leader U 的最后一个日志条目的任期号相同，那么 leader U 的日志至少和该投票者的一样长，所以 leader U 的日志一定包含该投票者日志中的所有日志条目。这是一个矛盾，因为该投票者包含了该已被提交的日志条目，但是在上述的假设里，leader U 是不包含的。 否则，leader U 的最后一个日志条目的任期号就必须比该投票者的大了。此外，该任期号也比 T 大，因为该投票者的最后一个日志条目的任期号至少和 T 一样大（它包含了来自任期 T 的已提交的日志）。创建了 leader U 最后一个日志条目的之前的 leader 一定已经包含了该已被提交的日志条目（根据上述假设，leader U 是第一个不包含该日志条目的 leader）。所以，根据日志匹配特性，leader U 一定也包含该已被提交的日志条目，这里产生了矛盾。 因此，所有比 T 大的任期的 leader 一定都包含了任期 T 中提交的所有日志条目。 日志匹配特性保证了未来的 leader 也会包含被间接提交的日志条目，例如图 8 (d) 中的索引 2。 通过 Leader 完整性特性，我们就能证明图 3 中的状态机安全特性，即如果某个服务器已经将某个给定的索引处的日志条目应用到自己的状态机里了，那么其他的服务器就不会在相同的索引处应用一个不同的日志条目。在一个服务器应用一个日志条目到自己的状态机中时，它的日志和 leader 的日志从开始到该日志条目都相同，并且该日志条目必须被提交。现在考虑如下最小任期号：某服务器在该任期号中某个特定的索引处应用了一个日志条目；日志完整性特性保证拥有更高任期号的 leader 会存储相同的日志条目，所以之后任期里服务器应用该索引处的日志条目也会是相同的值。因此，状态机安全特性是成立的。 最后，Raft 要求服务器按照日志索引顺序应用日志条目。再加上状态机安全特性，这就意味着所有的服务器都会按照相同的顺序应用相同的日志条目到自己的状态机中。 5.5 Follower 和 candidate 崩溃到目前为止，我们只关注了 leader 崩溃的情况。Follower 和 candidate 崩溃后的处理方式比 leader 崩溃要简单的多，并且两者的处理方式是相同的。如果 follower 或者 candidate 崩溃了，那么后续发送给他们的 RequestVote 和 AppendEntries RPCs 都会失败。Raft 通过无限的重试来处理这种失败；如果崩溃的机器重启了，那么这些 RPC 就会成功地完成。如果一个服务器在完成了一个 RPC，但是还没有响应的时候崩溃了，那么在它重启之后就会再次收到同样的请求。Raft 的 RPCs 都是幂等的，所以这样的重试不会造成任何伤害。例如，一个 follower 如果收到 AppendEntries 请求但是它的日志中已经包含了这些日志条目，它就会直接忽略这个新的请求中的这些日志条目。 5.6 定时（timing）和可用性Raft 的要求之一就是安全性不能依赖定时：整个系统不能因为某些事件运行得比预期快一点或者慢一点就产生错误的结果。但是，可用性（系统能够及时响应客户端）不可避免的要依赖于定时。例如，当有服务器崩溃时，消息交换的时间就会比正常情况下长，candidate 将不会等待太长的时间来赢得选举；没有一个稳定的 leader ，Raft 将无法工作。 Leader 选举是 Raft 中定时最为关键的方面。 只要整个系统满足下面的时间要求，Raft 就可以选举出并维持一个稳定的 leader： 广播时间（broadcastTime） &lt;&lt; 选举超时时间（electionTimeout） &lt;&lt; 平均故障间隔时间（MTBF） 在这个不等式中，广播时间指的是一个服务器并行地发送 RPCs 给集群中所有的其他服务器并接收到响应的平均时间；选举超时时间就是在 5.2 节中介绍的选举超时时间；平均故障间隔时间就是对于一台服务器而言，两次故障间隔时间的平均值。广播时间必须比选举超时时间小一个量级，这样 leader 才能够可靠地发送心跳消息来阻止 follower 开始进入选举状态；再加上随机化选举超时时间的方法，这个不等式也使得选票瓜分的情况变得不可能。选举超时时间需要比平均故障间隔时间小上几个数量级，这样整个系统才能稳定地运行。当 leader 崩溃后，整个系统会有大约选举超时时间不可用；我们希望该情况在整个时间里只占一小部分。 广播时间和平均故障间隔时间是由系统决定的，但是选举超时时间是我们自己选择的。Raft 的 RPCs 需要接收方将信息持久化地保存到稳定存储中去，所以广播时间大约是 0.5 毫秒到 20 毫秒之间，取决于存储的技术。因此，选举超时时间可能需要在 10 毫秒到 500 毫秒之间。大多数的服务器的平均故障间隔时间都在几个月甚至更长，很容易满足时间的要求。 6 集群成员变更到目前为止，我们都假设集群的配置（参与一致性算法的服务器集合）是固定不变的。但是在实践中，偶尔会改变集群的配置的，例如替换那些宕机的机器或者改变复制程度。尽管可以通过使整个集群下线，更新所有配置，然后重启整个集群的方式来实现，但是在更改期间集群会不可用。另外，如果存在手工操作步骤，那么就会有操作失误的风险。为了避免这样的问题，我们决定将配置变更自动化并将其纳入到 Raft 一致性算法中来。 为了使配置变更机制能够安全，在转换的过程中不能够存在任何时间点使得同一个任期里可能选出两个 leader 。不幸的是，任何服务器直接从旧的配置转换到新的配置的方案都是不安全的。一次性自动地转换所有服务器是不可能的，所以在转换期间整个集群可能划分成两个独立的大多数（见图 10）。 图 10：直接从一种配置转到另一种配置是不安全的，因为各个机器会在不同的时候进行转换。在这个例子中，集群从 3 台机器变成了 5 台。不幸的是，存在这样的一个时间点，同一个任期里两个不同的 leader 会被选出。一个获得旧配置里过半机器的投票，一个获得新配置里过半机器的投票。 为了保证安全性，配置变更必须采用一种两阶段方法。目前有很多种两阶段的实现。例如，有些系统（比如，[22]）在第一阶段停掉旧的配置所以不能处理客户端请求；然后在第二阶段在启用新的配置。在 Raft 中，集群先切换到一个过渡的配置，我们称之为联合一致（joint consensus）；一旦联合一致已经被提交了，那么系统就切换到新的配置上。联合一致结合了老配置和新配置： 日志条目被复制给集群中新、老配置的所有服务器。 新、旧配置的服务器都可以成为 leader 。 达成一致（针对选举和提交）需要分别在两种配置上获得过半的支持。 联合一致允许独立的服务器在不妥协安全性的前提下，在不同的时刻进行配置转换过程。此外，联合一致允许集群在配置变更期间依然响应客户端请求。 集群配置在复制日志中以特殊的日志条目来存储和通信；图 11 展示了配置变更过程。当一个 leader 接收到一个改变配置从 C-old 到 C-new 的请求，它就为联合一致将该配置（图中的 C-old,new）存储为一个日志条目，并以前面描述的方式复制该条目。一旦某个服务器将该新配置日志条目增加到自己的日志中，它就会用该配置来做出未来所有的决策（服务器总是使用它日志中最新的配置，无论该配置日志是否已经被提交）。这就意味着 leader 会使用 C-old,new 的规则来决定 C-old,new 的日志条目是什么时候被提交的。如果 leader 崩溃了，新 leader 可能是在 C-old 配置也可能是在 C-old,new 配置下选出来的，这取决于赢得选举的 candidate 是否已经接收到了 C-old,new 配置。在任何情况下， C-new 在这一时期都不能做出单方面决定。 一旦 C-old,new 被提交，那么 C-old 和 C-new 都不能在没有得到对方认可的情况下做出决定，并且 leader 完整性特性保证了只有拥有 C-old,new 日志条目的服务器才能被选举为 leader 。现在 leader 创建一个描述 C-new 配置的日志条目并复制到集群其他节点就是安全的了。此外，新的配置被服务器收到后就会立即生效。当新的配置在 C-new 的规则下被提交，旧的配置就变得无关紧要，同时不使用新配置的服务器就可以被关闭了。如图 11 所示，任何时刻 C-old 和 C-new 都不能单方面做出决定；这保证了安全性。 在关于配置变更还有三个问题需要解决。第一个问题是，新的服务器开始时可能没有存储任何的日志条目。当这些服务器以这种状态加入到集群中，它们需要一段时间来更新来赶上其他服务器，这段它们无法提交新的日志条目。为了避免因此而造成的系统短时间的不可用，Raft 在配置变更前引入了一个额外的阶段，在该阶段，新的服务器以没有投票权身份加入到集群中来（leader 也复制日志给它们，但是考虑过半的时候不用考虑它们）。一旦该新的服务器追赶上了集群中的其他机器，配置变更就可以按上面描述的方式进行。 第二个问题是，集群的 leader 可能不是新配置中的一员。在这种情况下，leader 一旦提交了 C-new 日志条目就会退位（回到 follower 状态）。这意味着有这样的一段时间（leader 提交 C-new 期间），leader 管理着一个不包括自己的集群；它复制着日志但不把自己算在过半里面。Leader 转换发生在 C-new 被提交的时候，因为这是新配置可以独立运转的最早时刻（将总是能够在 C-new 配置下选出新的领导人）。在此之前，可能只能从 C-old 中选出领导人。 第三个问题是，那些被移除的服务器（不在 C-new 中）可能会扰乱集群。这些服务器将不会再接收到心跳，所以当选举超时，它们就会进行新的选举过程。它们会发送带有新任期号的 RequestVote RPCs ，这样会导致当前的 leader 回到 follower 状态。新的 leader 最终会被选出来，但是被移除的服务器将会再次超时，然后这个过程会再次重复，导致系统可用性很差。 为了防止这种问题，当服务器认为当前 leader 存在时，服务器会忽略RequestVote RPCs 。特别的，当服务器在最小选举超时时间内收到一个 RequestVote RPC，它不会更新任期号或者投票。这不会影响正常的选举，每个服务器在开始一次选举之前，至少等待最小选举超时时间。相反，这有利于避免被移除的服务器的扰乱：如果 leader 能够发送心跳给集群，那它就不会被更大的任期号废黜。 7 日志压缩Raft 的日志在正常操作中随着包含更多的客户端请求不断地增长，但是在实际的系统中，日志不能无限制地增长。随着日志越来越长，它会占用越来越多的空间，并且需要花更多的时间来回放。如果没有一定的机制来清除日志中积累的过期的信息，最终就会带来可用性问题。 快照技术是日志压缩最简单的方法。在快照技术中，整个当前系统的状态都以快照的形式持久化到稳定的存储中，该时间点之前的日志全部丢弃。快照技术被使用在 Chubby 和 ZooKeeper 中，接下来的章节会介绍 Raft 中的快照技术。 增量压缩方法，例如日志清理或者日志结构合并树（log-structured merge trees，LSM 树），都是可行的。这些方法每次只对一小部分数据进行操作，这样就分散了压缩的负载压力。首先，它们先选择一个积累了大量已经被删除或者被覆盖的对象的数据区域，然后重写该区域还活着的对象，之后释放该区域。和快照技术相比，它们需要大量额外的机制和复杂性，快照技术通过操作整个数据集来简化该问题。状态机可以用和快照技术相同的接口来实现 LSM 树，但是日志清除方法就需要修改 Raft 了。 一台服务器用一个新快照替代了它日志中已经提交了的条目（索引 1 到 5），该快照只存储了当前的状态（变量 x 和 y 的值）。快照的 last included index 和 last included term 被保存来定位日志中条目 6 之前的快照 图 12 展示了 Raft 中快照的基本思想。每个服务器独立地创建快照，快照只包括自己日志中已经被提交的条目。主要的工作是状态机将自己的状态写入快照中。Raft 快照中也包含了少量的元数据：the last included index 指的是最后一个被快照取代的日志条目的索引值（状态机最后应用的日志条目），the last included term 是该条目的任期号。保留这些元数据是为了支持快照后第一个条目的 AppendEntries 一致性检查，因为该条目需要之前的索引值和任期号。为了支持集群成员变更（第 6 节），快照中也包括日志中最新的配置作为 last included index 。一旦服务器完成写快照，他就可以删除 last included index 之前的所有日志条目，包括之前的快照。 尽管通常服务器都是独立地创建快照，但是 leader 必须偶尔发送快照给一些落后的跟随者。这通常发生在 leader 已经丢弃了需要发送给 follower 的下一条日志条目的时候。幸运的是这种情况在常规操作中是不可能的：一个与 leader 保持同步的 follower 通常都会有该日志条目。然而一个例外的运行缓慢的 follower 或者新加入集群的服务器（第 6 节）将不会有这个条目。这时让该 follower 更新到最新的状态的方式就是通过网络把快照发送给它。 Leader 使用 InstallSnapshot RPC 来发送快照给太落后的 follower ；见图 13。当 follower 收到带有这种 RPC 的快照时，它必须决定如何处理已经存在的日志条目。通常该快照会包含接收者日志中没有的信息。在这种情况下，follower 丢弃它所有的日志；这些会被该快照所取代，并且可能一些没有提交的条目会和该快照产生冲突。如果接收到的快照是自己日志的前面部分（由于网络重传或者错误），那么被快照包含的条目将会被全部删除，但是快照之后的条目仍然有用并保留。 这种快照的方式违反了 Raft 的 strong leader 原则，因为 follower 可以在不知道 leader 状态的情况下创建快照。但是我们认为这种违背是合乎情理的。Leader 的存在，是为了防止在达成一致性的时候的冲突，但是在创建快照的时候，一致性已经达成，因此没有决策会冲突。数据依然只能从 leader 流到 follower ，只是 follower 可以重新组织它们的数据了。 我们考虑过一种可替代的基于 leader 的快照方案，在该方案中，只有leader 会创建快照，然后 leader 会发送它的快照给所有的 follower 。但是这样做有两个缺点。第一，发送快照会浪费网络带宽并且延缓了快照过程。每个 follower 都已经拥有了创建自己的快照所需要的信息，而且很显然，follower 从本地的状态中创建快照远比通过网络接收别人发来的要来得经济。第二，leader 的实现会更加复杂。例如，leader 发送快照给 follower 的同时也要并行地将新的日志条目发送给它们，这样才不会阻塞新的客户端请求。 还有两个问题会影响快照的性能。首先，服务器必须决定什么时候创建快照。如果快照创建过于频繁，那么就会浪费大量的磁盘带宽和其他资源；如果创建快照频率太低，就要承担耗尽存储容量的风险，同时也增加了重启时日志回放的时间。一个简单的策略就是当日志大小达到一个固定大小的时候就创建一次快照。如果这个阈值设置得显著大于期望的快照的大小，那么快照的磁盘带宽负载就会很小。 第二个性能问题就是写入快照需要花费一段时间，并且我们不希望它影响到正常的操作。解决方案是通过写时复制的技术，这样新的更新就可以在不影响正在写的快照的情况下被接收。例如，具有泛函数据结构的状态机天然支持这样的功能。另外，操作系统对写时复制技术的支持（如 Linux 上的 fork）可以被用来创建整个状态机的内存快照（我们的实现用的就是这种方法）。 8 客户端交互本节介绍客户端如何和 Raft 进行交互，包括客户端如何找到 leader 和 Raft 是如何支持线性化语义的。这些问题对于所有基于一致性的系统都存在，并且 Raft 的解决方案和其他的也差不多。 Raft 的客户端发送所有的请求给 leader 。当客户端第一次启动的时候，它会随机挑选一个服务器进行通信。如果客户端第一次挑选的服务器不是 leader ，那么该服务器会拒绝客户端的请求并且提供关于它最近接收到的领导人的信息（AppendEntries 请求包含了 leader 的网络地址）。如果 leader 已经崩溃了，客户端请求就会超时；客户端之后会再次随机挑选服务器进行重试。 我们 Raft 的目标是要实现线性化语义（每一次操作立即执行，只执行一次，在它的调用和回复之间）。但是，如上述，Raft 可能执行同一条命令多次：例如，如果 leader 在提交了该日志条目之后，响应客户端之前崩溃了，那么客户端会和新的 leader 重试这条指令，导致这条命令被再次执行。解决方案就是客户端对于每一条指令都赋予一个唯一的序列号。然后，状态机跟踪每个客户端已经处理的最新的序列号以及相关联的回复。如果接收到一条指令，该指令的序列号已经被执行过了，就立即返回结果，而不重新执行该请求。 只读的操作可以直接处理而不需要记录日志。但是，如果不采取任何其他措施，这么做可能会有返回过时数据（stale data）的风险，因为 leader 响应客户端请求时可能已经被新的 leader 替代了，但是它还不知道自己已经不是最新的 leader 了。线性化的读操作肯定不会返回过时数据，Raft 需要使用两个额外的预防措施来在不使用日志的情况下保证这一点。首先，leader 必须有关于哪些日志条目被提交了的最新信息。Leader 完整性特性保证了 leader 一定拥有所有已经被提交的日志条目，但是在它任期开始的时候，它可能不知道哪些是已经被提交的。为了知道这些信息，它需要在它的任期里提交一个日志条目。Raft 通过让 leader 在任期开始的时候提交一个空的没有任何操作的日志条目到日志中来处理该问题。第二，leader 在处理只读请求之前必须检查自己是否已经被替代了（如果一个更新的 leader 被选举出来了，它的信息就是过时的了）。Raft 通过让 leader 在响应只读请求之前，先和集群中的过半节点交换一次心跳信息来处理该问题。另一种可选的方案，leader 可以依赖心跳机制来实现一种租约的形式，但是这种方法依赖 timing 来保证安全性（假设时间误差是有界的）。 参考资料 [1] BOLOSKY, W. J., BRADSHAW, D., HAAGENS, R. B., KUSTERS, N. P., AND LI, P. Paxos replicated state machines as the basis of a high-performance data store. In Proc. NSDI’11, USENIX Conference on Networked Systems Design and Implementation (2011), USENIX, pp. 141–154. [2] BURROWS, M. The Chubby lock service for loosely- coupled distributed systems. In Proc. OSDI’06, Sympo- sium on Operating Systems Design and Implementation (2006), USENIX, pp. 335–350. [3] CAMARGOS, L. J., SCHMIDT, R. M., AND PEDONE, F. Multicoordinated Paxos. In Proc. PODC’07, ACM Sym- posium on Principles of Distributed Computing (2007), ACM, pp. 316–317. [4] CHANDRA, T. D., GRIESEMER, R., AND REDSTONE, J. Paxos made live: an engineering perspective. In Proc. PODC’07, ACM Symposium on Principles of Distributed Computing (2007), ACM, pp. 398–407. [5] CHANG, F., DEAN, J., GHEMAWAT, S., HSIEH, W. C., WALLACH, D. A., BURROWS, M., CHANDRA, T., FIKES, A., AND GRUBER, R. E. Bigtable: a distributed storage system for structured data. In Proc. OSDI’06, USENIX Symposium on Operating Systems Design and Implementation (2006), USENIX, pp. 205–218. [6] CORBETT, J. C., DEAN, J., EPSTEIN, M., FIKES, A., FROST, C., FURMAN, J. J., GHEMAWAT, S., GUBAREV, A., HEISER, C., HOCHSCHILD, P., HSIEH, W., KAN- THAK, S., KOGAN, E., LI, H., LLOYD, A., MELNIK, S., MWAURA, D., NAGLE, D., QUINLAN, S., RAO, R., ROLIG, L., SAITO, Y., SZYMANIAK, M., TAYLOR, C., WANG, R., AND WOODFORD, D. Spanner: Google’s globally-distributed database. In Proc. OSDI’12, USENIX Conference on Operating Systems Design and Implemen- tation (2012), USENIX, pp. 251–264. [7] COUSINEAU, D., DOLIGEZ, D., LAMPORT, L., MERZ, S., RICKETTS, D., AND VANZETTO, H. TLA+ proofs. In Proc. FM’12, Symposium on Formal Methods (2012), D. Giannakopoulou and D. Me ́ry, Eds., vol. 7436 of Lec- ture Notes in Computer Science, Springer, pp. 147–154. [8] GHEMAWAT, S., GOBIOFF, H., AND LEUNG, S.-T. The Google file system. In Proc. SOSP’03, ACM Symposium on Operating Systems Principles (2003), ACM, pp. 29–43. [9] GRAY,C.,ANDCHERITON,D.Leases:Anefficientfault- tolerant mechanism for distributed file cache consistency. In Proceedings of the 12th ACM Ssymposium on Operating Systems Principles (1989), pp. 202–210. [10] HERLIHY, M. P., AND WING, J. M. Linearizability: a correctness condition for concurrent objects. ACM Trans- actions on Programming Languages and Systems 12 (July 1990), 463–492. [11] HUNT, P., KONAR, M., JUNQUEIRA, F. P., AND REED, B. ZooKeeper: wait-free coordination for internet-scale systems. In Proc ATC’10, USENIX Annual Technical Con- ference (2010), USENIX, pp. 145–158. [12] JUNQUEIRA, F. P., REED, B. C., AND SERAFINI, M. Zab: High-performance broadcast for primary-backup sys- tems. In Proc. DSN’11, IEEE/IFIP Int’l Conf. on Depend- able Systems &amp; Networks (2011), IEEE Computer Society, pp. 245–256. [13] KIRSCH, J., AND AMIR, Y. Paxos for system builders. Tech. Rep. CNDS-2008-2, Johns Hopkins University, 2008. [14] LAMPORT, L. Time, clocks, and the ordering of events in a distributed system. Commununications of the ACM 21, 7 (July 1978), 558–565. [15] LAMPORT, L. The part-time parliament. ACM Transac- tions on Computer Systems 16, 2 (May 1998), 133–169. [16] LAMPORT, L. Paxos made simple. ACM SIGACT News 32, 4 (Dec. 2001), 18–25. [17] LAMPORT, L. Specifying Systems, The TLA+ Language and Tools for Hardware and Software Engineers. Addison- Wesley, 2002. [18] LAMPORT, L. Generalized consensus and Paxos. Tech. Rep. MSR-TR-2005-33, Microsoft Research, 2005. [19] LAMPORT, L. Fast paxos. Distributed Computing 19, 2 (2006), 79–103. [20] LAMPSON, B. W. How to build a highly available system using consensus. In Distributed Algorithms, O. Baboaglu and K. Marzullo, Eds. Springer-Verlag, 1996, pp. 1–17. [21] LAMPSON, B. W. The ABCD’s of Paxos. In Proc. PODC’01, ACM Symposium on Principles of Distributed Computing (2001), ACM, pp. 13–13. [22] LISKOV, B., AND COWLING, J. Viewstamped replica- tion revisited. Tech. Rep. MIT-CSAIL-TR-2012-021, MIT, July 2012.17 [23] LogCabin source code. logcabin/logcabin.http://github.com/ [24] LORCH, J. R., ADYA, A., BOLOSKY, W. J., CHAIKEN, R., DOUCEUR, J. R., AND HOWELL, J. The SMART way to migrate replicated stateful services. In Proc. Eu- roSys’06, ACM SIGOPS/EuroSys European Conference on Computer Systems (2006), ACM, pp. 103–115. [25] MAO, Y., JUNQUEIRA, F. P., AND MARZULLO, K. Mencius: building efficient replicated state machines forWANs. In Proc. OSDI’08, USENIX Conference on Operating Systems Design and Implementation (2008), USENIX, pp. 369–384. [26] MAZIE` RES, D. Paxos made practical.//www.scs.stanford.edu/ ̃dm/home/ papers/paxos.pdf, Jan. 2007. [27] MORARU, I., ANDERSEN, D. G., AND KAMINSKY, M. There is more consensus in egalitarian parliaments. In Proc. SOSP’13, ACM Symposium on Operating System Principles (2013), ACM. [28] Raft user study. http://ramcloud.stanford. edu/ ̃ongaro/userstudy/. [29] OKI, B. M., AND LISKOV, B. H. Viewstamped replication: A new primary copy method to support highly-available distributed systems. In Proc. PODC’88, ACM Symposium on Principles of Distributed Computing (1988), ACM, pp. 8–17. [30] O’NEIL, P., CHENG, E., GAWLICK, D., AND ONEIL, E. The log-structured merge-tree (LSM-tree). Acta Informat- ica 33, 4 (1996), 351–385. [31] ONGARO, D. Consensus: Bridging Theory and Practice. PhD thesis, Stanford University, 2014 (work in progress).http://ramcloud.stanford.edu/ ̃ongaro/ thesis.pdf. [32] ONGARO, D., AND OUSTERHOUT, J. In search of an understandable consensus algorithm. In Proc ATC’14, USENIX Annual Technical Conference (2014), USENIX. [33] OUSTERHOUT, J., AGRAWAL, P., ERICKSON, D., KOZYRAKIS, C., LEVERICH, J., MAZIE`RES, D., MI- TRA, S., NARAYANAN, A., ONGARO, D., PARULKAR, G., ROSENBLUM, M., RUMBLE, S. M., STRATMANN, E., AND STUTSMAN, R. The case for RAMCloud. Com- munications of the ACM 54 (July 2011), 121–130. [34] Raft consensus algorithm website. http://raftconsensus.github.io. [35] REED, B. Personal communications, May 17, 2013. [36] ROSENBLUM, M., AND OUSTERHOUT, J. K. The design and implementation of a log-structured file system. ACM Trans. Comput. Syst. 10 (February 1992), 26–52. [37] SCHNEIDER, F. B. Implementing fault-tolerant services using the state machine approach: a tutorial. ACM Com- puting Surveys 22, 4 (Dec. 1990), 299–319. [38] SHVACHKO, K., KUANG, H., RADIA, S., AND CHANSLER, R. The Hadoop distributed file system. In Proc. MSST’10, Symposium on Mass Storage Sys- tems and Technologies (2010), IEEE Computer Society, pp. 1–10. [39] VAN RENESSE, R. Paxos made moderately complex. Tech. rep., Cornell University, 2012. Raft 网站","raw":null,"content":null,"categories":[{"name":"分布式一致性算法","slug":"分布式一致性算法","permalink":"http://linbingdong.com/categories/分布式一致性算法/"},{"name":"Raft","slug":"分布式一致性算法/Raft","permalink":"http://linbingdong.com/categories/分布式一致性算法/Raft/"}],"tags":[{"name":"分布式系统","slug":"分布式系统","permalink":"http://linbingdong.com/tags/分布式系统/"},{"name":"分布式一致性算法","slug":"分布式一致性算法","permalink":"http://linbingdong.com/tags/分布式一致性算法/"},{"name":"论文翻译","slug":"论文翻译","permalink":"http://linbingdong.com/tags/论文翻译/"},{"name":"Raft","slug":"Raft","permalink":"http://linbingdong.com/tags/Raft/"}]},{"title":"Python用yield生成杨辉三角","slug":"Python用yield生成杨辉三角","date":"2017-02-15T16:00:00.000Z","updated":"2017-03-26T14:55:40.000Z","comments":true,"path":"2017/02/16/Python用yield生成杨辉三角/","link":"","permalink":"http://linbingdong.com/2017/02/16/Python用yield生成杨辉三角/","excerpt":"so funny","text":"so funny 代码： def triangles(): n = 0 line = [1] while n &lt; 10: yield(line) line.append(0) line = [line[i-1] + line[i] for i in range(len(line))] n += 1for line in triangles(): print(line) 输出： [1][1, 1][1, 2, 1][1, 3, 3, 1][1, 4, 6, 4, 1][1, 5, 10, 10, 5, 1][1, 6, 15, 20, 15, 6, 1][1, 7, 21, 35, 35, 21, 7, 1][1, 8, 28, 56, 70, 56, 28, 8, 1][1, 9, 36, 84, 126, 126, 84, 36, 9, 1]","raw":null,"content":null,"categories":[{"name":"Python","slug":"Python","permalink":"http://linbingdong.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://linbingdong.com/tags/Python/"}]},{"title":"《腾讯传》读书笔记","slug":"《腾讯传》读书笔记","date":"2016-12-31T16:00:00.000Z","updated":"2018-04-22T14:20:37.000Z","comments":true,"path":"2017/01/01/《腾讯传》读书笔记/","link":"","permalink":"http://linbingdong.com/2017/01/01/《腾讯传》读书笔记/","excerpt":"腾讯+吴晓波。一个是互联网巨头，一个是著名的财经作家，必将碰撞出绚丽的火花。","text":"腾讯+吴晓波。一个是互联网巨头，一个是著名的财经作家，必将碰撞出绚丽的火花。 简介 腾讯+吴晓波。一个是互联网巨头，一个是著名的财经作家，必将碰撞出绚丽的火花。 该书记录了腾讯崛起的经历，讲述了腾讯如何从通讯工具起步，逐渐进入到社交网络、互动娱乐、电子商务等领域。书中还列举了许多腾讯著名产品如QQ空间、微信等诞生的经过以及曾经轰动的“3Q”大战，相当精彩。 更难能可贵的是，书中还涉及到腾讯很多产品设计的哲学和运用到的心理学知识，产品经理应该能从本书中获得不少启发。 此外，创业者也可以看看此书，了解腾讯这个巨头在成长过程中遇到的风险和挫折，以及公司是如何解决这些问题的，这些对创业者来说也是满满的干货。 摘录 互联网公司取得成功的八字秘诀：小步快跑、试错迭代 跟那些走在时代前沿的人在一起，你也能走在时代前沿 拥抱变化，顺应时代潮流 不是万事俱备才去做，很多事情都是被逼出来的 任何事情都不是一帆风顺的，未来很难预料，要拥抱变化 有多个创始人的好处是在重要关头可以一起商量 创业路上运气也很重要 风水轮流转。抓住机遇，一切皆有可能 腾讯曾经被搜狐、新浪、雅虎和联想拒绝过 投资者应该深入人民群众的生活中，看看大家都在干什么、玩什么、用什么 创始人不要放弃公司的控制权 不到最后一刻绝不放弃。山重水复疑无路，柳暗花明又一村 想要创新，就要有敏锐的商业嗅觉 中国移动是在2000年从电信剥离出来的 每个国家的用户习惯可能不同，因此需要采取不同的策略，不同的商业模式 要敢于创新，做一些别人没做过的，比如QQ的会员制 企业运作过程中难免会走一些弯路，重要的是及时发现，及时挽救 产品中很多要用到心理学，比如QQ秀，其实是人们情感的寄托，装扮QQ秀的过程本质上是一次自我性格及身份确认的过程 产品要善于发掘人性，比如微信的摇一摇、附近的人等 腾讯内部的赛马机制——非顶层规划，谁提出，谁执行。这样做的好处的大家充分发挥自己的潜力，奇思妙想，而且很有动力，相当于内部孕育着很多创业公司 刘炽平在腾讯的发展过程中起了重大的作用，因此创业者要找到合适的人才 战略转型是激烈竞争的结果，而未必是事先成熟规划的产物，再次印证了要拥抱变化 当笼统的研发部已经不能满足需求，于是开始以产品为单位来划分 大权独揽，小权分散 开辟疆土是相对容易的，但如果统治能力的半径达不到疆域的半径的话，这样的帝国难以持久。因此企业也不要随意扩张，随意分散精力 腾讯战略：紧盯市场，以最快的方式复制成功者模式，利用QQ用户优势进行后发超越 创业初期很可能只能招到二流、三流的人才 有时候可以放任盗版，这样有助于产品流行起来，比如微软对windows的态度 互联网，唯快不破 程序员应该能提出一些新理念，并带来效果。不要只会实现功能，多思考 到用户中去 如果质量不过关，欲速则不达 QQ主要靠增值业务和虚拟道具挣钱，比如会员、QQ秀、QQ空间、QQ农场 QQ在腾讯其他业务的推广中起了极大的作用，包括游戏，还有QQ弹窗对腾讯网的导流等 紧盯市场热点，快速根据优化，利用自己的流量优势实现整体替代 扩张也会有风险，搞不好还会失去原本的优势 创业者要善于控制互联网舆论，比如3Q大战中的周鸿祎 互联网也是弱肉强食 互联网最好的审美观——极简主义 互联网产品能否成功的关键是在某一场景下的用户体验 产品经理更应该依靠直觉和感性，而非图表和分析，来把握用户需求 一个好的产品往往从不完美开始","raw":null,"content":null,"categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://linbingdong.com/categories/读书笔记/"}],"tags":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://linbingdong.com/tags/读书笔记/"}]},{"title":"Hive on Spark调优","slug":"Hive on Spark调优","date":"2016-11-29T16:00:00.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2016/11/30/Hive on Spark调优/","link":"","permalink":"http://linbingdong.com/2016/11/30/Hive on Spark调优/","excerpt":"之前在Hive on Spark跑TPCx-BB测试时，100g的数据量要跑十几个小时，一看CPU和内存的监控，发现    POWER_TEST阶段（依次执行30个查询）CPU只用了百分之十几，也就是没有把整个集群的性能利用起来，导致跑得很慢。因此，如何调整参数，使整个集群发挥最大性能显得尤为重要。","text":"之前在Hive on Spark跑TPCx-BB测试时，100g的数据量要跑十几个小时，一看CPU和内存的监控，发现 POWER_TEST阶段（依次执行30个查询）CPU只用了百分之十几，也就是没有把整个集群的性能利用起来，导致跑得很慢。因此，如何调整参数，使整个集群发挥最大性能显得尤为重要。 Spark作业运行原理 详细原理见上图。我们使用spark-submit提交一个Spark作业之后，这个作业就会启动一个对应的Driver进程。根据你使用的部署模式（deploy-mode）不同，Driver进程可能在本地启动，也可能在集群中某个工作节点上启动。Driver进程本身会根据我们设置的参数，占有一定数量的内存和CPU core。而Driver进程要做的第一件事情，就是向集群管理器（可以是Spark Standalone集群，也可以是其他的资源管理集群，美团•大众点评使用的是YARN作为资源管理集群）申请运行Spark作业需要使用的资源，这里的资源指的就是Executor进程。YARN集群管理器会根据我们为Spark作业设置的资源参数，在各个工作节点上，启动一定数量的Executor进程，每个Executor进程都占有一定数量的内存和CPU core。 Spark是根据shuffle类算子来进行stage的划分。如果我们的代码中执行了某个shuffle类算子（比如reduceByKey、join等），那么就会在该算子处，划分出一个stage界限来。可以大致理解为，shuffle算子执行之前的代码会被划分为一个stage，shuffle算子执行以及之后的代码会被划分为下一个stage。因此一个stage刚开始执行的时候，它的每个task可能都会从上一个stage的task所在的节点，去通过网络传输拉取需要自己处理的所有key，然后对拉取到的所有相同的key使用我们自己编写的算子函数执行聚合操作（比如reduceByKey()算子接收的函数）。这个过程就是shuffle。 task的执行速度是跟每个Executor进程的CPU core数量有直接关系的。一个CPU core同一时间只能执行一个线程。而每个Executor进程上分配到的多个task，都是以每个task一条线程的方式，多线程并发运行的。如果CPU core数量比较充足，而且分配到的task数量比较合理，那么通常来说，可以比较快速和高效地执行完这些task线程。 以上就是Spark作业的基本运行原理的说明，大家可以结合上图来理解。理解作业基本原理，是我们进行资源参数调优的基本前提。 参数调优了解完了Spark作业运行的基本原理之后，对资源相关的参数就容易理解了。所谓的Spark资源参数调优，其实主要就是对Spark运行过程中各个使用资源的地方，通过调节各种参数，来优化资源使用的效率，从而提升Spark作业的执行性能。以下参数就是Spark中主要的资源参数，每个参数都对应着作业运行原理中的某个部分。 num-executors/spark.executor.instances 参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。 参数调优建议：每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。 executor-memory/spark.executor.memory 参数说明：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。 参数调优建议：每个Executor进程的内存设置4G~8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源队列最大总内存的1/3~1/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。 executor-cores/spark.executor.cores 参数说明：该参数用于设置每个Executor进程的CPU core数量。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。 参数调优建议：Executor的CPU core数量设置为2~4个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过队列总CPU core的1/3~1/2左右比较合适，也是避免影响其他同学的作业运行。 driver-memory 参数说明：该参数用于设置Driver进程的内存。 参数调优建议：Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。 spark.default.parallelism 参数说明：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。 参数调优建议：Spark作业的默认task数量为500~1000个较为合适。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的2~3倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。 spark.storage.memoryFraction 参数说明：该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。 参数调优建议：如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。 spark.shuffle.memoryFraction 参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。 参数调优建议：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。 调优过程数据量：10g 可以看出： 随着每个executor占用的CPU core数增加，q04查询的时间显著下降，q03也下降，但幅度没那么大。 本次调优只设置了spark.executor.memory和spark.executor.cores两个参数，没有涉及到spark.executor.instances参数，而默认的spark.executor.instances为2，也就是每个作业只用到2个executor，因此还没将性能发挥到最佳。 接下来采用100g的数据量，并且增加spark.executor.instances参数的设置。 数据量：100g 可以看出： 调优前后查询时间有了很大的飞跃； 增加spark.executor.instances设置项指定每个作业占用的executor个数后性能又有很大提升（通过监控我们发现此时CPU利用率平均有好几十，甚至可以高到百分之九十几）； 至此，我们终于将整个集群性能充分发挥出来，达到目的。 最后一列配置项是根据美团技术团队博客的建议设置的，可以看出性能相比我们之前自己的设置还是有一定提升的，至少该博客里建议的设置是比较通用的，因此之后我们都采取最后一列的设置来跑TPCx-BB测试。 最后来张大图展示调优前和调优后跑100g数据的对比： 可以看出： 绝大多数查询调优前后查询时间有了极大的飞跃； 但是像q01/q04/q14…这几个查询，可能因为查询涉及到的表比较小，调优前时间就很短，因此调优后也看不出很多差别，如果想看到大的差别，可能需要提高数据量，比如1T，3T； q10和q18调优前后时间都较长，而且调优后性能没有提升，需要再深入探索下是什么原因。 最后，用调优后的集群，分别跑10g、30g、100g的数据，结果如下： 可以看出： 随着数据量增大，很多查询时间并没有明显增加，可能是因为集群性能太强，而且数据量还不够大，可以增大数据量继续观察 对于q10、q18和q30，随着数据量增大，时间明显增大，需再深入分析 参考文献http://tech.meituan.com/spark-tuning-basic.html","raw":null,"content":null,"categories":[{"name":"Hive","slug":"Hive","permalink":"http://linbingdong.com/categories/Hive/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://linbingdong.com/tags/大数据/"},{"name":"NoSQL","slug":"NoSQL","permalink":"http://linbingdong.com/tags/NoSQL/"},{"name":"Hive","slug":"Hive","permalink":"http://linbingdong.com/tags/Hive/"},{"name":"Spark","slug":"Spark","permalink":"http://linbingdong.com/tags/Spark/"}]},{"title":"Hive on Spark安装配置详解","slug":"Hive on Spark安装配置详解","date":"2016-10-09T16:00:00.000Z","updated":"2017-03-11T12:33:42.000Z","comments":true,"path":"2016/10/10/Hive on Spark安装配置详解/","link":"","permalink":"http://linbingdong.com/2016/10/10/Hive on Spark安装配置详解/","excerpt":"简介本文主要记录如何安装配置Hive on Spark，在执行以下步骤之前，请先确保已经安装Hadoop集群，Hive，MySQL，JDK，Scala，具体安装步骤不再赘述。","text":"简介本文主要记录如何安装配置Hive on Spark，在执行以下步骤之前，请先确保已经安装Hadoop集群，Hive，MySQL，JDK，Scala，具体安装步骤不再赘述。 背景Hive默认使用MapReduce作为执行引擎，即Hive on mr。实际上，Hive还可以使用Tez和Spark作为其执行引擎，分别为Hive on Tez和Hive on Spark。由于MapReduce中间计算均需要写入磁盘，而Spark是放在内存中，所以总体来讲Spark比MapReduce快很多。因此，Hive on Spark也会比Hive on mr快。为了对比Hive on Spark和Hive on mr的速度，需要在已经安装了Hadoop集群的机器上安装Spark集群（Spark集群是建立在Hadoop集群之上的，也就是需要先装Hadoop集群，再装Spark集群，因为Spark用了Hadoop的HDFS、YARN等），然后把Hive的执行引擎设置为Spark。 Spark运行模式分为三种1、Spark on YARN 2、Standalone Mode 3、Spark on Mesos。Hive on Spark默认支持Spark on YARN模式，因此我们选择Spark on YARN模式。Spark on YARN就是使用YARN作为Spark的资源管理器。分为Cluster和Client两种模式。 一、环境说明本教程Hadoop相关软件全部基于CDH5.5.1，用yum安装，系统环境如下： 操作系统：CentOS 7.2 Hadoop 2.6.0 Hive1.1.0 Spark1.5.0 MySQL 5.6 JDK 1.8 Maven 3.3.3 Scala 2.10 各节点规划如下： 192.168.117.51 Goblin01 nn1 jn1 rm1 worker master hive metastore mysql192.168.117.52 Goblin02 zk2 nn2 jn2 rm2 worker hive192.168.117.53 Goblin03 zk3 dn1 jn3 worker hive192.168.117.54 Goblin04 zk4 dn2 worker hive 说明：Goblin01~04是每台机器的hostname，zk代表zookeeper，nn代表hadoop的namenode，dn代表datanode，jn代表journalnode，rm代表resourcemanager，worker代表Spark的slaves，master代表Spark的master 二、编译和安装Spark（Spark on YARN）2.1 编译Spark源码要使用Hive on Spark，所用的Spark版本必须不包含Hive的相关jar包，hive on spark 的官网上说“Note that you must have a version of Spark which does not include the Hive jars”。在spark官网下载的编译的Spark都是有集成Hive的，因此需要自己下载源码来编译，并且编译的时候不指定Hive。 我们这里用的Spark源码是spark-1.5.0-cdh5.5.1版本,下载地址如下： http://archive.cloudera.com/cdh5/cdh/5/spark-1.5.0-cdh5.5.1-src.tar.gz 下载完后用 tar xzvf 命令解压，进入解压完的文件夹，准备编译。 注意：编译前请确保已经安装JDK、Maven和Scala，maven为3.3.3及以上版本，并在/etc/profile里配置环境变量。 命令行进入在源码根目录下，执行 ./make-distribution.sh --name &quot;hadoop2-without-hive&quot; --tgz &quot;-Pyarn,hadoop-provided,hadoop-2.6,parquet-provided&quot; 若编译过程出现内存不足的情况，需要在运行编译命令之前先运行： export MAVEN_OPTS=&quot;-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m&quot; 来设置Maven的内存。 编译过程由于要下载很多Maven依赖的jar包，需要时间较长（大概一两个小时），要保证网络状况良好，不然很容易编译失败。若出现以下结果，则编译成功： 编译成功后，会在源码根目录下多出一个文件(红色部分）： spark-1.5.0-cdh5.5.1-bin-hadoop2-without-hive.tgz 2.2 安装Spark 将编译完生成的spark-1.5.0-cdh5.5.1-bin-hadoop2-without-hive.tgz拷贝到Spark的安装路径，并用 tar -xzvf 命令解压 配置环境变量 $vim /etc/profileexport SPARK_HOME=spark安装路径$source /etc/profile 2.3 配置Spark配置spark-env.sh、slaves和spark-defaults.conf三个文件 spark-env.sh 主要配置JAVA\\_HOME、SCALA\\_HOME、HADOOP\\_HOME、HADOOP\\_CONF\\_DIR、SPARK\\_MASTER\\_IP等 export JAVA_HOME=/usr/lib/jvm/javaexport SCALA_HOME=/root/scalaexport HADOOP_HOME=/usr/lib/hadoopexport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoopexport SPARK_LAUNCH_WITH_SCALA=0export SPARK_WORKER_MEMORY=1gexport SPARK_DRIVER_MEMORY=1gexport SPARK_MASTER_IP=192.168.117.51export SPARK_LIBRARY_PATH=/root/spark-without-hive/libexport SPARK_MASTER_WEBUI_PORT=18080export SPARK_WORKER_DIR=/root/spark-without-hive/workexport SPARK_MASTER_PORT=7077export SPARK_WORKER_PORT=7078export SPARK_LOG_DIR=/root/spark-without-hive/logexport SPARK_PID_DIR=&apos;/root/spark-without-hive/run&apos; slaves（将所有节点都加入，master节点同时也是worker节点） Goblin01Goblin02Goblin03Goblin04 spark-defaults.conf spark.master yarn-clusterspark.home /root/spark-without-hivespark.eventLog.enabled truespark.eventLog.dir hdfs://Goblin01:8020/spark-logspark.serializer org.apache.spark.serializer.KryoSerializerspark.executor.memory 1gspark.driver.memory 1gspark.executor.extraJavaOptions -XX:+PrintGCDetails -Dkey=value -Dnumbers=&quot;one two three&quot; spark.master指定Spark运行模式，可以是yarn-client、yarn-cluster… spark.home指定SPARK_HOME路径 spark.eventLog.enabled需要设为true spark.eventLog.dir指定路径，放在master节点的hdfs中，端口要跟hdfs设置的端口一致（默认为8020），否则会报错 spark.executor.memory和spark.driver.memory指定executor和dirver的内存，512m或1g，既不能太大也不能太小，因为太小运行不了，太大又会影响其他服务 三、配置YARN配置yarn-site.xml，跟hdfs-site.xml在同一个路径下（$HADOOP_HOME/etc/hadoop) &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler&lt;/value&gt;&lt;/property&gt; 四、配置Hive 添加spark依赖到hive(将spark-assembly-1.5.0-cdh5.5.1-hadoop2.6.0.jar拷贝到$HIVE\\_HOME/lib目录下） 进入SPARK\\_HOME cp spark-assembly-1.5.0-cdh5.5.1-hadoop2.6.0.jar /usr/lib/hive/lib 配置hive-site.xml 配置的内容与spark-defaults.conf相同，只是形式不一样,以下内容是追加到hive-site.xml文件中的,并且注意前两个配置，如果不设置hive的spark引擎用不了，在后面会有详细的错误说明。 &lt;property&gt; &lt;name&gt;hive.execution.engine&lt;/name&gt; &lt;value&gt;spark&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.enable.spark.execution.engine&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;spark.home&lt;/name&gt; &lt;value&gt;/root/spark-without-hive&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;spark.master&lt;/name&gt; &lt;value&gt;yarn-client&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;spark.enentLog.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;spark.enentLog.dir&lt;/name&gt; &lt;value&gt;hdfs://Goblin01:8020/spark-log&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;spark.serializer&lt;/name&gt;&lt;value&gt;org.apache.spark.serializer.KryoSerializer&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;spark.executor.memeory&lt;/name&gt; &lt;value&gt;1g&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;spark.driver.memeory&lt;/name&gt; &lt;value&gt;1g&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;spark.executor.extraJavaOptions&lt;/name&gt; &lt;value&gt;-XX:+PrintGCDetails -Dkey=value -Dnumbers=&quot;one two three&quot;&lt;/value&gt;&lt;/property&gt; 五、验证是否安装配置成功1.验证Spark集群注意：在启动Spark集群之前，要确保Hadoop集群和YARN均已启动 进入$SPARK_HOME目录，执行： ./sbin/start-all.sh 用jps命令查看51节点上的master和worker，52、53、54节点上的worker是否都启动了 同样在$SPARK_HOME目录下，提交计算Pi的任务，验证Spark集群是否能正常工作，运行如下命令 ./bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode client lib/spark-examples-1.5.0-cdh5.5.1-hadoop2.6.0.jar 10 若无报错，并且算出Pi的值，说明Spark集群能正常工作 2.验证Hive on Spark是否可用 命令行输入 hive，进入hive CLI set hive.execution.engine=spark; (将执行引擎设为Spark，默认是mr，退出hive CLI后，回到默认设置。若想让引擎默认为Spark，需要在hive-site.xml里设置） create table test(ts BIGINT,line STRING); (创建表） select count(*) from test; 若整个过程没有报错，并出现正确结果，则Hive on Spark配置成功。 六、遇到的问题0编译spark基于maven有两种方式 用mvn 命令编译 ./build/mvn -Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0 -DskipTests clean package 编译到倒数MQTT模块一直报错，而且编译出的文件比较大，不适合安装集群，因此不推荐。使用Intellij IDEA maven 插件报错如下： 使用spark提供的预编译脚本，网络状况稳定，会编译出需要的安装版本，推荐。命令 ./make-distribution.sh --name &quot;hadoop2-without-hive&quot; --tgz &quot;-Pyarn,hadoop-provided,hadoop-2.6,parquet-provided&quot; 结果如上文所述。 1运行： ./bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn lib/spark-examples-1.5.0-cdh5.5.1-hadoop2.6.0.jar 10 报错： 原因： hdfs的默认端口为8020 ，而我们在spark-default.conf中配置成了8021端口，导致连接不上HDFS报错 spark.eventLog.enabled truespark.eventLog.dir hdfs://Goblin01:8021/spark-log 解决： 配置spark-default.conf中的spark.eventLog.dir 为本地路径，也就是不持久化日志到hdfs上，也就没有和hdfs的通行 or spark-default.conf 注释掉 spark.eventLog.enabled true or 在spark-default.conf里配置的eventLog端口跟hdfs的默认端口（8020）一致 or 由于配置的hdfs是高可用的，51,52都可以作为namenode,我们的spark集群的主节点在51上，当51上的namenode变成standby，导致无法访问hdfs的8020端口（hdfs默认端口），也就是说在51上读不出hdfs上spark-log的内容，在spark-default.conf中配置为spark.eventLog.dir hdfs://Goblin01:8021/spark-log，如果发生这种情况，直接kill掉52，让namenode只在51上运行。（这个后面要搭建spark的高可用模式解决） 2运行： 在hive里设置引擎为spark，执行select count(*) from a; 报错： Failed to execute spark task, with exception &apos;org.apache.hadoop.hive.ql.metadata.HiveException(Unsupported execution engine: Spark. Please set hive.execution.engine=mr)&apos; 解决： 这是因为CDH版的Hive默认运行支持Hive on Spark（By default, Hive on Spark is not enabled）. 需要用cloudera manager（cloudera官网给的的方法，但是要装cloudera manager，比较麻烦，不建议） Go to the Hive service.Click the Configuration tab.Enter Enable Hive on Sparkin the Search field.Check the box for Enable Hive on Spark (Unsupported).Locate the Spark On YARN Service and click SPARK_ON_YARN.Click Save Changes to commit the changes. 或者 在hive-site.xml添加配置(简单、推荐） &lt;property&gt;&lt;name&gt;hive.enable.spark.execution.engine&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 3终端输入hive无法启动hive CLI 原因：namenode挂了 解决：重启namenode 4运行： ./bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode client lib/spark-examples-1.5.0-cdh5.5.1-hadoop2.6.0.jar 10 问题： 没有报错，但是出现以下情况，停不下来 原因： ResourceManager或者NodeManager挂掉，一直没有NodeManager响应，任务无法执行，所有停不下来。 还有一种情况是spark有别的application在运行，导致本次spark任务的等待或者失败 解决： 对于原因1，重启ResourceManager和NodeManager。 service hadoop-yarn-resourcemanager start;service hadoop-yarn-nodemanager start; 对于原因2，解决办法是在hadoop配置文件中设置yarn的并行度，在/etc/hadoop/conf/capacity-scheduler.xml文件中配置yarn.scheduler.capacity.maximum-am-resource-percent from 0.1 to 0.5 &lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.maximum-am-resource-percent&lt;/name&gt; &lt;value&gt;0.5&lt;/value&gt; &lt;description&gt; Maximum percent of resources in the cluster which can be used to run application masters i.e. controls number of concurrent running applications. &lt;/description&gt; &lt;/property&gt; 参考资料 https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark:+Getting+Started http://www.cloudera.com/documentation/enterprise/5-5-x/topics/admin_hos_config.html http://spark.apache.org/docs/latest/building-spark.html http://stackoverflow.com/questions/31743586/apache-spark-running-locally-giving-refused-connection-error http://stackoverflow.com/questions/30828879/application-report-for-application-state-accepted-never-ends-for-spark-submi http://www.voidcn.com/blog/tianyiii/article/p-5986990.html http://www.imooc.com/article/8613 http://lxw1234.com/archives/2016/05/673.htm","raw":null,"content":null,"categories":[{"name":"Hive","slug":"Hive","permalink":"http://linbingdong.com/categories/Hive/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://linbingdong.com/tags/大数据/"},{"name":"NoSQL","slug":"NoSQL","permalink":"http://linbingdong.com/tags/NoSQL/"},{"name":"Hive","slug":"Hive","permalink":"http://linbingdong.com/tags/Hive/"},{"name":"Spark","slug":"Spark","permalink":"http://linbingdong.com/tags/Spark/"}]}]}